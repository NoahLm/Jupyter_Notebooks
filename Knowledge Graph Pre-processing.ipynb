{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPN7xS491YM3gqjH586xubp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install contractions"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QAUhW29GkH0F","executionInfo":{"status":"ok","timestamp":1660001659453,"user_tz":300,"elapsed":3198,"user":{"displayName":"Noé Lozano","userId":"09597272047670713906"}},"outputId":"c3fecc60-a972-47a6-a55f-94a1f1d5f85b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.1.72)\n","Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n","Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n","Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n"]}]},{"cell_type":"code","source":["!pip install nltk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S1HIwkvuarU7","executionInfo":{"status":"ok","timestamp":1660001663037,"user_tz":300,"elapsed":3593,"user":{"displayName":"Noé Lozano","userId":"09597272047670713906"}},"outputId":"912e208b-4ec8-4d37-be0c-f466d2c955dc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KfFDfEULtOJD","executionInfo":{"status":"ok","timestamp":1660001663038,"user_tz":300,"elapsed":8,"user":{"displayName":"Noé Lozano","userId":"09597272047670713906"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8434581c-b494-4b82-8409-a7417cfe104e"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":529}],"source":["import re\n","\n","import string\n","\n","from gensim.parsing.preprocessing import remove_stopwords\n","\n","import contractions\n","\n","from string import digits\n","\n","import nltk\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","nltk.download(\"punkt\")"]},{"cell_type":"code","source":["RAW = []\n","with open('MathematicsForMachineLearning.txt', 'r') as reader:\n","  for line in reader.readlines():\n","    RAW.append(line)"],"metadata":{"id":"E3aAlQwWPSQ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["RAW[0:20]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m9ZVQZvSXSrG","executionInfo":{"status":"ok","timestamp":1660001663272,"user_tz":300,"elapsed":13,"user":{"displayName":"Noé Lozano","userId":"09597272047670713906"}},"outputId":"94834221-0081-475f-bba6-1dac4e108aca"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['MATHEMATICS FOR MACHINE LEARNINGMarc Peter DeisenrothA. Aldo FaisalCheng Soon OngMATHEMATICS FOR MACHINE LEARNINGDEISENROTH ET AL.The fundamental mathematical tools needed to understand machine learning include linear algebra, analytic geometry, matrix decompositions, vector calculus, optimization, probability and statistics. These topics are traditionally taught in disparate courses, making it hard for data science or computer science students, or professionals, to efﬁ ciently learn the mathematics. This self-contained textbook bridges the gap between mathematical and machine learning texts, introducing the mathematical concepts with a minimum of prerequisites. It uses these concepts to derive four central machine learning methods: linear regression, principal component analysis, Gaussian mixture models and support vector machines. For students and others with a mathematical background, these derivations provide a starting point to machine learning texts. For those learning the mathematics for the ﬁ rst time, the methods help build intuition and practical experience with applying mathematical concepts. Every chapter includes worked examples and exercises to test understanding. Programming tutorials are offered on the book’s web site.MARC PETER DEISENROTH is Senior Lecturer in Statistical Machine Learning at the Department of Computing, Împerial College London.A. ALDO FAISAL leads the Brain & Behaviour Lab at Imperial College London, where he is also Reader in Neurotechnology at the Department of Bioengineering and the Department of Computing.CHENG SOON ONG is Principal Research Scientist at the Machine Learning Research Group, Data61, CSIRO. He is also Adjunct Associate Professor at Australian National University.Cover image courtesy of Daniel Bosma / Moment / Getty ImagesCover design by Holly JohnsonDeisenrith et al. 9781108455145 Cover. C M Y K\\x0c\\x0cContents\\n',\n"," '\\n',\n"," 'Foreword\\n',\n"," '\\n',\n"," 'Part I Mathematical Foundations\\n',\n"," '\\n',\n"," '1\\n',\n"," '1.1\\n',\n"," '1.2\\n',\n"," '1.3\\n',\n"," '\\n',\n"," 'Introduction and Motivation\\n',\n"," 'Finding Words for Intuitions\\n',\n"," 'Two Ways to Read This Book\\n',\n"," 'Exercises and Feedback\\n',\n"," '\\n',\n"," 'Linear Algebra\\n',\n"," 'Systems of Linear Equations\\n',\n"," '\\n',\n"," '2\\n']"]},"metadata":{},"execution_count":531}]},{"cell_type":"code","source":["textBook_lower = []\n","for line in RAW:\n","    y = line.lower()\n","    textBook_lower.append(y)"],"metadata":{"id":"Xfg3uNqpSjvT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["textBook_lower[1000:1020]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fet_CPHrXTuo","executionInfo":{"status":"ok","timestamp":1660001663273,"user_tz":300,"elapsed":11,"user":{"displayName":"Noé Lozano","userId":"09597272047670713906"}},"outputId":"c448be35-ae17-481f-c6a8-ac3e67168609"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['\\n',\n"," '11\\n',\n"," 'this material is published by cambridge university press as mathematics for machine learning by\\n',\n"," 'marc peter deisenroth, a. aldo faisal, and cheng soon ong (2020). this version is free to view\\n',\n"," 'and download for personal use only. not for re-distribution, re-sale, or use in derivative works.\\n',\n"," '©by m. p. deisenroth, a. a. faisal, and c. s. ong, 2021. https://mml-book.com.\\n',\n"," '\\n',\n"," '\\x0cpredictor\\n',\n"," '\\n',\n"," 'training\\n',\n"," '\\n',\n"," 'data as vectors\\n',\n"," '\\n',\n"," 'model\\n',\n"," '\\n',\n"," 'learning\\n',\n"," '\\n',\n"," '12\\n',\n"," '\\n',\n"," 'introduction and motivation\\n']"]},"metadata":{},"execution_count":533}]},{"cell_type":"code","source":["textBook_wthtn = []\n","for line in textBook_lower:\n","  l = line.replace('\\n','')\n","  textBook_wthtn.append(l)"],"metadata":{"id":"GRoUvVqTTPtK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["textBook_wthtn[1000:1020]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RFyfjJ9jXWNt","executionInfo":{"status":"ok","timestamp":1660001663273,"user_tz":300,"elapsed":7,"user":{"displayName":"Noé Lozano","userId":"09597272047670713906"}},"outputId":"55f5a4d0-01f9-4338-bd9f-ce01b37210c4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['',\n"," '11',\n"," 'this material is published by cambridge university press as mathematics for machine learning by',\n"," 'marc peter deisenroth, a. aldo faisal, and cheng soon ong (2020). this version is free to view',\n"," 'and download for personal use only. not for re-distribution, re-sale, or use in derivative works.',\n"," '©by m. p. deisenroth, a. a. faisal, and c. s. ong, 2021. https://mml-book.com.',\n"," '',\n"," '\\x0cpredictor',\n"," '',\n"," 'training',\n"," '',\n"," 'data as vectors',\n"," '',\n"," 'model',\n"," '',\n"," 'learning',\n"," '',\n"," '12',\n"," '',\n"," 'introduction and motivation']"]},"metadata":{},"execution_count":535}]},{"cell_type":"code","source":["textBook_wthtBlank = []\n","for line in textBook_wthtn:\n","  if line != '':\n","    textBook_wthtBlank.append(line)"],"metadata":{"id":"Q7uoPaOcVMrT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["textBook_wthtBlank[1000:1020]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yMVTRBZCVj4Z","executionInfo":{"status":"ok","timestamp":1660001663480,"user_tz":300,"elapsed":5,"user":{"displayName":"Noé Lozano","userId":"09597272047670713906"}},"outputId":"87f0f111-b773-46ed-bbd7-213574a58c4a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['objective is to ﬁnd functions that map inputs x',\n"," 'r, which we can interpret as the labels of their',\n"," 'served function values y',\n"," 'respective inputs. we will discuss classical model ﬁtting (parameter esti-',\n"," 'mation) via maximum likelihood and maximum a posteriori estimation,',\n"," 'as well as bayesian linear regression, where we integrate the parameters',\n"," 'out instead of optimizing them.',\n"," '∈',\n"," '∈',\n"," 'chapter 10 focuses on dimensionality reduction, the second pillar in fig-',\n"," 'ure 1.1, using principal component analysis. the key objective of dimen-',\n"," 'sionality reduction is to ﬁnd a compact, lower-dimensional representation',\n"," 'rd, which is often easier to analyze than',\n"," 'of high-dimensional data x',\n"," 'the original data. unlike regression, dimensionality reduction is only con-',\n"," 'cerned about modeling the data – there are no labels associated with a',\n"," 'data point x.',\n"," '∈',\n"," 'in chapter 11, we will move to our third pillar: density estimation. the',\n"," 'objective of density estimation is to ﬁnd a probability distribution that de-']"]},"metadata":{},"execution_count":537}]},{"cell_type":"code","source":["expanded_text = []   \n","\n","for line in textBook_wthtBlank:\n","  expanded_text.append(contractions.fix(line))"],"metadata":{"id":"hVG9oyUdj-Od"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["expanded_text[1000:1020]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jFLC4XKPmhs9","executionInfo":{"status":"ok","timestamp":1660001663886,"user_tz":300,"elapsed":215,"user":{"displayName":"Noé Lozano","userId":"09597272047670713906"}},"outputId":"5d87f5cb-c8d2-4864-c477-bc2bdaff9d67"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['objective is to ﬁnd functions that map inputs x',\n"," 'r, which we can interpret as the labels of their',\n"," 'served function values y',\n"," 'respective inputs. we will discuss classical model ﬁtting (parameter esti-',\n"," 'mation) via maximum likelihood and maximum a posteriori estimation,',\n"," 'as well as bayesian linear regression, where we integrate the parameters',\n"," 'out instead of optimizing them.',\n"," '∈',\n"," '∈',\n"," 'chapter 10 focuses on dimensionality reduction, the second pillar in fig-',\n"," 'ure 1.1, using principal component analysis. the key objective of dimen-',\n"," 'sionality reduction is to ﬁnd a compact, lower-dimensional representation',\n"," 'rd, which is often easier to analyze than',\n"," 'of high-dimensional data x',\n"," 'the original data. unlike regression, dimensionality reduction is only con-',\n"," 'cerned about modeling the data – there are no labels associated with a',\n"," 'data point x.',\n"," '∈',\n"," 'in chapter 11, we will move to our third pillar: density estimation. the',\n"," 'objective of density estimation is to ﬁnd a probability distribution that de-']"]},"metadata":{},"execution_count":539}]},{"cell_type":"code","source":["textBook_wthtStopWords = []\n","\n","for line in expanded_text:\n","  l = remove_stopwords(line)\n","  textBook_wthtStopWords.append(l)"],"metadata":{"id":"b48Y0-J5T3O8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["textBook_wthtStopWords[1000:1020]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o-ljUqNmXZr6","executionInfo":{"status":"ok","timestamp":1660001663887,"user_tz":300,"elapsed":8,"user":{"displayName":"Noé Lozano","userId":"09597272047670713906"}},"outputId":"3088adc3-514a-4e90-f2df-8c3ae0e727cb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['objective ﬁnd functions map inputs x',\n"," 'r, interpret labels',\n"," 'served function values y',\n"," 'respective inputs. discuss classical model ﬁtting (parameter esti-',\n"," 'mation) maximum likelihood maximum posteriori estimation,',\n"," 'bayesian linear regression, integrate parameters',\n"," 'instead optimizing them.',\n"," '∈',\n"," '∈',\n"," 'chapter 10 focuses dimensionality reduction, second pillar fig-',\n"," 'ure 1.1, principal component analysis. key objective dimen-',\n"," 'sionality reduction ﬁnd compact, lower-dimensional representation',\n"," 'rd, easier analyze',\n"," 'high-dimensional data x',\n"," 'original data. unlike regression, dimensionality reduction con-',\n"," 'cerned modeling data – labels associated',\n"," 'data point x.',\n"," '∈',\n"," 'chapter 11, pillar: density estimation.',\n"," 'objective density estimation ﬁnd probability distribution de-']"]},"metadata":{},"execution_count":541}]},{"cell_type":"code","source":["def remove_punctuations(data):\n","    punct_tag=re.compile(r'[^\\w\\s]')\n","    data=punct_tag.sub(r'',data)\n","    return data\n","\n","#Removes HTML syntaxes\n","def remove_html(data):\n","    html_tag=re.compile(r'<.*?>')\n","    data=html_tag.sub(r'',data)\n","    return data\n","\n","#Removes URL data\n","def remove_url(data):\n","    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n","    data=url_clean.sub(r'',data)\n","    return data\n","\n","def remove_tex(data):\n","    url_clean= re.compile(r\"(\\$+)(?:(?!\\1)[\\s\\S])*\\1\")\n","    data=url_clean.sub(r'',data)\n","    return data\n","\n","def remove_spaces(data):\n","    url_clean= re.compile(r\"\\s+\")\n","    data=url_clean.sub(r'',data)\n","    return data \n","\n","def remove_noisyWords(data):\n","    l = data\n","    l = l.replace('_',' ')\n","    l = l.replace('http','')\n","    l = l.replace('php','')\n","    l = l.replace('chapter','')\n","    l = l.replace('page','')\n","    l = l.replace('enwikibooksorgwikicalculusflhfpitalszsrule','')\n","    l = l.replace('wikibooks','')\n","    l = l.replace('titleuser','')\n","    l = l.replace('httpcreativecommonsorg','')\n","    l = l.replace('license','')\n","    l = l.replace('orgwindex','')\n","    l = l.replace('eee','')\n","    l = re.sub(r'\\b\\w\\b', '', l)\n","    l = l.replace('fx','')\n","    l = l.replace('fy','')\n","    l = l.replace('fz','')\n","    return l "],"metadata":{"id":"evWYcxWH9g_x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["textBook_punctTrtmnt = []\n","\n","for line in textBook_wthtStopWords:\n","  l = remove_punctuations(line)\n","  textBook_punctTrtmnt.append(l)"],"metadata":{"id":"ZLfVZFtK-nqw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["textBook_punctTrtmnt[1000:1020]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x_WLuZOgCFno","executionInfo":{"status":"ok","timestamp":1660001664079,"user_tz":300,"elapsed":4,"user":{"displayName":"Noé Lozano","userId":"09597272047670713906"}},"outputId":"014a4bed-93df-46a0-dbe3-85b396394350"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['objective ﬁnd functions map inputs x',\n"," 'r interpret labels',\n"," 'served function values y',\n"," 'respective inputs discuss classical model ﬁtting parameter esti',\n"," 'mation maximum likelihood maximum posteriori estimation',\n"," 'bayesian linear regression integrate parameters',\n"," 'instead optimizing them',\n"," '',\n"," '',\n"," 'chapter 10 focuses dimensionality reduction second pillar fig',\n"," 'ure 11 principal component analysis key objective dimen',\n"," 'sionality reduction ﬁnd compact lowerdimensional representation',\n"," 'rd easier analyze',\n"," 'highdimensional data x',\n"," 'original data unlike regression dimensionality reduction con',\n"," 'cerned modeling data  labels associated',\n"," 'data point x',\n"," '',\n"," 'chapter 11 pillar density estimation',\n"," 'objective density estimation ﬁnd probability distribution de']"]},"metadata":{},"execution_count":544}]},{"cell_type":"code","source":["textBook_wththtml = []\n","\n","for line in textBook_punctTrtmnt:\n","  l = remove_html(line)\n","  textBook_wththtml.append(l)"],"metadata":{"id":"OxAJZzBxDzSp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["textBook_wththtml[1000:1020]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Y0xCZLHFA7W","executionInfo":{"status":"ok","timestamp":1660001664305,"user_tz":300,"elapsed":4,"user":{"displayName":"Noé Lozano","userId":"09597272047670713906"}},"outputId":"831cf7ce-892b-4996-b8fc-3c98aeab4600"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['objective ﬁnd functions map inputs x',\n"," 'r interpret labels',\n"," 'served function values y',\n"," 'respective inputs discuss classical model ﬁtting parameter esti',\n"," 'mation maximum likelihood maximum posteriori estimation',\n"," 'bayesian linear regression integrate parameters',\n"," 'instead optimizing them',\n"," '',\n"," '',\n"," 'chapter 10 focuses dimensionality reduction second pillar fig',\n"," 'ure 11 principal component analysis key objective dimen',\n"," 'sionality reduction ﬁnd compact lowerdimensional representation',\n"," 'rd easier analyze',\n"," 'highdimensional data x',\n"," 'original data unlike regression dimensionality reduction con',\n"," 'cerned modeling data  labels associated',\n"," 'data point x',\n"," '',\n"," 'chapter 11 pillar density estimation',\n"," 'objective density estimation ﬁnd probability distribution de']"]},"metadata":{},"execution_count":546}]},{"cell_type":"code","source":["textBook_wthtTex = []\n","\n","for line in textBook_wththtml:\n","  l = remove_tex(line)\n","  textBook_wthtTex.append(l)"],"metadata":{"id":"JGKDESSHFBG6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["textBook_wthtTex[1000:1020]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EXrPdfEqFCPr","executionInfo":{"status":"ok","timestamp":1660001664529,"user_tz":300,"elapsed":15,"user":{"displayName":"Noé Lozano","userId":"09597272047670713906"}},"outputId":"0ea94929-a371-4884-cc7a-5911e437d980"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['objective ﬁnd functions map inputs x',\n"," 'r interpret labels',\n"," 'served function values y',\n"," 'respective inputs discuss classical model ﬁtting parameter esti',\n"," 'mation maximum likelihood maximum posteriori estimation',\n"," 'bayesian linear regression integrate parameters',\n"," 'instead optimizing them',\n"," '',\n"," '',\n"," 'chapter 10 focuses dimensionality reduction second pillar fig',\n"," 'ure 11 principal component analysis key objective dimen',\n"," 'sionality reduction ﬁnd compact lowerdimensional representation',\n"," 'rd easier analyze',\n"," 'highdimensional data x',\n"," 'original data unlike regression dimensionality reduction con',\n"," 'cerned modeling data  labels associated',\n"," 'data point x',\n"," '',\n"," 'chapter 11 pillar density estimation',\n"," 'objective density estimation ﬁnd probability distribution de']"]},"metadata":{},"execution_count":548}]},{"cell_type":"code","source":["textBook_wthtURL = []\n","\n","for line in textBook_wthtTex:\n","  l = remove_url(line)\n","  textBook_wthtURL.append(l)"],"metadata":{"id":"qYTNysDQFCyv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["textBook_wthtURL[1000:1020]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NjqsU1xcFC8R","executionInfo":{"status":"ok","timestamp":1660001664530,"user_tz":300,"elapsed":14,"user":{"displayName":"Noé Lozano","userId":"09597272047670713906"}},"outputId":"5427ad45-ea5c-4b1d-c43e-e254f8058c38"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['objective ﬁnd functions map inputs x',\n"," 'r interpret labels',\n"," 'served function values y',\n"," 'respective inputs discuss classical model ﬁtting parameter esti',\n"," 'mation maximum likelihood maximum posteriori estimation',\n"," 'bayesian linear regression integrate parameters',\n"," 'instead optimizing them',\n"," '',\n"," '',\n"," 'chapter 10 focuses dimensionality reduction second pillar fig',\n"," 'ure 11 principal component analysis key objective dimen',\n"," 'sionality reduction ﬁnd compact lowerdimensional representation',\n"," 'rd easier analyze',\n"," 'highdimensional data x',\n"," 'original data unlike regression dimensionality reduction con',\n"," 'cerned modeling data  labels associated',\n"," 'data point x',\n"," '',\n"," 'chapter 11 pillar density estimation',\n"," 'objective density estimation ﬁnd probability distribution de']"]},"metadata":{},"execution_count":550}]},{"cell_type":"code","source":["textBook_wthtnum = []\n","\n","for line in textBook_wthtURL:\n","  remove_digits = str.maketrans('', '', digits)\n","  res = line.translate(remove_digits)\n","  textBook_wthtnum.append(res)"],"metadata":{"id":"bjPFYj2K14T5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["textBook_wthtnum[2010:2030]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tVJJ7Yys18s4","executionInfo":{"status":"ok","timestamp":1660001664531,"user_tz":300,"elapsed":10,"user":{"displayName":"Noé Lozano","userId":"09597272047670713906"}},"outputId":"1829cce7-9bd6-4152-d053-e4b6fd213cae"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['',\n"," '',\n"," '',\n"," '',\n"," '',\n"," '',\n"," 'aa   aa',\n"," 'ab  ba',\n"," 'a  b',\n"," ' a  b',\n"," 'acidcid ',\n"," 'a  bcid  acid  bcid',\n"," 'abcid  bcidacid',\n"," '',\n"," 'right nown symmetric',\n"," 'symmetric matrix',\n"," 'square matrix',\n"," 'deﬁnition  symmetric matrix matrix',\n"," ' acid',\n"," '']"]},"metadata":{},"execution_count":552}]},{"cell_type":"code","source":["#CalcWikibook_wthtExtraSpaces = []\n","#for line in CalcWikibook_wthtnum:\n","  #l = remove_spaces(line)\n","  #CalcWikibook_wthtExtraSpaces.append(l)"],"metadata":{"id":"EQcb2tSy4WTs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#CalcWikibook_wthtExtraSpaces"],"metadata":{"id":"bbhGwsEV4aBP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["textBook_wthtNoisyWords = []\n","\n","for line in textBook_wthtnum:\n","  res = remove_noisyWords(line)\n","  textBook_wthtNoisyWords.append(res)"],"metadata":{"id":"obhbn7y3M-mA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["textBook_wthtNoisyWords[0:40]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ORDSOK9UNTEI","executionInfo":{"status":"ok","timestamp":1660001664712,"user_tz":300,"elapsed":5,"user":{"displayName":"Noé Lozano","userId":"09597272047670713906"}},"outputId":"51315a27-81ba-4d5d-9971-fc5d90df07e6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['mathematics machine learningmarc peter deisenrotha aldo faisalcheng soon ongmathematics machine learningdeisenroth et althe fundamental mathematical tools needed understand machine learning include linear algebra analytic geometry matrix decompositions vector calculus optimization probability statistics topics traditionally taught disparate courses making hard data science science students professionals efﬁ ciently learn mathematics selfcontained textbook bridges gap mathematical machine learning texts introducing mathematical concepts minimum prerequisites uses concepts derive central machine learning methods linear regression principal component analysis gaussian mixture models support vector machines students mathematical background derivations provide starting point machine learning texts learning mathematics  rst time methods help build intuition practical experience applying mathematical concepts  includes worked examples exercises test understanding programming tutorials offered books web sitemarc peter deisenroth senior lecturer statistical machine learning department computing împerial college londona aldo faisal leads brain  behaviour lab imperial college london reader neurotechnology department bioengineering department computingcheng soon ong principal research scientist machine learning research group data csiro adjunct associate professor australian national universitycover image courtesy daniel bosma  moment  getty imagescover design holly johnsondeisenrith et al  cover     contents',\n"," 'foreword',\n"," 'mathematical foundations',\n"," '',\n"," '',\n"," '',\n"," '',\n"," 'introduction motivation',\n"," 'finding words intuitions',\n"," 'ways read book',\n"," 'exercises feedback',\n"," 'linear algebra',\n"," 'systems linear equations',\n"," '',\n"," '',\n"," ' matrices',\n"," '',\n"," '',\n"," '',\n"," '',\n"," '',\n"," '',\n"," '',\n"," 'solving systems linear equations',\n"," 'vector spaces',\n"," 'linear independence',\n"," 'basis rank',\n"," 'linear mappings',\n"," 'afﬁne spaces',\n"," 'reading',\n"," 'exercises',\n"," 'analytic geometry',\n"," 'inner products',\n"," 'lengths distances',\n"," 'angles orthogonality',\n"," '',\n"," ' norms',\n"," '',\n"," '',\n"," '']"]},"metadata":{},"execution_count":556}]},{"cell_type":"code","source":["while(\"\" in textBook_wthtNoisyWords) :\n","    textBook_wthtNoisyWords.remove(\"\")"],"metadata":{"id":"HExhaqwuV6i9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["textBook_wthtNoisyWords[2200:2300]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rvEaJ8vtWuTv","executionInfo":{"status":"ok","timestamp":1660001672605,"user_tz":300,"elapsed":12,"user":{"displayName":"Noé Lozano","userId":"09597272047670713906"}},"outputId":"a91aba02-b30e-48f1-bd2a-937aef18eec0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['represent  ie linear combinations vectors      xk',\n"," 'coefﬁcients λi  ',\n"," '      xk',\n"," 'deﬁnition  linear independence let consider vector space',\n"," '  nontrivial linear com',\n"," ' ',\n"," 'bination   cidk',\n"," '  vectors',\n"," '     xk linearly dependent trivial solution exists ie',\n"," '      λk   vectors      xk linearly independent',\n"," ' λixi λi',\n"," 'linear independence important concepts linear',\n"," 'algebra intuitively set linearly independent vectors consists vectors',\n"," 'redundancy ie remove vectors',\n"," 'set lose something sections',\n"," 'formalize intuition more',\n"," 'example  linearly dependent vectors',\n"," 'geographic example help clari concept linear indepen',\n"," 'dence person nairobi kenya describing kigali rwanda',\n"," 'you kigali ﬁrst going  northwest kam',\n"," 'pala uganda  southwest sufﬁcient information',\n"," 'draft  mathematics machine learning feedback smmlbookcom',\n"," 'linearly dependent',\n"," 'linearly',\n"," 'independent',\n"," 'cid',\n"," ' linear independence',\n"," 'location kigali geographic coordinate sys',\n"," 'tem considered twodimensional vector space ignoring altitude',\n"," 'earths curved surface person add it ',\n"," 'west here statement true necessary',\n"," 'ﬁnd kigali given previous information see figure  illus',\n"," 'tration example  northwest vector blue',\n"," ' southwest vector purple linearly independent means',\n"," 'southwest vector described terms northwest vec',\n"," 'tor vice versa however  west vector black',\n"," 'linear combination vectors makes set vec',\n"," 'tors linearly dependent equivalently given  west ',\n"," 'southwest linearly combined obtain  northwest',\n"," 'figure ',\n"," 'geographic example',\n"," 'with crude',\n"," 'approximations',\n"," 'cardinal directions',\n"," 'linearly',\n"," 'dependent vectors',\n"," 'twodimensional',\n"," 'space plane',\n"," 'remark following properties useful ﬁnd vectors',\n"," 'linearly independent',\n"," '     xk  xi',\n"," '        ',\n"," ' vectors linearly dependent linearly independent',\n"," 'option',\n"," 'vectors      xk  linearly de',\n"," 'pendent holds vectors identical',\n"," '  cid  linearly',\n"," 'vectors',\n"," 'dependent at least linear combination',\n"," 'others particular vector multiple vector',\n"," 'ie xi  λxj ',\n"," 'linearly dependent',\n"," 'practical way checking vectors      xk',\n"," ' linearly',\n"," 'independent use gaussian elimination write vectors columns',\n"," 'matrix perform gaussian elimination matrix',\n"," 'row echelon form the reduced rowechelon form unnecessary here',\n"," '        ',\n"," ' set',\n"," '     xk  xi',\n"," '   deisenroth   faisal   ong published cambridge university press ',\n"," 'kmnorthwestkmwestkmsouthwestkmsouthwestkampalanairobikigalicid',\n"," 'cid',\n"," 'linear algebra',\n"," ' pivot columns indicate vectors linearly indepen',\n"," 'dent vectors left note ordering vec',\n"," 'tors matrix built',\n"," ' nonpivot columns expressed linear combinations',\n"," 'pivot columns left instance rowechelon form',\n"," 'cid  ',\n"," '  ',\n"," 'cid',\n"," 'tells ﬁrst columns pivot columns sec',\n"," 'ond column nonpivot column times ﬁrst',\n"," 'column',\n"," 'column vectors linearly independent columns',\n"," 'pivot columns nonpivot column columns',\n"," 'and therefore corresponding vectors linearly dependent',\n"," 'example ',\n"," 'consider ',\n"," ' ',\n"," '  ',\n"," '  ',\n"," 'check linearly dependent follow general ap',\n"," 'proach solve',\n"," 'λx  λx  λx  ',\n"," ' ',\n"," ' ',\n"," ' ',\n"," '      write vectors xi     columns']"]},"metadata":{},"execution_count":558}]},{"cell_type":"code","source":["finalTxt = ' '.join(textBook_wthtNoisyWords)"],"metadata":{"id":"mfd4yT8EdqMm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["finalTxt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"id":"QsaKP5DkekMn","executionInfo":{"status":"ok","timestamp":1660001673010,"user_tz":300,"elapsed":415,"user":{"displayName":"Noé Lozano","userId":"09597272047670713906"}},"outputId":"cab8cf63-b2e6-4624-bc59-25d2e419ab14"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'mathematics machine learningmarc peter deisenrotha aldo faisalcheng soon ongmathematics machine learningdeisenroth et althe fundamental mathematical tools needed understand machine learning include linear algebra analytic geometry matrix decompositions vector calculus optimization probability statistics topics traditionally taught disparate courses making hard data science science students professionals efﬁ ciently learn mathematics selfcontained textbook bridges gap mathematical machine learning texts introducing mathematical concepts minimum prerequisites uses concepts derive central machine learning methods linear regression principal component analysis gaussian mixture models support vector machines students mathematical background derivations provide starting point machine learning texts learning mathematics  rst time methods help build intuition practical experience applying mathematical concepts  includes worked examples exercises test understanding programming tutorials offered books web sitemarc peter deisenroth senior lecturer statistical machine learning department computing împerial college londona aldo faisal leads brain  behaviour lab imperial college london reader neurotechnology department bioengineering department computingcheng soon ong principal research scientist machine learning research group data csiro adjunct associate professor australian national universitycover image courtesy daniel bosma  moment  getty imagescover design holly johnsondeisenrith et al  cover     contents foreword mathematical foundations introduction motivation finding words intuitions ways read book exercises feedback linear algebra systems linear equations  matrices solving systems linear equations vector spaces linear independence basis rank linear mappings afﬁne spaces reading exercises analytic geometry inner products lengths distances angles orthogonality  norms  orthonormal basis  orthogonal complement  orthogonal projections rotations  reading exercises inner product functions matrix decompositions determinant trace material published cambridge university press mathematics machine learning marc peter deisenroth  aldo faisal cheng soon ong  version free view download personal use only redistribution resale use derivative works by   deisenroth   faisal   ong  smmlbookcom ii contents eigenvalues eigenvectors cholesky decomposition eigendecomposition diagonalization singular value decomposition  matrix approximation  matrix phylogeny reading exercises vector calculus differentiation univariate functions partial differentiation gradients gradients vectorvalued functions gradients matrices useful identities computing gradients backpropagation automatic differentiation  higherorder derivatives linearization multivariate taylor series reading exercises probability distributions construction probability space discrete continuous probabilities sum rule product rule bayes theorem summary statistics independence gaussian distribution conjugacy exponential family change variablesinverse transform reading exercises continuous optimization  optimization gradient descent constrained optimization lagrange multipliers convex optimization reading exercises ii central machine learning problems models meet data data models learning empirical risk minimization parameter estimation probabilistic modeling inference directed graphical models draft  mathematics machine learning feedback smmlbookcom contents  model selection linear regression problem formulation parameter estimation bayesian linear regression  maximum likelihood orthogonal projection reading  problem setting  maximum variance perspective  projection perspective  eigenvector computation lowrank approximations  pca high dimensions  key steps pca practice  latent variable perspective  reading dimensionality reduction principal component analysis  density estimation gaussian mixture models  gaussian mixture model  parameter learning maximum likelihood  algorithm  latentvariable perspective  reading classiﬁcation support vector machines  separating hyperplanes  primal support vector machine  dual support vector machine  kernels  numerical solution  reading references iii    deisenroth   faisal   ong published cambridge university press  foreword machine learning latest long line attempts distill human knowledge reasoning form suitable constructing ma chines engineering automated systems machine learning ubiquitous software packages easier use nat ural desirable lowlevel technical details abstracted away hidden practitioner however brings danger practitioner unaware design decisions and hence limits machine learning algorithms enthusiastic practitioner interested learn magic successful machine learning algorithms currently faces daunting set prerequisite knowledge programming languages data analysis tools largescale computation associated frameworks mathematics statistics machine learning builds universities introductory courses machine learning tend spend early parts course covering prerequisites histori cal reasons courses machine learning tend taught science department students trained ﬁrst areas knowledge mathematics statistics current machine learning textbooks primarily focus machine learn ing algorithms methodologies assume reader com petent mathematics statistics therefore books spend  background mathematics beginning book appendices people want delve foundations basic machine learning methods strug gle mathematical knowledge required read machine learning textbook having taught undergraduate graduate courses universi ties ﬁnd gap high school mathematics math ematics level required read standard machine learning textbook big people book brings mathematical foundations basic machine learn ing concepts fore collects information single place skills gap narrowed closed material published cambridge university press mathematics machine learning marc peter deisenroth  aldo faisal cheng soon ong  version free view download personal use only redistribution resale use derivative works by   deisenroth   faisal   ong  smmlbookcom math linked popular mind phobia anxiety think discussing spiders strogatz    foreword book machine learning machine learning builds language mathematics express concepts intuitively obvious surprisingly difﬁcult formalize formalized properly gain insights task want solve common complaint students mathematics globe topics covered little relevance practical problems believe machine learning obvious direct motivation people learn mathematics book intended guidebook vast mathematical lit erature forms foundations modern machine learning mo tivate need mathematical concepts directly pointing usefulness context fundamental machine learning problems keeping book short details advanced concepts left out equipped basic concepts presented here ﬁt larger context machine learning reader ﬁnd numerous resources study provide end respective  readers mathematical back ground book provides brief precisely stated glimpse machine learning contrast books focus methods models machine learning mackay  bishop  alpaydin  bar ber  murphy  shalevshwartz bendavid  rogers girolami  programmatic aspects machine learning muller guido  raschka mirjalili  chollet allaire  provide representative examples machine learning algo rithms instead focus mathematical concepts models themselves hope readers able gain deeper understand ing basic questions machine learning connect practical ques tions arising use machine learning fundamental choices mathematical model aim write classical machine learning book instead intention provide mathematical background applied cen tral machine learning problems easier read machine learning textbooks target audience applications machine learning widespread society believe everybody understanding underlying principles book written academic mathematical style enables precise concepts machine learning encourage readers unfamiliar seemingly terse style persevere goals topic mind sprinkle comments remarks text hope provides useful guidance respect big picture book assumes reader mathematical knowledge commonly draft  mathematics machine learning feedback smmlbookcom foreword covered high school mathematics physics example reader seen derivatives integrals before geometric vectors dimensions starting there generalize con cepts therefore target audience book includes undergraduate university students evening learners learners participating online machine learning courses analogy music types interaction people machine learning astute listener democratization machine learning pro vision opensource software online tutorials cloudbased tools al lows users worry speciﬁcs pipelines users focus extracting insights data offtheshelf tools enables non techsavvy domain experts beneﬁt machine learning sim ilar listening music user able choose discern different types machine learning beneﬁts it experi enced users like music critics asking important questions application machine learning society ethics fairness pri vacy individual hope book provides foundation thinking certiﬁcation risk management machine learning systems allows use domain expertise build better machine learning systems experienced artist skilled practitioners machine learning plug play different tools libraries analysis pipeline stereo typical practitioner data scientist engineer understands machine learning interfaces use cases able perform wonderful feats prediction data similar virtuoso play ing music highly skilled practitioners bring existing instru ments life bring enjoyment audience mathe matics presented primer practitioners able under stand beneﬁts limits favorite method extend generalize existing machine learning algorithms hope book provides impetus rigorous principled development machine learning methods fledgling composer machine learning applied new domains developers machine learning need develop new methods extend existing algorithms researchers need understand mathematical basis machine learning uncover relationships be tween different tasks similar composers music who rules structure musical theory create new amazing pieces hope book provides highlevel overview technical books people want composers machine learning great need society new researchers able propose explore novel approaches attacking challenges learning data    deisenroth   faisal   ong published cambridge university press  foreword acknowledgments grateful people looked early drafts book suffered painful expositions concepts tried imple ment ideas vehemently disagree with like especially acknowledge christfried webers careful reading parts book detailed suggestions structure presentation friends colleagues kind provide time energy different versions  lucky beneﬁt generosity online commu nity suggested improvements sgithubcom greatly improved book following people bugs proposed clariﬁcations sug gested relevant literature sgithubcom personal communication names sorted alphabetically abdulganiy usman adam gaier adele jackson aditya menon alasdair tran aleksandar krnjaic alexander makrigiorgos alfredo canziani ali shafti amr khalifa andrew tanggara angus gruen antal  buss antoine toisoul le cann areg sarvazyan artem artemev artyom stepanov kromydas bob williamson boon ping lim chao qu cheng li chris sherlock christopher gray daniel mcnamara daniel wood darren siegel david johnston dawei chen ellen broad fengkuangtian zhu fiona condon georgios theodorou xin irene raissa kameni jakub nabaglo james hensman jamie liu jean kaddour jeanpaul ebejer jerry qiang jitesh sindhare john lloyd jonas ngnawe jon martin justin hsi kai arulkumaran kamil dreczkowski lily wang lionel tondji ngoupeyou lydia knyouﬁng mahmoud aslan mark hartenstein mark van der wilk markus hegland martin hewing matthew alger matthew lee draft  mathematics machine learning feedback smmlbookcom foreword maximus mccann mengyan zhang michael bennett michael pedersen minjeong shin mohammad malekzadeh naveen kumar nico montali oscar armas patrick henriksen patrick wieschollek pattarawat chormai paul kelly petros christodoulou piotr januszewski pranav subramani quyu kong ragib zaman rui zhang ryanrhys grifﬁths salomon kabongo samuel ogunmola sandeep mavadia sarvesh nikumbh sebastian raschka senanayak sesh kumar karri seungheon baek shahbaz chaudhary shakir mohamed shawn berry sheikh abdul raheem ali sheng xue sridhar thiagarajan syed nouman hasany szymon brych thomas buhler timur sharapov tom melamed vincent adam vincent dutordoir vu minh wasim aftab wen zhi wojciech stokowiec xiaonan chong xiaowei zhang yazhou hao yicheng luo young lee yu lu yun cheng yuxiao huang zac cranko zijian cao zoe nolan contributors github real names listed github proﬁle are samdatamad bumptiousmonkey idoamihai deepakiim insad horizonp csmaillist kudo empet victorbigand skye jessjing grateful parameswaran raman anony mous reviewers organized cambridge university press read  earlier versions manuscript provided con structive criticism led considerable improvements special men tion goes dinesh singh negi latex support detailed prompt advice latexrelated issues least grateful editor lauren cowles patiently guiding gestation process book    deisenroth   faisal   ong published cambridge university press  foreword table symbols cid    typical meaning scalars lowercase vectors bold lowercase matrices bold uppercase transpose vector matrix inverse matrix inner product   dot product   ordered tuple symbol             xcid acid   cid xcidy           matrix column vectors stacked horizontally set vectors unordered integers natural numbers respectively real complex numbers respectively ndimensional vector space real numbers universal quantiﬁer  existential quantiﬁer exists  deﬁned   deﬁned proportional  ie  constant function composition    implies sets element set set     right     ab mn mn ei dim rka amφ kerφ spanb tra deta    cidcid eλ  set elements number dimensions indexed         number data points indexed         identity matrix size  matrix zeros size  matrix ones size  standardcanonical vector where component  dimensionality vector space rank matrix image linear mapping  kernel null space linear mapping  span generating set  trace determinant absolute value determinant depending context norm euclidean speciﬁed eigenvalue lagrange multiplier eigenspace corresponding eigenvalue  draft  mathematics machine learning feedback smmlbookcom foreword symbol   cidn cidn  xn  xn df dx   minx   cid cidn vxx exx covxy   cidµ σcid berµ binn  betaα  typical meaning vectors   orthogonal vector space orthogonal complement vector space  sum xn       xn product xn  xn parameter vector partial derivative  respect  total derivative  respect  gradient smallest function value     lagrangian negative loglikelihood binomial coefﬁcient  choose  variance  respect random variable  expectation  respect random variable  covariance    conditionally independent  given  random variable  distributed according  gaussian distribution mean  covariance  bernoulli distribution parameter  binomial distribution parameters   beta distribution parameters   arg minx   value  minimizes  note arg min returns set values table abbreviations acronyms acronym meaning eg gmm ie iid map mle onb pca ppca ref spd svm exempli gratia latin example gaussian mixture model id est latin means independent identically distributed maximum posteriori maximum likelihood estimationestimator orthonormal basis principal component analysis probabilistic principal component analysis rowechelon form symmetric positive deﬁnite support vector machine    deisenroth   faisal   ong published cambridge university press  mathematical foundations material published cambridge university press mathematics machine learning marc peter deisenroth  aldo faisal cheng soon ong  version free view download personal use only redistribution resale use derivative works by   deisenroth   faisal   ong  smmlbookcom introduction motivation machine learning designing algorithms automatically extract valuable information data emphasis automatic ie machine learning concerned generalpurpose methodologies applied datasets producing mean ingful concepts core machine learning data model learning data machine learning inherently data driven data core machine learning goal machine learning design general purpose methodologies extract valuable patterns data ideally domainspeciﬁc expertise example given large corpus documents eg books libraries machine learning methods automatically ﬁnd relevant topics shared documents hoffman et al  achieve goal design mod els typically related process generates data similar model dataset given example regression setting model function maps inputs realvalued outputs paraphrase mitchell  model said learn data per formance given task improves data taken account goal ﬁnd good models generalize unseen data care future learning understood way automatically ﬁnd patterns structure data optimizing parameters model learning machine learning seen success stories software readily available design train rich ﬂexible machine learning systems believe mathematical foundations machine learn ing important order understand fundamental principles complicated machine learning systems built understand ing principles facilitate creating new machine learning solutions understanding debugging existing approaches learning inherent assumptions limitations methodologies work ing with material published cambridge university press mathematics machine learning marc peter deisenroth  aldo faisal cheng soon ong  version free view download personal use only redistribution resale use derivative works by   deisenroth   faisal   ong  smmlbookcom predictor training data vectors model learning introduction motivation  finding words intuitions challenge face regularly machine learning concepts words slippery particular component machine learning abstracted different mathematical concepts example word algorithm different senses con text machine learning ﬁrst sense use phrase machine learning algorithm mean makes predictions based in data refer algorithms predictors second sense use exact phrase machine learning algorithm mean adapts internal parameters predictor performs future unseen input data refer adapta tion training system book resolve issue ambiguity want high light upfront that depending context expressions mean different things however attempt context sufﬁ ciently clear reduce level ambiguity ﬁrst book introduces mathematical concepts foundations needed talk main components machine learning system data models learning brieﬂy outline components here revisit   discussed necessary mathematical concepts data numerical useful consider data number format book assume data appropriately converted numerical representation suitable read ing program therefore think data vectors illustration subtle words are at least different ways think vectors vector array numbers  science view vector arrow direction magni tude  physics view vector object obeys addition scaling  mathematical view model typically process generating data sim ilar dataset hand therefore good models thought simpliﬁed versions real unknown datagenerating process capturing aspects relevant modeling data extracting hidden patterns it good model predict happen real world performing realworld experi ments come crux matter learning component machine learning assume given dataset suitable model training model means use data available optimize pa rameters model respect utility function evaluates model predicts training data training methods thought approach analogous climbing hill reach peak analogy peak hill corresponds maximum draft  mathematics machine learning feedback smmlbookcom  ways read book desired performance measure however practice interested model perform unseen data performing data seen training data mean good way memorize data however generalize unseen data and practical applications need expose machine learning situations encountered before let summarize main concepts machine learning cover book represent data vectors choose appropriate model probabilistic opti mization view learn available data numerical optimization methods aim model performs data training  ways read book consider strategies understanding mathematics machine learning bottomup building concepts foundational ad vanced preferred approach technical ﬁelds mathematics strategy advantage reader times able rely previously learned concepts unfor tunately practitioner foundational concepts particularly interesting themselves lack motivation means foundational deﬁnitions quickly forgotten topdown drilling practical needs basic require ments goaldriven approach advantage readers know times need work particular concept clear path required knowledge downside strat egy knowledge built potentially shaky foundations readers remember set words way understanding decided write book modular way separate foundational mathematical concepts applications book read ways book split parts lays math ematical foundations ii applies concepts set fundamental machine learning problems form pillars machine learning illustrated figure  regression dimensionality reduction density estimation classiﬁcation  build previous ones possible skip  work backward necessary  ii loosely coupled read order pointers forward backward    deisenroth   faisal   ong published cambridge university press  figure  foundations pillars machine learning linear algebra analytic geometry matrix decomposition introduction motivation parts book link mathematical concepts machine learning algorithms course ways read book readers learn combination topdown bottomup approaches some times building basic mathematical skills attempting com plex concepts choosing topics based applications machine learning mathematics pillars machine learning cover book see figure  require solid mathematical foundation laid  represent numerical data vectors represent table data matrix study vectors matrices called linear algebra introduce   collection vectors matrix described there given vectors representing objects real world want statements similarity idea vectors similar predicted similar outputs machine learning algorithm our predictor formalize idea similarity be tween vectors need introduce operations vectors input return numerical value representing similarity con struction similarity distances central analytic geometry discussed     introduce fundamental concepts matri ces matrix decomposition operations matrices extremely useful machine learning allow intuitive interpretation data efﬁcient learning consider data noisy observations true underly ing signal hope applying machine learning identi signal noise requires language quanti ing noise means like predictors draft  mathematics machine learning feedback smmlbookcom classiﬁcation density estimation regression dimensionality reduction machine learningvector calculusprobability  distributionsoptimizationanalytic geometrymatrix decompositionlinear algebra  ways read book allow express sort uncertainty eg quanti conﬁ dence value prediction particular test data point quantiﬁcation uncertainty realm probability theory covered   train machine learning models typically ﬁnd parameters maximize performance measure optimization techniques re quire concept gradient tells direction search solution   vector calculus details concept gradients subsequently use   talk optimization ﬁnd maximaminima functions ii machine learning second book introduces pillars machine learning shown figure  illustrate mathematical concepts in troduced ﬁrst book foundation pillar broadly speaking  ordered difﬁculty in ascending order   restate components machine learning data models parameter estimation mathematical fashion addition provide guidelines building experimental setups guard overly optimistic evaluations machine learning sys tems recall goal build predictor performs unseen data   close look linear regression rd corresponding ob objective ﬁnd functions map inputs   interpret labels served function values  respective inputs discuss classical model ﬁtting parameter esti mation maximum likelihood maximum posteriori estimation bayesian linear regression integrate parameters instead optimizing them   focuses dimensionality reduction second pillar fig ure  principal component analysis key objective dimen sionality reduction ﬁnd compact lowerdimensional representation rd easier analyze highdimensional data  original data unlike regression dimensionality reduction con cerned modeling data  labels associated data point    pillar density estimation objective density estimation ﬁnd probability distribution de scribes given dataset focus gaussian mixture models purpose discuss iterative scheme ﬁnd parameters model dimensionality reduction labels associated rd however seek lowdimensional data points  representation data instead interested density model describes data   concludes book indepth discussion fourth    deisenroth   faisal   ong published cambridge university press  probability theory vector calculus optimization linear regression dimensionality reduction density estimation classiﬁcation introduction motivation pillar classiﬁcation discuss classiﬁcation context support vector machines similar regression   inputs  corresponding labels  however unlike regression labels realvalued labels classiﬁcation integers requires special care  exercises feedback provide exercises  pen paper ii provide programming tutorials jupyter notebooks explore properties machine learning algorithms discuss book appreciate cambridge university press strongly supports aim democratize education learning making book freely available download smmlbookcom tutorials errata additional materials found mistakes reported feedback provided preceding url draft  mathematics machine learning feedback smmlbookcom linear algebra algebra formalizing intuitive concepts common approach construct set objects symbols set rules manipulate objects known algebra linear algebra study vectors certain rules manipulate vectors vectors know school called geometric vectors usually denoted small arrow letter eg    book discuss general concepts vectors use bold letter represent them eg   general vectors special objects added multiplied scalars produce object kind abstract mathematical viewpoint object satisﬁes properties considered vector examples vector objects  geometric vectors example vector familiar high school mathematics physics geometric vectors  figure   directed segments drawn at dimen  added sions geometric vectors geometric vector furthermore multiplication scalar  geometric vector fact original vector scaled  therefore geometric vectors instances vector concepts introduced previously interpreting vectors geometric vec tors enables use intuitions direction magnitude reason mathematical operations        polynomials vectors figure  polynomials   figure  different types vectors vectors surprising objects including  geometric vectors  polynomials  geometric vectors  polynomials material published cambridge university press mathematics machine learning marc peter deisenroth  aldo faisal cheng soon ong  version free view download personal use only redistribution resale use derivative works by   deisenroth   faisal   ong  smmlbookcom xy  linear algebra added together results polynomial  result polynomial multiplied scalar  well therefore polynomials rather unusual instances vectors note polynomials different geometric vectors geometric vectors concrete drawings polynomials abstract concepts however vectors sense previously de scribed  audio signals vectors audio signals represented series numbers add audio signals together sum new audio signal scale audio signal obtain audio signal therefore audio signals type vector too  elements right tuples  real numbers vectors right abstract polynomials concept focus book instance right example triplet numbers adding vectors   right now moreover componentwise results vector     right now multiplying considering vectors elements right additional beneﬁt loosely corresponds arrays real numbers computer programming languages support array operations allow con venient implementation algorithms involve vector operations  results scaled vector λa right  linear algebra focuses similarities vector concepts add multiply scalars largely focus vectors right algorithms linear algebra for mulated right now   consider data represented vectors right now book focus ﬁnite dimensional vector spaces case  correspondence kind vector right now convenient use intuitions geometric vectors consider arraybased algorithms major idea mathematics idea closure ques tion set things result proposed oper ations case vectors set vectors result starting small set vectors adding scaling them results vector space section  concept vector space properties underlie machine learning concepts introduced  summarized figure   based lecture notes books drumm weil  strang  hogben  liesen mehrmann  pavel grinfelds linear algebra series excellent draft  mathematics machine learning feedback smmlbookcom careful check array operations actually perform vector operations implementing computer pavel grinfelds series linear algebra tinyurl comnahclwm gilbert strangs course linear algebra tinyurl compqj bluebrown series linear algebra stinyurl comhgkps  systems linear equations   vector calculus matrix abelian vector space group  ts        propertyof   oses vector linearafﬁne mapping linear equations solves matrix inverse gaussian elimination figure  mind map concepts introduced parts book linear independence basis   analytic geometry   classiﬁcation   dimensionality reduction resources gilbert strangs linear algebra course mit linear algebra series bluebrown linear algebra plays important role machine learning gen eral mathematics concepts introduced  ex panded include idea geometry     discuss vector calculus principled knowledge matrix op erations essential   use projections to intro duced section  dimensionality reduction principal compo nent analysis pca   discuss linear regression linear algebra plays central role solving leastsquares problems  systems linear equations systems linear equations play central linear algebra problems formulated systems linear equations linear algebra gives tools solving them example  company produces products      nn resources      rm required produce unit product nj aij units resource ri needed                objective ﬁnd optimal production plan ie plan units xj product nj produced total bi units resource ri available ideally resources left over produce      xn units corresponding products need    deisenroth   faisal   ong published cambridge university press  total linear algebra aix   ainxn    units resource ri optimal production plan      xn therefore satis following equations ax  amx   anxn    amnxn  bm       aij  bi linear equations solution equation  general form linear equations      xn unknowns system ntuple      xn right satisﬁes  solution linear equation system example  linear equations                 solution adding ﬁrst equations yields     contradicts equation  let look linear equations                  ﬁrst equation follows          ie        therefore    possible unique solution veri    solution plugging in example consider                  omit equation redundancy      free variable triplet     deﬁne   cid    cid draft  mathematics machine learning feedback smmlbookcom right now  systems linear equations figure  solution space linear equations variables geometrically interpreted intersection lines linear equation represents line solution linear equations ie obtain solution set contains inﬁnitely solutions general realvalued linear equations obtain no exactly one inﬁnitely solutions linear regression   solves version example  solve linear equations remark geometric interpretation systems linear equations linear equations variables   linear equation deﬁnes line xxplane solution linear equations satis equations simultaneously solution set intersection lines intersection set line if linear equations line point when lines parallel illustration given figure          solution space point        similarly variables linear equation determines plane threedimensional space intersect planes ie satis linear equations time obtain solution set plane line point when planes common intersection systematic approach solving systems linear equations introduce useful compact notation collect coefﬁcients aij vectors collect vectors matrices words write  following form am am an amn bm           xn     deisenroth   faisal   ong published cambridge university press  xxxxxx  am       an amn     bm xn linear algebra following close look matrices de ﬁne computation rules return solving linear equations sec tion   matrices matrices play central role linear algebra com pactly represent systems linear equations represent linear functions linear mappings later section  discuss interesting topics let ﬁrst deﬁne matrix kind operations matrices properties matrices    realvalued   matrix ntuple elements aij                ordered deﬁnition  matrix   according rectangular scheme consisting  rows  columns am am       an an amn aij    convention  nmatrices called rows  matrices called columns special matrices called rowcolumn vectors rmn set realvalued  nmatrices rmn rmn stacking  columns equivalently represented matrix long vector figure  matrix row column row vector column vector figure  stacking columns matrix represented long vector  reshape  matrix addition multiplication sum matrices wise sum ie rmn  rmn deﬁned element    rmn  an  bn amn  bmn      am  bm rmn        note size matrices   npeinsumil lj   matrices   ab rmk computed right nowk elements cij product cij  ailblj                cid draft  mathematics machine learning feedback smmlbookcom  matrices means compute element cij multiply elements ith row jth column  sum up later section  dot product corresponding row column cases need explicit performing multiplication  denote multiplication explicitly showing use notation remark matrices multiplied neighboring dimensions match instance  matrix  left side kmatrix multiplied   columns  rows compute ailblj         commonly dot product vectors   denoted acidb cida bcid cidcidcidcid nk cidcidcidcid km   cidcidcidcid nm product ba deﬁned  match   neighboring dimensions remark matrix multiplication deﬁned elementwise operation  aijbij even size   cho matrix elements ie cij sen appropriately kind elementwise multiplication appears programming languages multiply multidimensional arrays other called hadamard product example  cid cid          obtain ab  ba  cid cid        cid cid      cid cid             hadamard product example matrix multiplication commutative ie ab  ba figure  illustration deﬁnition  identity matrix right nown deﬁne identity matrix                                               right nown    deisenroth   faisal   ong published cambridge university press  figure  matrix multiplications ab ba deﬁned dimensions results different identity matrix cid cid cid associativity distributivity associativity distributivity nmatrix containing  diagonal  else deﬁned matrix multiplication matrix addition identity matrix let look properties matrices linear algebra rmn  right nowp  rpq  abc  abecause   rmn   right nowp    bc  ac   ac    ac  ad  multiplication identity matrix note       rmn  ma  ai   square matrix possesses number columns rows inverse regular invertible nonsingular singular noninvertible  inverse transpose right nown let matrix right nown property ab    ba  called deﬁnition  inverse consider square matrix inverse denoted  unfortunately matrix possesses inverse  inverse exist called regularinvertiblenonsingular singularnoninvertible matrix inverse exists unique sec tion  discuss general way compute inverse matrix solving linear equations remark existence inverse  cida    cid   matrix consider matrix multiply acid  cid  cid cidaa aa cid aa aa  aa aai  obtain aacid  therefore aa aa cid  cid   aa aa   section  aa draft  mathematics machine learning feedback smmlbookcom cid cid cid  matrices aa determinant  use determinant check matrix invertible matrix furthermore generally example  inverse matrix matrices              inverse ab   ba deﬁnition  transpose bij  aji called transpose  write   acid rmn matrix  right nowm general acid obtained writing columns rows acid following important properties inverses transposes transpose main diagonal sometimes called principal diagonal primary diagonal leading diagonal major diagonal matrix collection entries aij   scalar case     cid       aa   aa ab  ba        acidcid    bcid  acid  bcid abcid  bcidacid right nown symmetric symmetric matrix square matrix deﬁnition  symmetric matrix matrix  acid note  nmatrices symmetric generally  nmatrices square matrices possess num ber rows columns moreover invertible acid acid  acid  acid remark sum product symmetric matrices sum symmet right nown symmetric however ric matrices   product deﬁned generally symmetric cid    cid cid    cid    cid cid  multiplication scalar let look happens matrices multiplied  λa   kij   aij scalar   following holds practically  scales element    rmn   let    deisenroth   faisal   ong published cambridge university press  cid associativity distributivity linear algebra rmn associativity λψc  λψc  λbecause  λbc  bλc  becauseλ  note allows scalar values around λccid   cidλcid   cidλ  λc cid   λcid  distributivity   ψc  λc  ψc  λb    λb  λc   rmn rmn rmn  right nowk example  distributivity deﬁne   cid cid        ψc   obtain cid cidλ            cid   cid cid       cid cid             cid  λc  ψc   compact representations systems linear equations consider linear equations                 use rules matrix multiplication write equation compact form     note  scales ﬁrst column  second one  one generally linear equations compactly represented matrix form ax    product ax linear combination columns  discuss linear combinations section  draft  mathematics machine learning feedback smmlbookcom  solving systems linear equations  solving systems linear equations  introduced general form equation system ie ax   anxn   amx   amnxn  bm         bi  known constants xj unknowns aij                far saw matrices compact way formulating systems linear equations write ax    moreover deﬁned basic matrix operations addition multiplication matrices following focus solving systems linear equations provide algorithm ﬁnding inverse matrix  particular general solution discussing generally solve systems linear equations let look example consider equations cid      cid cid cid equations unknowns therefore general expect inﬁnitely solutions equations particularly easy form ﬁrst columns consist   remember want ﬁnd scalars       cid  xici   deﬁne ci ith column matrix  righthandside  solution problem  immediately taking  times ﬁrst column  times second column       cid cid cid cid cid cid therefore solution    cid solution called particular solution special solution however solution linear equations capture solutions need creative generating  nontrivial way columns matrix adding  special solution change special solution so express column ﬁrst columns which simple form cid cid cid cid cid     cid    deisenroth   faisal   ong published cambridge university press  particular solution special solution general solution linear algebra      fact scaling solution               produces  vector ie cid      cid  λc       following line reasoning express fourth column matrix  ﬁrst columns generate set nontrivial versions  cid      cid          putting together obtain solutions equation  called general solution set            remark general approach followed consisted following steps  particular solution ax    solutions ax    combine solutions steps   general solution general particular solution unique linear equations preceding example easy solve matrix  particularly convenient form allowed ﬁnd particular general solution in spection however general equation systems simple form fortunately exists constructive algorithmic way transforming linear equations particularly simple form gaussian elimination key gaussian elimination elementary transformations systems linear equations transform equation simple form then apply steps simple form discussed context example   elementary transformations elementary transformations key solving linear equations elementary transformations solution set same transform equation simpler form draft  mathematics machine learning feedback smmlbookcom  solving systems linear equations exchange equations rows matrix representing equations multiplication equation row constant  addition equations rows example   seek solutions following equations                          start converting equations compact matrix notation ax   longer mention variables  explicitly build augmented matrix in form cida augmented matrix bcid swap  swap  vertical line separate lefthand righthand  use cid indicate transformation augmented matrix elementary transformations swapping rows   leads apply indicated transformations eg subtract row  times row  obtain augmented matrix cida  bcid compactly represents linear equations ax   cid cid            deisenroth   faisal   ong published cambridge university press  linear algebra rowechelon form augmented matrix convenient form rowechelon form ref reverting compact notation explicit notation variables seek obtain                  particular solution  solved particular solution general solution general solution captures set possible solutions            pivot following constructive way obtain particular general solution linear equations remark pivots staircase structure leading coefﬁcient row ﬁrst nonzero number left called pivot strictly right pivot row it therefore equa tion rowechelon form staircase structure rowechelon form deﬁnition  rowechelon form matrix rowechelon form pivot leading coefﬁcient texts required pivot  basic variable free variable rows contain zeros matrix corre spondingly rows contain nonzero element rows contain zeros looking nonzero rows only ﬁrst nonzero number left also called pivot leading coefﬁcient strictly right pivot row it remark basic free variables variables corresponding pivots rowechelon form called basic variables variables free variables example     basic variables   free variables remark obtaining particular solution rowechelon form makes draft  mathematics machine learning feedback smmlbookcom  solving systems linear equations lives easier need determine particular solution this express righthand equation pivot columns   cidp  λipi pi         pivot columns λi determined easiest start rightmost pivot column work way left previous example try ﬁnd            here ﬁnd relatively directly      together forget nonpivot columns set coefﬁcients implicitly  therefore particular solution     remark reduced row echelon form equation reduced rowechelon form also rowreduced echelon form row canonical form   cid rowechelon form pivot  pivot nonzero entry column reduced rowechelon form reduced rowechelon form play important role later sec tion  allows determine general solution sys tem linear equations straightforward way remark gaussian elimination gaussian elimination algorithm performs elementary transformations bring linear equations reduced rowechelon form gaussian elimination example  reduced row echelon form veri following matrix reduced rowechelon form the pivots bold               key idea ﬁnding solutions ax   look non pivot columns need express linear combination pivot columns reduced row echelon form makes relatively straightforward express nonpivot columns terms sums multiples pivot columns left second col umn  times ﬁrst column we ignore pivot columns right second column therefore obtain  need subtract    deisenroth   faisal   ong published cambridge university press  linear algebra second column times ﬁrst column now look ﬁfth column second nonpivot column ﬁfth column expressed  times ﬁrst pivot column  times second pivot  times pivot column need track column indices pivot columns translate  times ﬁrst col umn  times second column which nonpivot column  times  times column which second pivot column fourth column which pivot column need subtract ﬁfth column obtain  end solving homogeneous equation system summarize solutions ax     given            minus trick following introduce practical trick reading solu tions  homogeneous linear equations ax   start assume reduced rowechelon form rkn  right now rows contain zeros ie                                                       arbitrary real number constraints ﬁrst nonzero entry row  entries corresponding column  columns      jk pivots marked rk extend matrix bold standard unit vectors      ek nmatrix  adding  cid  rows form   cid       diagonal augmented matrix  contains  then columns  contain  pivots solutions draft  mathematics machine learning feedback smmlbookcom  solving systems linear equations homogeneous equation ax   precise columns form basis section  solution space ax   later kernel null space see section  kernel null space example  minus trick let revisit matrix  reduced ref  matrix adding rows augment matrix  form  places pivots diagonal missing obtain                            form immediately read solutions ax   taking columns  contain  diagonal           identical solution  obtained insight calculating inverse compute inverse  right nown need ﬁnd matrix  satisﬁes ax   then    write set simultaneous linear equations ax   solve    xn use augmented matrix notation compact representation set systems linear equations obtain      cida cid cid cid cidi     acid  means bring augmented equation reduced rowechelon form read inverse righthand equation system hence determining inverse matrix equiv alent solving systems linear equations    deisenroth   faisal   ong published cambridge university press  linear algebra example  calculating inverse matrix gaussian elimination determine inverse                 write augmented matrix use gaussian elimination bring reduced rowechelon form desired inverse given righthand side   veri  inverse performing multi plication aa observing recover   algorithms solving linear equations following brieﬂy discuss approaches solving lin ear equations form ax   assumption solu tion exists solution need resort approximate solutions cover  way solve ap proximate problem approach linear regression discuss   special cases able determine inverse  solution ax   given   ab however possible square matrix invertible case otherwise mild assumptions ie needs linearly independent columns use transformation ax   acidax  acidb   acidaacidb draft  mathematics machine learning feedback smmlbookcom  vector spaces use moorepenrose pseudoinverse acidaacid determine moorepenrose pseudoinverse solution  solves ax   corresponds mini mum norm leastsquares solution disadvantage approach requires computations matrixmatrix product comput ing inverse acida moreover reasons numerical precision generally recommended compute inverse pseudoinverse following brieﬂy discuss alternative approaches solving systems linear equations gaussian elimination plays important role computing deter minants section  checking set vectors linearly inde pendent section  computing inverse matrix section  computing rank matrix section  determining basis vector space section  gaussian elimination intuitive constructive way solve linear equations thousands variables however systems millions variables impracti cal required number arithmetic operations scales cubically number simultaneous equations practice systems linear equations solved indirectly ei ther stationary iterative methods richardson method ja cobi method gaußseidel method successive overrelaxation method krylov subspace methods conjugate gradients gener alized minimal residual biconjugate gradients refer books stoer burlirsch  strang  liesen mehrmann  details let  solution ax   key idea iterative methods set iteration form xk  cxk   suitable   reduces residual error iteration converges  introduce norms compute similarities vectors section  cid  allow cid  cid cid xk  vector spaces far looked systems linear equations solve section  saw systems linear equations com pactly represented matrixvector notation  following closer look vector spaces ie structured space vectors live beginning  informally characterized vectors objects added multiplied scalar remain objects type now ready formalize this start introducing concept group set elements operation deﬁned elements keeps structure set intact    deisenroth   faisal   ong published cambridge university press  group closure associativity neutral element inverse element abelian group      linear algebra  groups groups play important role science providing fundamental framework operations sets heavily cryptography coding theory graphics   deﬁnition  group consider set     deﬁned operation  called group following hold  closure  associativity  neutral element  inverse element        neutral element write  denote inverse element                                           remark inverse element deﬁned respect operation necessarily mean        abelian         additionally group commutative   example  groups let look examples sets associated operations groups   abelian group   group   possesses neutral element  inverse elements missing  group    inverse elements   abelian right now  zn    contains neutral element   group  possess inverse element  abelian  deﬁned componentwise ie  missing     xn       xn      then       neutral element    rmn  set  addition deﬁned  let closer look right nown matrix multiplication deﬁned      yn      xn  yn    xn inverse element nmatrices abelian with componentwise  ie set  nmatrices  closure associativity follow directly deﬁnition matrix multiplication  neutral element identity matrix  neutral element respect matrix multiplication   right nown draft  mathematics machine learning feedback smmlbookcom cid  vector spaces  inverse element inverse exists  regular  right nown exactly case right nown inverse element group called general linear group deﬁnition  general linear group set regular invertible right nown group respect matrix multiplication matrices deﬁned  called general linear group gln  however matrix multiplication commutative group abelian general linear group  vector spaces       ie mappings discussed groups looked sets operate elements inner operations following consider sets addition inner operation  contain outer operation  think inner operation form addition scalar  outer operation form scaling note innerouter operations innerouter products  multiplication vector    deﬁnition  vector space realvalued vector space    set operations   vector space                abelian group    distributivity                 associativity outer operation  neutral element respect outer operation                           λψ          elements  zero vector        cid inner operation  called vector  called scalars outer operation addition elements   called vectors neutral element  multiplication scalars note scalar product multiplication different section  vector addition vector scalar scalars right now deﬁned theoret remark vector multiplication ab   ically deﬁne elementwise multiplication   ab cj  ajbj array multiplication common program ming languages makes mathematically limited sense stan dard rules matrix multiplication treating vectors   matrices    deisenroth   faisal   ong published cambridge university press  outer product linear algebra which usually do use matrix multiplication deﬁned  however dimensions vectors match following multiplications vectors deﬁned abcid right nown outer product acidb innerscalardot product example  vector spaces let look important examples  right now   vector space operations deﬁned follows  addition xy       xny     yn        xn yn  multiplication scalars λx  λx     xn  λx     λxn   right    rmn   right  addition     vector space    am  bm    an  bn amn  bmn  deﬁned ele    λa λam λan λamn        deﬁned mentwise      multiplication scalars λa  section  remember rmn equivalent rmn   standard deﬁnition addition complex numbers   standard vector addition scalar multiplication simpli remark following denote vector space  moreover use notation  notation remark vector spaces right now right now rn different way write vectors following distinction right right now allows write ntuples column vectors  vectors   column vector     xn row vector transpose simpliﬁes notation vector space operations however distinguish right now rn the row vectors avoid con fusion matrix multiplication default write  denote col umn vector row vector denoted xcid transpose  draft  mathematics machine learning feedback smmlbookcom  vector spaces  vector subspaces following introduce vector subspaces intuitively sets contained original vector space property perform vector space operations elements subspace leave it sense closed vector subspaces key idea machine learning example   demonstrates use vector subspaces dimensionality reduction   deﬁnition  vector subspace let    vector subspace linear subspace vector space vector space operations  linear subspace    vector space  called vector subspace  or     denote subspace  write restricted     cid       vector space naturally inherits prop erties directly  hold   particular  includes abelian group properties distribu tivity associativity neutral element determine  particular   closure   subspace  need     cid  respect outer operation  respect inner operation    λx     example  vector subspaces let look examples vector space   trivial subspaces  example  figure  subspace  with usual inner outer operations  closure property violated  contain  solution set homogeneous linear equations ax    unknowns        xncid subspace right now solution inhomogeneous linear equations ax    intersection arbitrarily subspaces subspace itself   subspace right now    deisenroth   faisal   ong published cambridge university press  figure  subsets  subspaces  closure property violated  contain   subspace cid linear combination      λk  linear combination vectors      xk linear algebra remark subspace geneous linear equations ax     solution space homo right now right now   linear independence following close look vectors elements vector space particular add vectors multiply scalars closure property guarantees end vector vector space possible ﬁnd set vectors represent vector vector space adding scaling them set vectors basis discuss section  there need introduce concepts linear combinations linear independence deﬁnition  linear combination consider vector space  ﬁnite number vectors      xk   then   form   λx   λkxk     cid λixi vector written linear combination  vec tors      xk   cidk  xi true following interested nontrivial linear combinations set vectors represent  ie linear combinations vectors      xk coefﬁcients λi         xk deﬁnition  linear independence let consider vector space   nontrivial linear com   bination   cidk   vectors      xk linearly dependent trivial solution exists ie       λk   vectors      xk linearly independent  λixi λi linear independence important concepts linear algebra intuitively set linearly independent vectors consists vectors redundancy ie remove vectors set lose something sections formalize intuition more example  linearly dependent vectors geographic example help clari concept linear indepen dence person nairobi kenya describing kigali rwanda you kigali ﬁrst going  northwest kam pala uganda  southwest sufﬁcient information draft  mathematics machine learning feedback smmlbookcom linearly dependent linearly independent cid  linear independence location kigali geographic coordinate sys tem considered twodimensional vector space ignoring altitude earths curved surface person add it  west here statement true necessary ﬁnd kigali given previous information see figure  illus tration example  northwest vector blue  southwest vector purple linearly independent means southwest vector described terms northwest vec tor vice versa however  west vector black linear combination vectors makes set vec tors linearly dependent equivalently given  west  southwest linearly combined obtain  northwest figure  geographic example with crude approximations cardinal directions linearly dependent vectors twodimensional space plane remark following properties useful ﬁnd vectors linearly independent      xk  xi           vectors linearly dependent linearly independent option vectors      xk  linearly de pendent holds vectors identical   cid  linearly vectors dependent at least linear combination others particular vector multiple vector ie xi  λxj  linearly dependent practical way checking vectors      xk  linearly independent use gaussian elimination write vectors columns matrix perform gaussian elimination matrix row echelon form the reduced rowechelon form unnecessary here           set      xk  xi    deisenroth   faisal   ong published cambridge university press  kmnorthwestkmwestkmsouthwestkmsouthwestkampalanairobikigalicid cid linear algebra  pivot columns indicate vectors linearly indepen dent vectors left note ordering vec tors matrix built  nonpivot columns expressed linear combinations pivot columns left instance rowechelon form cid      cid tells ﬁrst columns pivot columns sec ond column nonpivot column times ﬁrst column column vectors linearly independent columns pivot columns nonpivot column columns and therefore corresponding vectors linearly dependent example  consider          check linearly dependent follow general ap proach solve λx  λx  λx               write vectors xi     columns matrix apply elementary row operations identi pivot columns   cid cid            here column matrix pivot column therefore nontrivial solution require          solve equation system hence vectors    linearly independent draft  mathematics machine learning feedback smmlbookcom   λibi  cid cid xm  λimbi    λj λkj  linear independence remark consider vector space   linearly independent vectors      bk  linear combinations deﬁning        bk matrix columns linearly independent vectors      bk write xj  bλj  λj           compact form want test      xm linearly independent  ψjxj   purpose follow general approach testing cidm  obtain cid cid cid ψjxj  ψjbλj   ψjλj  means column vectors      xm      λm linearly independent linearly independent remark vector space    linear combinations  vectors      xk linearly dependent    example  consider set linearly independent vectors    right                       right linearly independent answer vectors       question investigate column vectors    deisenroth   faisal   ong published cambridge university press  linear algebra linearly independent reduced rowechelon form corre sponding linear equation coefﬁcient matrix             given corresponding linear equation nontrivially solv able column pivot column   therefore       linearly dependent  expressed linear combination        basis rank vector space   particularly interested sets vectors possess property vector  combination vectors following characterize them  obtained linear  vectors special vectors  generating set basis  set vectors   expressed linear combination      xk deﬁnition  generating set span consider vector space   generating set   set linear combinations vectors called span   spanx     xk spans vector space   write   span called      xk  vector     generating sets sets vectors span vector subspaces ie vector represented linear combination vectors generating set now speciﬁc characterize smallest generating set spans vector subspace generating set span minimal basis deﬁnition  basis consider vector space     generating set cid minimal called basis      called minimal exists smaller set spans   linearly independent generating set    draft  mathematics machine learning feedback smmlbookcom  basis rank let      following statements equivalent  vector space  then     cid basis   minimal generating set maximal linearly independent set vectors   ie adding vector set linearly dependent vector  linear combination unique ie  linear combination vectors   λibi  ψibi cid cid λi ψi  bi   follows λi  ψi        example  basis minimal generating set maximal linearly independent set vectors  canonicalstandard basis     canonical basis different bases        set         linearly independent generating set and basis  instance vector    cid obtained linear com bination elements remark vector space  possesses basis  preceding exam ples bases vector space   ie unique basis however bases possess number elements basis vectors consider ﬁnitedimensional vector spaces   case dimension  number basis vectors   write dimv   subspace   dimyou  cid dimv  dimyou      deisenroth   faisal   ong published cambridge university press  basis vector dimension dimension vector space corresponds number basis vectors linear algebra dimv     intuitively dimension vector space thought number independent directions vector space remark dimension vector space necessarily number elements vector instance vector space   span cid cid onedimensional basis vector possesses elements remark basis subspace  spanx     xm executing following steps right  write spanning vectors columns matrix  determine rowechelon form   spanning vectors associated pivot columns basis   example  determining basis vector subspace       spanned vectors       interested ﬁnding vectors       basis  this need check       linearly independent therefore need solve cid λixi    leads homogeneous equations matrix cid  cidx    basic transformation rules systems linear equations obtain rowechelon form    cid cid draft  mathematics machine learning feedback smmlbookcom  basis rank pivot columns indicate set vectors linearly indepen dent rowechelon form    linearly inde pendent because linear equations λx  λx  λx   solved        therefore basis     rank  rank rmn span subspace  rmn number linearly independent columns matrix equals number linearly independent rows called rank denoted rka remark rank matrix important properties rka  rkacid ie column rank equals row rank rm dimyou   rmn span subspace columns rka later subspace image range basis applying gaussian elimination identi pivot columns right dimw   rows rka basis  applying gaussian elimination acid rka   ax   solved rka  rka  denotes augmented system sion  space rmn rank rank equals largest possible matrix rank matrix dimensions means rank fullrank matrix lesser number rows columns ie rka  minm  matrix said rank deﬁcient rank rmn subspace solutions ax   possesses dimen rka later subspace kernel null rm holds linear equation right nown holds regular invertible rmn  kernel null space rank rank deﬁcient example  rank          linearly independent rowscolumns rka      deisenroth   faisal   ong published cambridge university press  linear algebra     use gaussian elimination determine rank  cid   cid               here number linearly independent rows columns  rka    linear mappings following study mappings vector spaces preserve structure allow deﬁne concept coordinate beginning  said vectors objects added multiplied scalar resulting object vector wish preserve property applying mapping consider real vector spaces    mapping     preserves structure vector space φx    φx  φy φλx  λφx   deﬁnition    summarize following deﬁnition  linear mapping vector spaces    mapping     called linear mapping or vector space homomorphism linear transformation      φλx  ψy  λφx  ψφy  turns represent linear mappings matrices sec tion  recall collect set vectors columns matrix working matrices mind matrix represents linear mapping collection vectors linear mappings   continue brieﬂy introduce special mappings deﬁnition  injective surjective bijective consider mapping   linear mapping vector space homomorphism linear transformation injective surjective bijective arbitrary sets  called      injective   surjective    bijective injective surjective  φx  φy     draft  mathematics machine learning feedback smmlbookcom  linear mappings  surjective element reached  bijective  undone ie exists mapping   φx   mapping  called inverse     normally denoted  deﬁnitions introduce following special cases linear mappings vector spaces    isomorphism    endomorphism    automorphism    deﬁne idv   automorphism    linear bijective  linear  linear bijective    cid  identity mapping identity identity mapping isomorphism endomorphism automorphism identity automorphism example  homomorphism mapping    cidcid cidcidx cidy cid  φx    ix homomorphism      ix      ix    iy   cidcid cidcidx   cidcid cidcidy cidcid cid cidx  λx  λix  λx  ix  λφ cidcidx cidcid justiﬁes complex numbers represented tuples  bijective linear mapping converts elementwise addi tion tuples  set complex numbers correspond ing addition note showed linearity bijection theorem  theorem  axler  finitedimensional vector spaces   isomorphic dimv   dimw  theorem  states exists linear bijective mapping be tween vector spaces dimension intuitively means vector spaces dimension kind thing transformed incurring loss theorem  gives justiﬁcation treat rmn the vector nmatrices rmn the vector space vectors length space  mn same dimensions mn exists linear bi jective mapping transforms other remark consider vector spaces    then linear mappings          phism too  linear  isomorphism         mapping  isomor    deisenroth   faisal   ong published cambridge university press  figure  different coordinate systems deﬁned sets basis vectors vector  different coordinate representations depending coordinate chosen linear algebra    linear too      linear    λφ   matrix representation linear mappings ndimensional vector space isomorphic right theorem  ndimensional vector space   consider basis following order basis vectors important therefore write      bn        bn      bn remark notation point notation gets bit tricky therefore summarize parts here        bn ordered unordered basis        bn basis matrix columns vectors      bn deﬁnition  coordinates consider vector space  ordered basis        bn     obtain unique represen tation linear combination   αb      αnbn ordered basis ntuple ordered basis   coordinate  respect       αn coordinates  respect  vector   αn right   coordinate vector coordinate representation coordinate vectorcoordinate representation  respect ordered basis  draft  mathematics machine learning feedback smmlbookcom  linear mappings basis effectively deﬁnes coordinate system familiar cartesian coordinate dimensions spanned canonical basis vectors   coordinate system vector  representation tells linearly combine   obtain  however basis  deﬁnes valid coordinate system vector  different coordinate rep resentation   basis figure  coordinates  respect standard basis    cid however respect basis   vector  represented  cid ie      following sections discover obtain representation example   coordinates  cid let look geometric vector  respect standard basis    means write      however choose standard basis cid    cid represent vector use basis vectors    obtain coordinates   cid represent vector   respect   see figure  figure  different coordinate representations vector  depending choice basis           remark ndimensional vector space  ordered basis    mapping   right   φei  bi        linear and theorem  isomorphism      en standard basis right now ready explicit connection matrices linear mappings ﬁnitedimensional vector spaces deﬁnition  transformation matrix consider vector spaces   corresponding ordered bases        bn        cm moreover consider linear mapping               cid φbj  αjc   αmjcm  αijci    unique representation φbj respect  then nmatrix aφ elements given aφi   αij  transformation matrix  with respect ordered bases      transformation matrix coordinates φbj respect ordered basis   jth column aφ consider ﬁnitedimensional vector spaces   ordered bases   linear mapping       deisenroth   faisal   ong published cambridge university press  linear algebra transformation matrix aφ ˆx coordinate vector  respect  ˆy coordinate vector   φx  respect ˆy  aφ ˆx  means transformation matrix map coordinates respect ordered basis  coordinates respect ordered basis   example  transformation matrix consider homomorphism                      ordered bases      φb   φb         φb       transformation matrix aφ respect   satisﬁes φbk  cid  αikci         given aφ                αj      coordinate vectors φbj respect example  linear transformations vectors  original data  rotation   stretch horizontal axis  general mapping linear consider linear transformations set vectors  transformation matrices cid cidcos  sin      cos  sin                cid cid    cid cid draft  mathematics machine learning feedback smmlbookcom figure  examples linear transformations vectors shown dots   rotation   stretching horizontal coordinates   combination reﬂection rotation stretching  linear mappings figure  gives examples linear transformations set vec tors figure  shows  vectors  represented dot corresponding  xcoordinates vectors ar ranged square use matrix   linearly transform vectors obtain rotated square figure  apply linear mapping represented  obtain rectangle figure  xcoordinate stretched  figure  shows original square figure  linearly transformed  combination reﬂection rotation stretch  basis change following closer look transformation matrices linear mapping     change change bases    consider ordered bases        bn        bn  ordered bases        cm        cm  respect bases   aφ rmn transformation matrix linear   moreover aφ rmn mapping    corresponding transformation mapping respect   following investigate  related ie how transform aφ aφ choose perform basis change     remark effectively different coordinate representations identity mapping idv  context figure  mean map coordinates respect   coordinates respect   changing vector  changing basis corre spondingly representation vectors transformation matrix respect new basis particularly simple form allows straightforward computation example  basis change consider transformation matrix respect canonical basis  deﬁne new basis cid cid       cid cid cid cid     deisenroth   faisal   ong published cambridge university press  linear algebra obtain diagonal transformation matrix cid   cid    respect  easier work  following look mappings transform coordinate vectors respect basis coordinate vectors respect different basis state main result ﬁrst provide explanation theorem  basis change linear mapping    bases   ordered        bn        bn        cm        cm   transformation matrix aφ  respect   corresponding transformation matrix aφ respect bases   given aφ   aφs  right nown transformation matrix idv maps coordinates here  respect  coordinates respect   rmm transformation matrix idw maps coordinates respect  coordinates respect  proof following drumm weil  write vectors new basis   linear combination basis vectors  bj  sjb      snjbn  sijbi           similarly write new basis vectors   linear combination basis vectors  yields ck  tkc   tmkcm  tlkcl              right nown transformation matrix maps deﬁne   sij coordinates respect  coordinates respect  rmm transformation matrix maps coordinates   tlk respect  coordinates respect  particular jth column  coordinate representation bj respect  cid cid draft  mathematics machine learning feedback smmlbookcom  linear mappings kth column  coordinate representation ck respect  note   regular going look φbj perspectives first applying mapping          φbj  cid akjck cid cidcid cid cid cid akj tlkcl  cid cid  cid cid tlkakj cl   ﬁrst expressed new basis vectors ck binations basis vectors cl summation  linear com  swapped order alternatively express bj  linear combinations   arrive bj φbj   sijbi sijφbi  alicl cid cid sij cid  cid cid cid cid cid cid  cid alisij cl           exploited linearity  comparing   follows                 and therefore cid cid tlkakj  alisij  aφ  aφs rmn  aφ   aφs  proves theorem  theorem  tells basis change   replaced    replaced  transformation matrix aφ  replaced equivalent matrix aφ linear mapping    aφ   aφs figure  illustrates relation consider homomorphism     ordered bases        mapping φcb instantiation  maps basis vectors  linear combinations basis vectors  assume know transformation matrix aφ φcb respect ordered bases   perform basis change        determine    deisenroth   faisal   ong published cambridge university press  figure  homomorphism      ordered bases       marked blue express mapping    respect bases   equivalently composition homomorphisms      cc  φcb  ψb  respect bases subscripts corresponding transformation matrices red equivalent similar vector spaces ordered bases ψb  ξc  ψb     cc     φcb aφ aφ    φcb aφ aφ    linear algebra corresponding transformation matrix aφ follows first ﬁnd ma trix representation linear mapping ψb     maps coordi nates respect new basis  unique coordinates respect old basis  in   then use transformation ma  map coordinates coordinates trix aφ φcb   respect    finally use linear mapping  cc   map coordinates respect  coordinates respect  therefore express linear mapping    composition linear mappings involve old basis      cc  φcb ψb    φcb ψb      concretely use ψb   idv ξc   idw  ie identity mappings map vectors themselves respect different basis deﬁnition  equivalence matrices   right nown  exist regular matrices     as rmn equivalent rmm deﬁnition  similarity matrices   exists regular matrix  right nown   sas right nown similar remark similar matrices equivalent however equivalent ma trices necessarily similar remark consider vector spaces    remark follows theorem  know linear mappings        linear transformation matrices aφ aψ corresponding mappings overall transformation matrix aψφ  aψaφ  mapping  light remark look basis changes perspec    tive composing linear mappings aφ transformation matrix linear mapping φcb   respect bases   aφ transformation matrix linear mapping      respect bases    transformation matrix linear mapping ψb    automorphism represents  terms  normally   idv identity mapping   draft  mathematics machine learning feedback smmlbookcom  linear mappings  transformation matrix linear mapping ξc    automorphism represents  terms  normally   idw identity mapping   informally write transformations terms bases aφ        aφ              aφ   aφs  note execution order  right left vec tors multiplied righthand   cidaφsxcid  aφx aφsx sx cid cid cid example  basis change consider linear mapping     transformation matrix aφ            respect standard bases             seek transformation matrix aφ  respect new bases        then                                     ith column  coordinate representation bi terms basis vectors   standard basis co ordinate representation straightforward ﬁnd general basis  need solve linear equation ﬁnd λi    deisenroth   faisal   ong published cambridge university press  linear algebra  λibi  bj         similarly jth column  coordi cid nate representation cj terms basis vectors  therefore obtain aφ   aφs             able exploit concept basis change ﬁnd basis respect transformation matrix en domorphism particularly simple diagonal form   look data compression problem ﬁnd convenient basis project data minimizing compression loss  image kernel image kernel linear mapping vector subspaces cer tain important properties following characterize carefully deﬁnition  image kernel      deﬁne kernelnull space kerφ  φw   imagerange   φv   amφ  φv     φv     domain codomain  respectively   image set vectors  intuitively kernel set vectors    maps neutral element  reached  vector   illustration given figure  remark consider linear mapping    spaces     vector holds φv    and therefore  particular null space empty amφ  subspace   kerφ  subspace   kerφ draft  mathematics machine learning feedback smmlbookcom kernel null space image range domain codomain  linear mappings    figure  kernel image linear mapping       kerφ amφ column space  injective onetoone kerφ  remark null space column space let consider rm  linear mapping   right ax cid       an ai columns  obtain cid  cid cid amφ  ax   right xiai       xn rmn  spana     an rm  ie image span columns  called column space therefore column space image subspace rm  height matrix rka  dimi amφ kernelnull space kerφ general solution homoge neous linear equations ax   captures possible linear combinations elements right produce  kernel subspace right now  width matrix kernel focuses relationship columns use determine whetherhow express column linear combination columns rm example  image kernel linear mapping mapping     cid cid    cid   cidx   cid       deisenroth   faisal   ong published cambridge university press  linear algebra         cid cid cid cid cid cid cid cid linear determine amφ span columns transformation matrix obtain amφ  span cid cid cid cid cid cid cid   cid compute kernel null space  need solve ax   ie need solve homogeneous equation system this use gaussian elimination transform reduced rowechelon form cid    cid   cid cid    cid    cid   matrix reduced rowechelon form use minus  trick compute basis kernel see section  alternatively express nonpivot columns columns   linear com binations pivot columns columns   column    equivalent way    overall gives kernel null space  times second column  therefore          and therefore    kerφ  span   ranknullity theorem theorem  ranknullity theorem vector spaces   lin ear mapping     holds dimkerφ  dimi amφ  dimv   fundamental theorem linear mappings ranknullity theorem referred fundamental theorem linear mappings axler  theorem  following direct consequences theorem  dimi amφ  dimv  kerφ nontrivial ie kernel contains  dimkerφ cid  aφ transformation matrix  respect ordered basis dimi amφ  dimv  linear equations aφx   inﬁnitely solutions dimv   dimw  following threeway equivalence holds   injective   surjective   bijective amφ   draft  mathematics machine learning feedback smmlbookcom  afﬁne spaces  afﬁne spaces following closer look spaces offset origin ie spaces longer vector subspaces moreover brieﬂy discuss properties mappings afﬁne spaces resemble linear mappings remark machine learning literature distinction linear afﬁne clear ﬁnd references afﬁne spacesmappings linear spacesmappings  afﬁne subspaces deﬁnition  afﬁne subspace let  vector space   subspace subset                called afﬁne subspace linear manifold   called direction direction space  called support point   refer subspace hyperplane note deﬁnition afﬁne subspace excludes    therefore afﬁne subspace linear subspace vector subspace    examples afﬁne subspaces points lines planes  necessarily origin remark consider afﬁne subspaces         you vector space   then  you  you  afﬁne subspaces described parameters consider kdimen sional afﬁne space            bk ordered basis  element   uniquely described     λb      λkbk       λk  directional vectors      bk parameters      λk  representation called parametric equation example  afﬁne subspaces afﬁne subspace linear manifold direction direction space support point hyperplane parametric equation parameters onedimensional afﬁne subspaces called lines written right one     λb  dimensional subspace right now means line deﬁned sup port point  vector  deﬁnes direction figure  illustration   spanb line    deisenroth   faisal   ong published cambridge university press  plane hyperplane figure  lines afﬁne subspaces vectors  line   λb lie afﬁne subspace  support point  direction  linear algebra twodimensional afﬁne subspaces right called planes para metric equation planes     λb  λb   right now means plane deﬁned  spanb  support point  linearly independent vectors   span direction space right now  dimensional afﬁne subspaces called hyperplanes corresponding parametric equation     cidn  λibi      bn form basis  dimensional subspace right now means hyperplane deﬁned support point    linearly independent vectors      bn span direction space  line hyperplane  plane hyperplane         rmn  remark inhomogeneous systems linear equations afﬁne subspaces rm solution linear equa tions aλ   set afﬁne subspace right dimension  rka particular solution linear equation λb      λnbn        λn        hyperplane right now right now kdimensional afﬁne subspace solution inho mogeneous linear equations ax   rm rka    recall homogeneous equation systems ax   solution vector subspace think special afﬁne space support point    rmn   afﬁne mappings similar linear mappings vector spaces discussed section  deﬁne afﬁne mappings afﬁne spaces linear afﬁne mappings closely related therefore properties know linear mappings eg composition linear mappings linear mapping hold afﬁne mappings deﬁnition  afﬁne mapping vector spaces    linear draft  mathematics machine learning feedback smmlbookcom cid  reading mapping        mapping    cid  φx afﬁne mapping    vector called translation vector  afﬁne mapping translation vector  composition linear     translation     mappings   uniquely determined  afﬁne mappings    afﬁne mapping    mapping       composition φcid afﬁne afﬁne mappings geometric structure invariant pre serve dimension parallelism   φcid    reading resources learning linear algebra including text books strang  golan  axler  liesen mehrmann  online resources men tioned introduction  covered gaussian elim ination here approaches solving systems linear equations refer numerical linear algebra textbooks stoer burlirsch  golub van loan  horn johnson  indepth discussion book distinguish topics linear algebra eg vectors matrices linear independence basis topics related geometry vector space   introduce inner product induces norm concepts allow deﬁne angles lengths distances use orthogonal projections pro jections turn key machine learning algorithms linear regression principal component analysis cover    respectively    deisenroth   faisal   ong published cambridge university press  linear algebra  consider  cid exercises cid   ab          cid abelian group  solve  cid  cid    abelian group  cid cid deﬁned  let   let    deﬁne congruence class  integer  set            modn                  deﬁne znz sometimes written zn set congruence classes modulo  euclidean division implies set ﬁnite set con taining  elements    zn deﬁne zn                 zn  group abelian  deﬁne operation   zn         represents usual multiplication  let    draw times table elements   ie calculate products     hence  closed  possesses neutral element  display inverse elements   conclude   abelian group    group  recall bezout theorem states integers  relatively prime ie gcda    exist integers  au  bv   zn  group    prime  consider set     matrices deﬁned follows         cid cid cid cid cid cid      deﬁne  standard matrix multiplication   group yes abelian justi answer  compute following matrix products possible draft  mathematics machine learning feedback smmlbookcom exercises cid cid        cid cid    set  solutions  following inhomogeneous linear systems ax    deﬁned follows                           gaussian elimination ﬁnd solutions inhomogeneous equa tion ax            deisenroth   faisal   ong published cambridge university press  linear algebra solutions      equation ax   cid  xi    determine inverses following matrices possible  following sets subspaces                           let                               following sets vectors linearly independent                    write   linear combination           draft  mathematics machine learning feedback smmlbookcom exercises  consider subspaces    span     span   determine basis     consider subspaces    solution space homogeneous equation ax    solution space homogeneous equation ax                  determine dimension    determine bases    determine basis     consider subspaces    spanned columns   spanned columns                 determine dimension    determine bases    determine basis     let         xyz     ab ab ab         subspaces   calculate    resorting basis vector  basis   calculate   basis vectors previously check result previous question  following mappings linear  let     la  denotes set integrable functions     la     cid φf    xdx  cid        cid φf    cid   cid  ck denotes set  times continuously differen tiable functions  denotes set continuous functions    deisenroth   faisal   ong published cambridge university press  linear algebra  let     consider linear mapping      cid φx  cosx      cid  cid cid       cid cid cosθ  sinθ cid sinθ cosθ                          transformation matrix aφ determine rkaφ compute kernel image  dimkerφ dimi amφ  let  vector space let   automorphisms      ide ie    identity mapping ide kerf   kerg    amg  amg    kerf   amg    consider endomorphism      transformation matrix with respect standard basis  aφ       determine kerφ amφ  determine transformation matrix aφ respect basis          ie perform basis change new basis   let consider   bcid  bcid   vectors  expressed standard basis     cid cid cid cid bcid   cid cid  bcid   cid cid let deﬁne ordered bases     bcid  bcid  bcid   draft  mathematics machine learning feedback smmlbookcom exercises   bcid bases  draw basis vectors  compute matrix   performs basis change bcid   consider    vectors  deﬁned standard basis           deﬁne        basis  eg determinants see section  ii let ccid  ccid matrix   performs basis change  ccid  ccid  ccid  standard basis  determine  consider homomorphism      φb       φb                  ordered bases   respectively determine transformation matrix aφ  respect or dered bases    determine acid transformation matrix  respect bases bcid ccid  let consider vector    coordinates bcid  cid words   bcid   bcid  calculate coordinates   ii based that compute coordinates φx expressed  iii then write φx terms ccid iv use representation  bcid matrix acid ﬁnd  ccid  ccid result directly    deisenroth   faisal   ong published cambridge university press  analytic geometry   studied vectors vector spaces linear mappings general abstract level  add geomet ric interpretation intuition concepts particular look geometric vectors compute lengths distances angles vectors able this equip vec tor space inner product induces geometry vector space inner products corresponding norms metrics capture intuitive notions similarity distances use develop support vector machine   use concepts lengths angles vectors discuss orthogonal projections play central role discuss principal component anal ysis   regression maximum likelihood estimation   figure  gives overview concepts  related connected  book figure  mind map concepts introduced parts book    norm inner product   classiﬁcation lengths orthogonal projection angles rotations   regression   matrix decomposition   dimensionality reduction material published cambridge university press mathematics machine learning marc peter deisenroth  aldo faisal cheng soon ong  version free view download personal use only redistribution resale use derivative works by   deisenroth   faisal   ong  smmlbookcom  norms figure  different norms red lines indicate set vectors norm  left manhattan norm right euclidean distance  norms think geometric vectors ie directed line segments start origin intuitively length vector distance end directed line segment origin following discuss notion length vectors concept norm deﬁnition  norm norm vector space  function norm cid  cid   cid cid cid cid cid  assigns vector  length    following hold absolutely homogeneous triangle inequality positive deﬁnite cid λx cid cid cid    cid cid  cid cid cid cid cid cid cid   cid cid cid       length geometric terms triangle inequality states triangle sum lengths sides greater equal length remaining side figure  illustration deﬁnition  terms general vector space  section  book consider ﬁnitedimensional vector space right now right denote elements vector recall vector  subscript is xi ith element vector  absolutely homogeneous triangle inequality positive deﬁnite figure  triangle inequality example  manhattan norm manhattan norm right deﬁned  right   cid cid cid   xi manhattan norm    vectors  norm absolute value left panel figure  shows    manhattan norm called cid cid cid cid norm    deisenroth   faisal   ong published cambridge university press  cidxcidcidxcidabcab  analytic geometry euclidean norm example  euclidean norm euclidean norm  right deﬁned   cid cid cid cid cid cid cid  xcidx euclidean distance cid norm computes euclidean distance  origin right panel    euclidean figure  shows vectors  norm called cid norm cid cid remark book use euclidean norm  default stated otherwise  inner products inner products allow introduction intuitive geometrical con cepts length vector angle distance vectors major purpose inner products determine vectors orthogonal other scalar product dot product familiar particular type inner product scalar productdot product right now given  dot product xcidy  xiyi  cid refer particular inner product dot product book however inner products general concepts speciﬁc properties introduce bilinear mapping  general inner products recall linear mapping section  rearrange mapping respect addition multiplication scalar bi linear mapping  mapping arguments linear argument ie look vector space  holds       ωλx  ψy   λωx   ψωy  ωx λy  ψz  λωx   ψωx   here  asserts  linear ﬁrst argument  asserts  linear second argument see  draft  mathematics machine learning feedback smmlbookcom  inner products  bilinear deﬁnition  let  vector space    mapping takes vectors maps real number  called symmetric ωx   ωy    order arguments matter  called positive deﬁnite   ie symmetric positive deﬁnite  ωx           bilinear deﬁnition  let  vector space    mapping takes vectors maps real number positive deﬁnite symmetric bilinear mapping      inner product   typically write pair   called inner product space real vector space inner product use dot product deﬁned  instead ωx   euclidean vector space cid cid  called cid cid cid cid refer spaces inner product spaces book inner product inner product space vector space inner product euclidean vector space example  inner product dot product consider    deﬁne   cid cid  xy xy  xy  xy inner product different dot product proof cid exercise cid  symmetric positive deﬁnite matrices symmetric positive deﬁnite matrices play important role machine learning deﬁned inner product section  return symmetric positive deﬁnite matrices context matrix decompositions idea symmetric positive semideﬁnite matrices key deﬁnition kernels section  consider ndimensional vector space  inner product cid see deﬁnition  ordered basis        bn  written  bilinearity   recall section  vectors   linear combinations basis vectors   cidn   cidn  suitable ψi λj inner product holds    λjbj  ψibi cid   cid cid cid  cid cid ψibi λjbj cid cid cid ψi bi bj cid cid λj  ˆxcidaˆy   ˆx ˆy coordinates   respect aij  basis  implies inner product uniquely deter mined  symmetry inner product means bi bj cid cid cid cid    deisenroth   faisal   ong published cambridge university press  symmetric positive deﬁnite positive deﬁnite symmetric positive semideﬁnite analytic geometry symmetric furthermore positive deﬁniteness inner product implies  xcidax    deﬁnition  symmetric positive deﬁnite matrix symmetric matrix right nown satisﬁes  called symmetric positive deﬁnite positive deﬁnite cid holds  called symmetric positive semideﬁnite example  symmetric positive deﬁnite matrices consider matrices      cid cid    cid cid     positive deﬁnite symmetric xcidax  cidx  cid cid cid cidx cid             xx        xcidax    eg     contrast  symmetric positive deﬁnite   xx        cid right nown symmetric positive deﬁnite    ˆxcidaˆy cid cid deﬁnes inner product respect ordered basis  ˆx ˆy coordinate representations    respect  theorem  realvalued ﬁnitedimensional vector space   inner product ordered basis    holds cid right nown exists symmetric positive deﬁnite matrix   cid following properties hold right nown symmetric positive deﬁnite    ˆxcidaˆy  cid cid   implies ax null space kernel consists  xcidax      diagonal elements aii positive aii  ecid ei ith vector standard basis right now aei     draft  mathematics machine learning feedback smmlbookcom cid cid cid  lengths distances  lengths distances section  discussed norms use compute length vector inner products norms closely related sense inner product induces norm cid cid cid   cid cid inner products induce norms natural way compute lengths vectors in ner product however norm induced inner product manhattan norm  example norm corresponding inner product following focus norms induced inner products introduce geometric concepts lengths dis tances angles remark cauchyschwarz inequality inner product vector space satisﬁes cauchyschwarz inequality  induced norm cid cid cid  cid    cid cid  cid cid cidcid cid cauchyschwarz inequality example  lengths vectors inner products geometry interested lengths vectors use inner product compute  let    cid  use dot product inner product  obtain  xcidx       cid cid length  let choose different inner product    xcid cid cid cid  cid   xy xy  xy  xy  compute norm vector inner product returns smaller values dot product   sign and xx   otherwise returns greater values dot product inner product obtain   cid cid     xx                   cid cid  shorter inner product dot product deﬁnition  distance metric consider inner product space cid cid cid dx     called distance       use dot product inner product distance called euclidean distance cid cid cid cid distance euclidean distance    deisenroth   faisal   ong published cambridge university press  metric called metric mapping analytic geometry      cid dx  remark similar length vector distance vectors require inner product norm sufﬁcient norm induced inner product distance vary depending choice inner product metric  satisﬁes following positive deﬁnite   positive deﬁnite ie dx  cid     dx   symmetric triangle inequality   symmetric ie dx   dy     triangle inequality dx  cid dx   dy             remark ﬁrst glance lists properties inner products met rics look similar however comparing deﬁnition  deﬁni dx  behave opposite directions tion  observe similar   result large value inner product small value metric   cid cid figure  restricted      cosω returns unique number interval   angle  angles orthogonality addition enabling deﬁnition lengths vectors distance vectors inner products capture geometry vector space deﬁning angle  vectors use cauchyschwarz inequality  deﬁne angles  inner prod uct spaces vectors   notion coincides intuition   assume          cid cid cid cid cid therefore exists unique  cid   cid cid   illustrated figure    cos   cid cid cid cid number  angle vectors   intuitively angle vectors tells similar orientations are example dot product angle     ie  scaled version   orientation same cid cid draft  mathematics machine learning feedback smmlbookcom ππωcosωcid cid  angles orthogonality example  angle vectors let compute angle    cid figure  use dot product inner product     cid cos     cid cid   cid cid   cid cid cid cid xcidy xcidxycidy angle vectors arccos  corresponds     rad key feature inner product allows characterize vectors orthogonal deﬁnition  orthogonality vectors   orthogonal ie vectors unit vectors   orthonormal   write   additionally   cid    cid cid cid cid cid implication deﬁnition vector orthogonal vector vector space remark orthogonality generalization concept perpendic ularity bilinear forms dot product context geometrically think orthogonal vectors having right angle respect speciﬁc inner product example  orthogonal vectors figure  angle  vectors   computed inner product orthogonal orthonormal figure  angle  vectors   change depending inner product consider vectors    cid     figure  interested determining angle  different inner products dot product inner product yields angle       however choose inner product  cid   cid cid  xcid cid cid         deisenroth   faisal   ong published cambridge university press  convention matrices orthogonal precise description orthonormal transformations orthogonal matrices preserve distances angles orthogonal matrix deﬁnition  orthogonal matrix square matrix orthogonal matrix columns orthonormal right nown analytic geometry angle    given   cos   cid cid cid cid cidcid  rad     orthogonal therefore vectors orthogonal respect inner product orthogonal re spect different inner product implies aacid   acida    acid  ie inverse obtained simply transposing matrix transformations orthogonal matrices special length vector  changed transforming orthogonal matrix  dot product obtain ax cid   axcidax  xcidacidax  xcidix  xcidx  cid cid cid   moreover angle vectors   measured inner product unchanged transforming orthogonal matrix  assuming dot product inner product angle images ax ay given cos   axciday ay ax cid cid cid cid cid xcidaciday xcidacidaxycidaciday xcidy cid cid cid cid gives exactly angle   means orthog onal matrices acid   preserve angles distances turns orthogonal matrices deﬁne transformations rota tions with possibility ﬂips section  discuss details rotations  orthonormal basis section  characterized properties basis vectors ndimensional vector space need  basis vectors ie  vectors linearly independent sections   inner products compute length vectors angle vectors following discuss special case basis vectors orthogonal length basis vector  basis orthonormal basis draft  mathematics machine learning feedback smmlbookcom  orthogonal complement let introduce formally deﬁnition  orthonormal basis consider ndimensional vector   space  basis      bn bi bj cid bi bi cid cid cid                basis called orthonormal basis onb  satisﬁed basis called orthogonal basis note  implies basis vector lengthnorm  orthonormal basis onb orthogonal basis      bn recall section  use gaussian elimination ﬁnd basis vector space spanned set vectors assume given set nonorthogonal unnormalized basis vectors concatenate matrix        bn apply gaussian elim ination augmented matrix section      obtain orthonormal basis constructive way iteratively build orthonor called gramschmidt process strang  mal basis cid      bn example  orthonormal basis canonicalstandard basis euclidean vector space right or thonormal basis inner product dot product vectors  vectors   cid cid   cid cid  cid cid form orthonormal basis bcid        cid cid exploit concept orthonormal basis     discuss support vector machines principal com ponent analysis  orthogonal complement having deﬁned orthogonality look vector spaces orthogonal other play important role   discuss linear dimensionality reduction geometric per spective consider ddimensional vector space   dimensional sub   orthogonal complement   space subspace  contains vectors  orthogonal vector  furthermore vector   dimensional orthogonal complement      deisenroth   faisal   ong published cambridge university press  cid figure  plane threedimensional vector space described normal vector spans orthogonal complement  normal vector analytic geometry uniquely decomposed   λmbm  cid dm cid ψjb   λm ψj      bm  basis         dm  basis  therefore orthogonal complement plane twodimensional subspace threedimensional vector space speciﬁcally vector    orthogonal cid plane  basis vector  figure  illustrates setting vectors orthogonal  by construction lie plane  vector  called normal vector  cid generally orthogonal complements hyperplanes ndimensional vector afﬁne spaces  inner product functions far looked properties inner products compute lengths angles distances focused inner products ﬁnitedimensional vectors following look example inner products different type vectors inner products functions inner products discussed far deﬁned vectors right function ﬁnite number entries think vector   function values concept inner product generalized vectors inﬁnite number entries countably inﬁnite continuousvalued functions uncountably inﬁnite sum individual components vectors see equation  example turns integral inner product functions       deﬁned deﬁnite integral you  cid cid cid  youxvxdx draft  mathematics machine learning feedback smmlbookcom  orthogonal projections lower upper limits     respectively usual inner product deﬁne norms orthogonality looking inner product  evaluates  functions  orthogonal preceding inner product mathematically precise need care measures deﬁnition integrals leading deﬁnition hilbert space furthermore unlike inner products ﬁnitedimensional vectors inner products functions diverge have inﬁnite value requires diving intricate details real functional analysis cover book example  inner product functions choose  sinx   cosx integrand    youxvx  shown figure  function odd ie       product evaluates  therefore sin cos orthogonal functions   therefore integral limits    figure     sinx cosx remark holds collection functions  cosx cosx cosx      ie pair functions orthogonal integrate orthogonal other collection functions  spans   large subspace functions periodic  projecting functions subspace fundamental idea fourier series section  look second type unconventional inner products inner product random variables  orthogonal projections projections important class linear transformations besides rota tions reﬂections play important role graphics coding the ory statistics machine learning machine learning deal data highdimensional highdimensional data hard analyze visualize however highdimensional data pos sesses property dimensions contain information dimensions essential key properties data compress visualize highdimensional data lose information minimize compression loss ideally ﬁnd informative dimensions data discussed   data represented vectors  discuss fundamental tools data compression speciﬁcally project original highdimensional data lowerdimensional feature space work lowerdimensional space learn dataset extract relevant patterns example machine    deisenroth   faisal   ong published cambridge university press  feature common expression data representation xsinxcosx figure  orthogonal projection orange dots twodimensional dataset blue dots onedimensional subspace straight line projection projection matrix line analytic geometry learning algorithms principal component analysis pca pear son  hotelling  deep neural networks eg deep autoencoders deng et al  heavily exploit idea dimension ality reduction following focus orthogonal projections use   linear dimensionality reduction   classiﬁcation linear regression discuss   interpreted orthogonal projections given lowerdimensional subspace orthogonal projections highdimensional data retain information possible minimize difference error original data corresponding projection il lustration orthogonal projection given figure  obtain projections let deﬁne projection actually is deﬁnition  projection let  vector space subspace   linear mapping       called projection    linear mappings expressed transformation matrices see section  preceding deﬁnition applies equally special kind transformation matrices projection matrices   exhibit property       following derive orthogonal projections vectors inner product space right now  subspaces start one dimensional subspaces called lines mentioned oth erwise assume dot product  xcidy inner product cid cid   cid cid  projection onedimensional subspaces lines assume given line onedimensional subspace ori right now line onedimensional subspace gin basis vector  right  seek closest  geometric arguments let vector πyou  right spanned  project  draft  mathematics machine learning feedback smmlbookcom xx  orthogonal projections figure  examples projections onedimensional subspaces πyou   projection    subspace basis vector  cos  sin   projection twodimensional vector  cidxcid   onedimensional subspace spanned  characterize properties projection πyou  figure  serves illustration cid πyou  cid minimal follows segment πyou  projection πyou  closest  closest implies distance πyou   orthogonal  basis vector   orthogonality condition yields   angles vectors deﬁned inner product projection πyou   element and there fore multiple basis vector  spans  hence πyou   λb πyou  cid   cid following steps determine coordinate  projection right  πyou   projection matrix   maps   finding coordinate  orthogonality condition yields πyou   cid   cid  cid exploit bilinearity inner product arrive cid πyou xλb λb       cid     cid  cid cid   cid cid     cid cid    cid cid cid cid step exploited fact inner products symmet ric choose dot product obtain cid cid   bcidx bcidb bcidx cid cid   coordinate  projection given bcidx cid cid    deisenroth   faisal   ong published cambridge university press  coordinate πyou  respect  general inner product   cidx bcid cidbcid   analytic geometry  finding projection point πyou  diately obtain   πyou   λb imme πyou   λb  cid cid equality holds dot product only compute length πyou  means deﬁnition     cid      cid cid bcidx cid πyou  cid cid λb cid cid times length  hence projection length adds intuition  coordinate πyou  respect basis vector  spans onedimensional subspace  use dot product inner product  cid cid πyou  cid cid   cid bcidx  cid cid cid cos   cid cid cid cid cid cid   cid cid cos   cid cid here  angle   equation familiar    lies unit circle follows trigonometry projection horizontal axis spanned  exactly cos  length corresponding vector πyou   illustration given figure  cos  cid cid  finding projection matrix   know projection lin ear mapping see deﬁnition  therefore exists projection matrix   πyou    πx dot product inner product πyou   λb  bλ   bcidx cid cid   bbcid    cid cid immediately    bbcid cid cid note bbcid and consequently   symmetric matrix of rank scalar cid projection matrix   projects vector  origin direction  equivalently subspace spanned  right line   cid   cid cid right ndimensional vector remark projection πyou  scalar however longer require  coordinates represent projection single want express respect basis vector  spans subspace   draft  mathematics machine learning feedback smmlbookcom horizontal axis onedimensional subspace projection matrices symmetric  orthogonal projections πyou  πyou  figure  projection twodimensional subspace basis   projection πyou     expressed linear combination   displacement vector   πyou  orthogonal   example  projection line projection matrix   line origin spanned   cid  cidcid   direction basis onedimensional subspace line origin  obtain    bbcid bcidb cid  cid             let choose particular  lies subspace spanned    cid  cidcid            πyou    πx   projection span   note application   πyou  change anything ie  ππyou   πyou  expected according deﬁnition  know projection matrix   satisﬁes   πx   πx  remark results   πyou  eigenvector   corresponding eigenvalue   projection general subspaces following look orthogonal projections vectors  lowerdimensional subspaces illustration given figure  right right dimyou    cid  assume      bm ordered basis  projection πyou  necessarily element  therefore represented given set spanning vectors basis sure determine basis      bm proceeding    deisenroth   faisal   ong published cambridge university press  basis vectors form columns   right nowm        bm analytic geometry linear combinations basis vectors      bm  πyou   cidm  λibi  case follow threestep procedure ﬁnd projec tion πyou  projection matrix    coordinates      λm projection with respect basis  linear combination πyou   λibi  bλ  cid        bm right nowm        λmcid rm  right now  case closest means minimum closest  distance implies vector connecting πyou  right orthogonal basis vectors  therefore obtain  simultaneous conditions assuming dot product inner product   cid πyou  cid   πyou     bcid  bcid which πyou   bλ written bm  cid πyou  cid mx πyou    bcid   bcid mx bλ   bλ   obtain homogeneous linear equation bcid bcid    bλ bcidx bλ   bcidbλ  bcidx  expression called normal equation      bm basis and therefore linearly independent bcidb rmm reg ular inverted allows solve coefﬁcients coordinates   bcidbbcidx  matrix bcidbbcid called pseudoinverse  computed nonsquare matrices  requires bcidb positive deﬁnite case  rank practical ap plications eg linear regression add jitter term cidi draft  mathematics machine learning feedback smmlbookcom normal equation pseudoinverse  orthogonal projections bcidb guarantee increased numerical stability positive deﬁnite ness ridge rigorously derived bayesian inference   details  projection πyou   established πyou   bλ therefore  πyou   bbcidbbcidx   projection matrix    immediately projection matrix solves  πx  πyou     bbcidbbcid  remark solution projecting general subspaces includes  case special case dimyou    bcidb  scalar rewrite projection matrix     bbcidbbcid    bbcid bcidb  exactly projection matrix  example  projection twodimensional subspace subspace  span       ﬁnd coordinates   terms subspace  projection point πyou  projection matrix   first generating set basis linear indepen dence write basis vectors matrix         second compute matrix bcidb vector bcidx bcidb  cid   cid            cid cid     bcidx  cid   cid      cid cid third solve normal equation bcidbλ  bcidx ﬁnd  cid cid cid    cid cidλ   cid cid cid  fourth projection πyou    ie column space  directly computed πyou   bλ       deisenroth   faisal   ong published cambridge university press  projection error projection error called reconstruction error ﬁnd approximate solutions unsolvable linear equation systems projections leastsquares solution analytic geometry corresponding projection error norm difference vector original vector projection  ie cid cid πyou  cid cid cid cid  cidcidcid cid cid     given fifth projection matrix for       bbcidbbcid    veri results  check displacement vector  orthogonal basis vectors   veri  see deﬁnition  πyou       remark projections πyou  vectors right lie right now however represent projected mdimensional subspace vector need  coordinates      λm respect basis vectors      bm  remark vector spaces general inner products pay attention computing angles distances deﬁned means inner product projections allow look situations linear ax   solution recall means  lie span  ie vector  lie subspace spanned columns  given linear equation solved exactly ﬁnd approximate solution idea ﬁnd vector subspace spanned columns closest  ie compute orthogonal projection  subspace spanned columns  problem arises practice solution called leastsquares solution assuming dot product inner product overdetermined system discussed section  reconstruction errors  possible approach derive principal component analysis section  remark looked projections vectors  subspace basis vectors  basis onb ie   satisﬁed projection equation  simpliﬁes greatly      bk bcidb  coordinates πyou   bbcidx   bcidx  means longer compute inverse  saves computation time draft  mathematics machine learning feedback smmlbookcom  orthogonal projections  gramschmidt orthogonalization projections core gramschmidt method allows constructively transform basis      bn ndimensional vector space  orthogonalorthonormal basis      un   basis exists liesen mehrmann  spanb     bn  spanu     un gramschmidt orthogonalization method iteratively constructs orthogonal basis      un basis      bn  follows    uk  bk πspanuukbk            kth basis vector bk projected subspace spanned ﬁrst   constructed orthogonal vectors      uk sec tion  projection subtracted bk yields vector uk orthogonal  dimensional subspace spanned      uk repeating procedure  basis vectors      bn yields orthogonal basis      un   normalize uk obtain onb           uk cid cid example  gramschmidt orthogonalization πspanub πspanub  original nonorthogonal basis vectors    new basis vector    projection  subspace spanned  orthogonal basis vectors      πspanub consider basis        cid cid cid cid figure  gramschmidt method construct orthogonal basis    follows assuming dot product inner product     cid cid    πspanub   uucid    cid cid cid cid cid    cid cid cid cid cid    deisenroth   faisal   ong published cambridge university press  gramschmidt orthogonalization figure  gramschmidt orthogonalization  nonorthogonal basis     ﬁrst constructed basis vector  orthogonal projection  spanu  orthogonal basis     setting figure  projection afﬁne space  original setting  setting shifted    projected direction space   projection translated   πyou    gives ﬁnal orthogonal projection πlx analytic geometry πlx πyou     reduce problem pro jection πyou vector sub space  add support point afﬁne projection πl steps illustrated figures   immediately   orthogonal ie youcid      projection afﬁne subspaces far discussed project vector lowerdimensional subspace  following provide solution projecting vector afﬁne subspace consider setting figure  given afﬁne space        basis vectors  determine orthogonal projection πlx   transform problem problem know solve projection vector subspace order there subtract support point      exactly vector subspace  use orthogonal projections subspace discussed section  obtain projection πyou   illustrated figure  projection translated  adding  obtain orthogonal projection afﬁne space  πlx    πyou    πyou  direction space  figure   orthogonal projection subspace  ie figure  evident distance  afﬁne space  identical distance    ie dx   cid  dx πlx cid  πyou    πyou  cid   dx cid    use projections afﬁne subspace derive concept separating hyperplane section  draft  mathematics machine learning feedback smmlbookcom  rotations figure  rotation rotates objects plane origin rotation angle positive rotate counterclockwise figure  robotic arm needs rotate joints order pick objects place correctly figure taken deisenroth et al   rotations length angle preservation discussed section  characteristics linear mappings orthogonal transformation matri ces following closer look speciﬁc orthogonal transformation matrices rotations rotation linear mapping more speciﬁcally automorphism euclidean vector space rotates plane angle  origin ie origin ﬁxed point positive angle    com mon convention rotate counterclockwise direction example shown figure  transformation matrix rotation cid cid important application areas rotations include graphics robotics example robotics important know rotate joints robotic arm order pick place object figure     deisenroth   faisal   ong published cambridge university press  originalrotatedby  analytic geometry figure  rotation standard basis  angle  φe   sin  cos θcid cos  φe  cos  sin θcid sin  sin  cos  rotation matrix cid consider standard basis  rotations  cid cid cid standard coordinate  aim rotate coordinate angle  illustrated figure  note rotated vectors linearly independent and therefore basis  means rotation performs basis change  deﬁnes      cidcid rotations  linear mappings express rotation matrix rθ trigonometry see figure  allows de termine coordinates rotated axes the image  respect standard basis  obtain φe   φe  cid cidcos  sin  cid cid sin  cos  therefore rotation matrix performs basis change rotated coordinates rθ given rθ  cidφe φecid  cidcos  sin  cid sin  cos   rotations  contrast  case  rotate twodimensional plane onedimensional axis easiest way speci general rota tion matrix speci images standard basis    supposed rotated making sure images re re re orthonormal other obtain general rotation matrix  combining images standard basis meaningful rotation angle deﬁne coun terclockwise means operate dimensions use convention counterclockwise planar rotation axis refers rotation axis look axis head on end origin  planar rotations standard basis vectors see figure  draft  mathematics machine learning feedback smmlbookcom figure  rotation vector gray  angle  eaxis rotated vector shown blue  rotations rotation eaxis rθ  cidφe φe φecid   cos   sin    sin  cos  here  coordinate ﬁxed counterclockwise rotation performed ee plane rotation eaxis rotate ee plane  axis need look  axis tip origin rotation eaxis rθ   sin  cos  sin   cos    rθ  cos  sin  sin   cos    figure  illustrates this  rotations  dimensions generalization rotations   ndimensional eu clidean vector spaces intuitively described ﬁxing   dimen sions restrict rotation twodimensional plane ndimen sional space threedimensional case rotate plane twodimensional subspace right now deﬁnition  givens rotation let  ndimensional euclidean  automorphism transformation ma vector space       deisenroth   faisal   ong published cambridge university press  trix rijθ  cos  sin     ji    sin  cos  nj analytic geometry right nown  givens rotation     cid   cid   essentially rijθ identity matrix      rijθ called givens rotation rii  cos   rij  sin   rji  sin   rjj  cos   dimensions ie    obtain  special case  properties rotations rotations exhibit number useful properties derived considering orthogonal matrices deﬁnition  cid cid rθy cid rθx cid rotations preserve distances ie words rotations leave distance points unchanged transformation rotations preserve angles ie angle rθx rθy equals angle   rotations or more dimensions generally commuta tive therefore order rotations applied important rotate point dimensions vector rotations commutative rφrθ  rθrφ   form abelian group with multiplication   rotate point eg origin  reading  gave brief overview important concepts analytic geometry use later  book broader indepth overview concepts presented refer following excellent books axler  boyd vandenberghe  inner products allow determine speciﬁc bases vector subspaces vector orthogonal orthogonal bases gramschmidt method bases important optimization numerical algorithms solving linear equation systems instance krylov subspace methods conjugate gradients generalized minimal residual method gmres minimize residual errors or thogonal stoer burlirsch  machine learning inner products important context draft  mathematics machine learning feedback smmlbookcom  reading kernel methods scholkopf smola  kernel methods exploit fact linear algorithms expressed purely inner prod uct computations then kernel trick allows compute inner products implicitly potentially inﬁnitedimensional feature space knowing feature space explicitly allowed nonlinearization algorithms machine learning kernelpca scholkopf et al  dimensionality reduction gaus sian processes rasmussen williams  fall category kernel methods current state art probabilistic re gression ﬁtting curves data points idea kernels explored   projections graphics eg generate shad ows optimization orthogonal projections iteratively minimize residual errors applications machine learning eg linear regression want ﬁnd linear function minimizes residual errors ie lengths orthogonal projec tions data linear function bishop  investi gate   pca pearson  hotelling  uses projections reduce dimensionality highdimensional data discuss      deisenroth   faisal   ong published cambridge university press  analytic geometry cid cid deﬁned    xcid      ycid   exercises cidx ycid  xy  xy  xy  xy inner product  consider  cid cid deﬁned    cid cid inner product  compute distance  cidx ycid  xcidy  cidx ycid  xciday    compute angle cidx ycid  xcid cid cid   cid cidcid cid               cid cid cid cid  cidx ycid  xcidy  cidx ycid  xcidby     consider euclidean vector space  dot product subspace cid cid      given  span      determine orthogonal projection πyou    determine distance dx   consider  inner product cidx ycid  xcid        furthermore deﬁne    standardcanonical basis  draft  mathematics machine learning feedback smmlbookcom exercises  determine orthogonal projection πyou    spane   hint orthogonality deﬁned inner product  compute distance de   draw scenario standard basis vectors πyou  let  vector space  endomorphism    prove  projection idv   projection idv identity endomorphism    assume  projection calculate amidv  keridv  function amπ kerπ  gramschmidt method turn basis     two dimensional subspace   onb              let    let      xn    positive real numbers       xn   use cauchyschwarz inequality  cidn  cidn hint think dot product right now then choose speciﬁc vectors    right apply cauchyschwarz inequality   xi cid  cid   rotate vectors      cid cid cid cid     deisenroth   faisal   ong published cambridge university press  matrix decompositions    studied ways manipulate measure vectors projections vectors linear mappings mappings transforma tions vectors conveniently described operations performed matrices moreover data represented matrix form well eg rows matrix represent different people columns different features people weight height socio economic status  present aspects matrices summarize matrices matrices decomposed decompositions matrix approximations ﬁrst consider methods allow matrices numbers characterize overall properties matrices sections determinants section  eigenval ues section  important special case square matrices characteristic numbers important mathematical consequences allow quickly grasp useful properties matrix has proceed matrix decomposition methods analogy ma trix decomposition factoring numbers factoring  reason matrix decomposition  prime numbers  referred matrix factorization matrix decompositions matrix means different representation factors interpretable matrices ﬁrst cover squarerootlike operation symmetric positive deﬁnite matrices cholesky decomposition section  look related methods factorizing matrices canoni cal forms ﬁrst known matrix diagonalization section  allows represent linear mapping diagonal trans formation matrix choose appropriate basis second method singular value decomposition section  extends factorization nonsquare matrices considered fundamental concepts linear algebra decompositions helpful matrices represent ing numerical data large hard analyze conclude  systematic overview types matrices characteristic properties distinguish form matrix tax onomy section  methods cover  important material published cambridge university press mathematics machine learning marc peter deisenroth  aldo faisal cheng soon ong  version free view download personal use only redistribution resale use derivative works by   deisenroth   faisal   ong  smmlbookcom matrix factorization  determinant trace determinant tests invertibility cholesky eigenvalues   probability  distributions usedin eigenvectors constructs orthogonal matrix diagonalization figure  mind map concepts introduced parts book     svd   dimensionality reduction subsequent mathematical    applied  dimensionality reduction   den sity estimation    overall structure depicted mind map figure   determinant trace determinant notation  confused absolute value determinants important concepts linear algebra determinant mathematical object analysis solution systems linear right nown equations determinants deﬁned square matrices ie matrices number rows columns book write determinant deta cid cid cid cid cid cid cid cid cid     an an    an    an       ann right nown function maps determinant determinant square matrix deta  cid cid cid cid cid cid cid cid cid    deisenroth   faisal   ong published cambridge university press  matrix decompositions real number providing deﬁnition determinant general   matrices let look motivating examples deﬁne determinants special matrices example  testing matrix invertibility let begin exploring square matrix invertible see sec tion  smallest cases know matrix  matrix ie scalar number invertible      holds  matrices deﬁnition inverse deﬁnition         know aa   then  inverse   aa aa cid  cid hence invertible aa aa     ie quantity determinant cid cid cid cid cid     deta  cid cid cid cid cid  aa aa  example  points relationship determinants existence inverse matrices theorem states result   matrices theorem  square matrix deta   right nown holds invertible explicit closedform expressions determinants small matrices terms elements matrix    deta  deta       deta  cid cid cid cid     cid cid cid cid  aa aa  observed preceding example    known sarrus rule cid cid cid cid cid cid          cid cid cid cid cid cid  aaa  aaa  aaa aaa aaa aaa  draft  mathematics machine learning feedback smmlbookcom cid cid cid  determinant trace memory aid product terms sarrus rule try tracing elements triple products matrix square matrix  uppertriangular matrix tij     ie matrix zero diagonal analogously deﬁne lowertriangular matrix matrix zeros diagonal tri right nown determinant product diagonal angular matrix  elements ie uppertriangular matrix lowertriangular matrix dett   tii  cid determinant signed volume parallelepiped formed columns matrix figure  area parallelogram shaded region spanned vectors   detb  figure  volume parallelepiped shaded volume spanned vectors    detr   sign determinant indicates orientation spanning vectors example  determinants measures volume notion determinant natural consider mapping set  vectors spanning object right now turns de terminant deta signed volume ndimensional parallelepiped formed columns matrix     columns matrix form parallelogram fig ure  angle vectors gets smaller area parallel ogram shrinks too consider vectors   form columns matrix    then absolute value determinant area parallelogram vertices       particular    longer linearly dependent   λg  form twodimensional parallelogram therefore corresponding area  contrary   linearly independent multiples cid    bg cid   bg      determinant canonical basis vectors   written   cid cid cid cid cid cid cid cid sign determinant indicates orientation spanning vectors   respect standard basis   ﬁgure ﬂip ping order   swaps columns reverses orientation shaded area familiar formula area  height length intuition extends higher dimensions  consider  spanning edges parallelepiped ie vectors    solid faces parallel parallelograms see figure  ab  matrix    volume solute value determinant  solid thus determinant acts function measures signed volume formed column vectors composed matrix consider linearly independent vectors     given cid cidb              deisenroth   faisal   ong published cambridge university press  laplace expansion detakj  called minor kj detakj  cofactor matrix decompositions writing vectors columns matrix allows compute desired volume          deta    computing determinant   matrix requires general algo rithm solve cases    going explore fol lowing theorem  reduces problem computing deter minant  matrices recursively applying laplace expansion theorem   matrices ultimately compute determinants  computing determinants   matrix computing determinant   matrices theorem  laplace expansion consider matrix         right nown then  expansion column  deta  kjakj detakj   expansion row  deta  kjajk detajk  akj ing row  column  rnn submatrix obtain delet cid cid example  laplace expansion let compute determinant          laplace expansion ﬁrst row applying  yields cid cid cid cid cid cid          cid cid cid cid cid cid   cid cid cid cid     cid cid cid cid   cid cid cid cid     cid cid cid cid   cid cid cid cid     cid cid cid cid draft  mathematics machine learning feedback smmlbookcom  determinant trace use  compute determinants   matrices obtain deta          completeness compare result computing determi nant sarrus rule  deta           right nown determinant exhibits following properties deta  determinant matrix product product corresponding determinants detab  detadetb determinants invariant transposition ie deta  detacid regular invertible deta   similar matrices deﬁnition  possess determinant there fore linear mapping     transformation matrices aφ  determinant thus determinant invariant choice basis linear mapping adding multiple columnrow change deta multiplication columnrow  particular detλa  λn deta swapping rowscolumns changes sign deta  scales deta  properties use gaussian elimination see section  compute deta bringing rowechelon form stop gaussian elimination triangular form elements diagonal  recall  determinant triangular matrix product diagonal elements theorem  square matrix rka   words invertible rank right nown deta   mathematics mainly performed hand determinant calculation considered essential way analyze matrix invertibil ity however contemporary approaches machine learning use direct numerical methods superseded explicit calculation deter minant example   learned inverse matrices computed gaussian elimination gaussian elimination compute determinant matrix determinants play important theoretical role following sections especially learn eigenvalues eigenvectors section  characteristic polynomial deﬁnition  trace square matrix right nown deﬁned trace    deisenroth   faisal   ong published cambridge university press  cid trace invariant cyclic permutations matrix decompositions tra  aii  cid ie  trace sum diagonal elements  trace satisﬁes following properties tra    tra  trb   trαa  αtra   tri    trab  trba right nowk  right nown rkn right nown shown function satisﬁes properties to gether  trace gohberg et al  properties trace matrix products general specif ically trace invariant cyclic permutations ie trakl  trkla rla property generalizes matrices products arbitrary number matrices special case  right follows vectors   rak  rkl  trxycid  trycidx  ycidx given linear mapping       vector space deﬁne trace map trace matrix representation  given basis    means transfor mation matrix  trace  trace  different basis   holds corresponding transformation matrix   obtained basis change form sas suitable  see section  corresponding trace  means trb  trsas  trass  tra  hence matrix representations linear mappings basis depen dent trace linear mapping  independent basis section covered determinants traces functions char acterizing square matrix taking understanding determi nants traces deﬁne important equation describing matrix terms polynomial use extensively following sections deﬁnition  characteristic polynomial  trix right nown  square ma paλ  deta λi    cλ  cλ   cnλn   nλn     characteristic polynomial      cn  characteristic polynomial  particular draft  mathematics machine learning feedback smmlbookcom  eigenvalues eigenvectors   deta  cn   ntra  characteristic polynomial  allow compute eigen values eigenvectors covered section  eigenvalues eigenvectors know new way characterize matrix associ ated linear mapping recall section  linear mapping unique transformation matrix given ordered basis in terpret linear mappings associated transformation matrices performing eigen analysis see eigenvalues lin ear mapping tell special set vectors eigenvectors transformed linear mapping deﬁnition  let eigenvalue  right nown square matrix  corresponding eigenvector right ax  λx  eigenvalue eigenvector eigen german word meaning characteristic self own  eigenvalue equation eigenvalue equation remark linear algebra literature software conven tion eigenvalues sorted descending order largest eigenvalue associated eigenvector called ﬁrst eigenvalue associated eigenvector second largest called second eigen value associated eigenvector on however textbooks publications different notion orderings want presume ordering book stated explicitly following statements equivalent right nown  eigenvalue right exists  λi nx   solved nontrivially ie  rka deta λi    λi      ax  λx equivalently  deﬁnition  collinearity codirection vectors point direction called codirected vectors collinear point opposite direction codirected collinear remark nonuniqueness eigenvectors  eigenvector associated eigenvalue   holds cx eigenvector eigenvalue acx  cax  cλx  λcx  thus vectors collinear  eigenvectors     deisenroth   faisal   ong published cambridge university press  cid theorem   root characteristic polynomial paλ   eigenvalue matrix decompositions right nown  algebraic multiplicity deﬁnition  let square matrix eigenvalue λi algebraic multiplicity λi number times root appears character istic polynomial eigenspace eigenspectrum spectrum right nown set deﬁnition  eigenspace eigenspectrum eigenvectors associated eigenvalue  spans subspace right now called eigenspace respect  denoted eλ set eigenvalues called eigenspectrum spectrum   eigenvalue right nown corresponding eigenspace eλ solution space homogeneous linear equations λix   geometrically eigenvector corresponding nonzero eigenvalue points direction stretched linear mapping eigenvalue factor stretched eigenvalue negative direction stretching ﬂipped λi   example  the case identity matrix right nown characteristic polynomial piλ  identity matrix λn   eigenvalue    oc deti curs  times moreover ix  λx   holds vectors  this sole eigenspace  identity matrix spans  di mensions  standard basis vectors right eigenvectors  right useful properties eigenvalues eigenvectors include following matrix transpose acid possess eigenvalues necessarily eigenvectors eigenspace eλ null space λi ax  λx ax λx   λix   kera λi similar matrices see deﬁnition  possess eigenvalues therefore linear mapping  eigenvalues independent choice basis transformation matrix makes eigenvalues determinant trace key characteristic param eters linear mapping invariant basis change symmetric positive deﬁnite matrices positive real eigen values draft  mathematics machine learning feedback smmlbookcom  eigenvalues eigenvectors example eigenspaces let ﬁnd eigenvalues eigenvectors  eigenvalues computing eigenvectors  matrix cid cid    step  characteristic polynomial deﬁnition eigen   eigenvalue   vector vector  λix      requires kernel ax  λx ie  λi contains elements  means null space λi   hence need compute roots characteristic polynomial  ﬁnd eigenvalues λi invertible deta step  eigenvalues characteristic polynomial paλ  deta λi cid cidcid     det cidcid cidλ    cid cid cid cid cid cid cid cid     factorize characteristic polynomial obtain pλ      giving roots              step  eigenvectors eigenspaces ﬁnd eigenvectors correspond eigenvalues looking vectors     obtain cid cid cid cidx cid cid cid cidx    solve homogeneous obtain solution space cid cid       span cid   cid eigenspace onedimensional possesses single basis vector analogously ﬁnd eigenvector    solving homoge neous equations cid cid   cid cid           deisenroth   faisal   ong published cambridge university press  cid cid matrix decompositions means vector   eigenvector eigenvalue  corresponding eigenspace given cid    cidx   span cid   cid  cid cid  eigenspaces   example  onedimensional spanned single vector however cases multiple identical eigenvalues see deﬁnition  eigenspace dimension deﬁnition  let λi eigenvalue square matrix  geometric multiplicity λi number linearly independent eigen vectors associated λi words dimensionality eigenspace spanned eigenvectors associated λi remark speciﬁc eigenvalues geometric multiplicity eigenvalue associated eigenvector eigenvalues geometric multiplicity exceed algebraic multiplic ity lower example  matrix  repeated eigenvalues      algebraic multiplicity  eigenvalue has however distinct unit eigenvector   and thus geometric multiplicity  cid cid    cid cid graphical intuition dimensions let gain intuition determinants eigenvectors eigenval ues different linear mappings figure  depicts ﬁve transformation matrices       impact square grid points centered origin cid cid     direction eigenvectors correspond   canonical basis vectors  ie cardinal axes vertical axis extended factor  eigenvalue    horizontal axis compressed factor   eigenvalue      mapping area preserving deta     cid   corresponds shearing mapping  ie shears   cid   points horizontal axis right positive draft  mathematics machine learning feedback smmlbookcom geometric multiplicity geometry areapreserving properties type shearing parallel axis known cavalieris principle equal areas parallelograms katz   eigenvalues eigenvectors figure  determinants eigenspaces overview ﬁve linear mappings associated transformation matrices ai   projecting  colorcoded points    left column target points aix right column central column depicts ﬁrst eigenvector stretched associated eigenvalue  second eigenvector stretched eigenvalue  row depicts effect ﬁve transformation matrices ai respect standard basis half vertical axis left vice versa mapping area preserving deta   eigenvalue      repeated eigenvectors collinear drawn emphasis opposite directions indicates mapping acts direction the horizontal axis cidcos    sin    cid sin    cos        matrix  rotates   points   rad   counterclockwise complex eigen values reﬂecting mapping rotation hence eigenvectors drawn rotation volume preserving deter minant  details rotations refer section  cid cid represents mapping standard basis col   cid  cid lapses twodimensional domain dimension eigen    deisenroth   faisal   ong published cambridge university press  detaλλdetaλλdetaλjλjdetaλλdetaλλ figure  caenorhabditis elegans neural network kaiser hilgetag  sym metrized connectivity matrix  eigenspectrum matrix decompositions value  space direction blue eigenvector corresponding    collapses orthogonal red eigenvector stretches space factor    therefore area image  cid cid deta   shearandstretch mapping scales space      stretches space red eigenvector  factor  compresses orthogonal blue eigenvector factor  example  eigenspectrum biological neural network  connectivity matrix  eigenspectrum methods analyze learn network data essential com ponent machine learning methods key understanding networks connectivity network nodes especially nodes connected not data science applications useful study matrix captures connectivity data build connectivityadjacency matrix  complete neural network worm celegans rowcolumn represents  neurons worms brain connectivity matrix value aij   neuron talks neuron  synapse aij   otherwise connectivity matrix symmetric am plies eigenvalues real valued therefore compute symmetrized version connectivity matrix asym   acid new matrix asym shown figure  nonzero value aij neurons connected white pixels irrespective direction connection figure  correspond ing eigenspectrum asym horizontal axis shows index eigenvalues sorted descending order vertical axis shows corre sponding eigenvalue slike shape eigenspectrum typical biological neural networks underlying mechanism responsible area active neuroscience research draft  mathematics machine learning feedback smmlbookcom neuronindexneuronindexindexofsortedeigenvalueeigenvalue  eigenvalues eigenvectors theorem  eigenvectors      xn matrix distinct eigenvalues      λn linearly independent right nown  theorem states eigenvectors matrix  distinct eigen values form basis right now deﬁnition  square matrix fewer  linearly independent eigenvectors right nown defective possesses defective nondefective matrix right nown necessarily require  dis tinct eigenvalues require eigenvectors form basis right now looking eigenspaces defective matrix follows sum dimensions eigenspaces  speciﬁcally de fective matrix eigenvalue λi algebraic multiplicity    geometric multiplicity  remark defective matrix  distinct eigenvalues distinct eigenvalues linearly independent eigenvectors theorem  rmn obtain sym theorem  given matrix metric positive semideﬁnite matrix  right nown deﬁning   acida  remark rka     acida symmetric positive deﬁnite understanding theorem  holds insightful use symmetrized matrices symmetry requires   scid insert ing  obtain   acida  acidacidcid  acidacid  scid more over positive semideﬁniteness section  requires xcidsx cid  inserting  obtain xcidsx  xcidacidax  xcidacidax  axcidax cid  dot product computes sum squares which nonnegative right nown symmetric ex theorem  spectral theorem ists orthonormal basis corresponding vector space  consisting eigenvectors  eigenvalue real direct implication spectral theorem eigendecompo sition symmetric matrix exists with real eigenvalues ﬁnd onb eigenvectors   dp cid  diagonal columns  contain eigenvectors example  consider matrix               deisenroth   faisal   ong published cambridge university press  spectral theorem matrix decompositions characteristic polynomial paλ    obtain eigenvalues        repeated eigenvalue following standard procedure computing eigenvectors obtain eigenspaces   span    span   cid cidcid cid cid cidcid cid cidcidcidcid  orthogonal   however xcid      orthogonal spectral theorem theorem  states exists orthogonal basis orthogonal however construct one construct basis exploit fact   eigenvec tors associated eigenvalue  therefore   holds aαx  βx  axα  axβ  λαx  βx  ie linear combination   eigenvector as sociated  gramschmidt algorithm section  method iteratively constructing orthogonalorthonormal basis set basis vectors linear combinations therefore   orthogonal apply gramschmidt algorithm ﬁnd eigenvectors associated    orthogonal and  example obtain xcid     xcid     orthogonal other orthogonal  eigenvectors associated    conclude considerations eigenvalues eigenvectors useful tie matrix characteristics concepts determinant trace theorem  determinant matrix eigenvalues ie right nown product deta  λi  cid λi  possibly repeated eigenvalues  draft  mathematics machine learning feedback smmlbookcom cid figure  geometric interpretation eigenvalues eigenvectors stretched corresponding eigenvalues area unit square changes λλ perimeter changes factor      eigenvalues eigenvectors theorem  trace matrix ues ie right nown sum eigenval tra  λi  cid λi  possibly repeated eigenvalues  let provide geometric intuition theorems consider  possesses linearly independent eigenvectors matrix   example assume   onb  orthogonal area square span  figure  section  know determinant computes change area unit square transformation  example compute change area explicitly mapping eigenvectors gives vectors   ax  λx   ax  λx ie new vectors vi scaled versions eigenvectors xi scaling factors corresponding eigenvalues λi   orthogonal area rectangle span given   in example orthonormal directly compute perimeter unit square    mapping eigen vectors creates rectangle perimeter  therefore sum absolute values eigenvalues tells perimeter unit square changes transformation matrix λλ example  googles rank  webs eigenvectors google uses eigenvector corresponding maximal eigenvalue matrix determine rank  search idea rank algorithm developed stanford university larry  sergey brin  importance web  ap proximated importance  link it this write web sites huge directed graph shows  links which rank computes weight importance xi cid  web site ai counting number  pointing ai moreover  ank takes account importance web sites link ai navigation behavior user modeled transition matrix graph tells click probability somebody end    deisenroth   faisal   ong published cambridge university press  matrix decompositions rank different web site matrix property ini tial rankimportance vector  web site sequence  ax ax    converges vector  vector called rank satisﬁes ax   ie eigenvector with corresponding eigenvalue   normalizing    interpret entries cid probabilities details different perspectives rank original technical report  et al  cid cholesky decomposition cholesky factorization cholesky factor  cholesky decomposition ways factorize special types matrices en counter machine learning positive real numbers squareroot operation gives decomposition number  matrices need identical components eg    careful compute squarerootlike operation positive quanti ties symmetric positive deﬁnite matrices see section  choose number squareroot equivalent operations cholesky decompositioncholesky factorization provides squareroot equivalent op eration symmetric positive deﬁnite matrices useful practice theorem  cholesky decomposition symmetric positive deﬁnite matrix factorized product  llcid  lower triangular matrix positive diagonal elements an       an ann   ln       lnn         ln lnn     called cholesky factor   unique       example  cholesky factorization consider symmetric positive deﬁnite matrix ested ﬁnding cholesky factorization  llcid ie  inter            llcid     multiplying righthand yields ll ll ll    ll  ll ll ll  ll         draft  mathematics machine learning feedback smmlbookcom  eigendecomposition diagonalization comparing lefthand  righthand  shows simple pattern diagonal elements lii                similarly elements diagonal lij   repeating pattern           ll  thus constructed cholesky decomposition symmetric pos  matrix key realization backward itive deﬁnite  calculate components lij  be given values aij previously computed values lij cid cid cholesky decomposition important tool numerical computations underlying machine learning here symmetric positive def inite matrices require frequent manipulation eg covariance matrix multivariate gaussian variable see section  symmetric positive deﬁnite cholesky factorization covariance matrix allows generate samples gaussian distribution allows perform linear transformation random variables heavily exploited computing gradients deep stochastic models varia tional autoencoder jimenez rezende et al  kingma welling  cholesky decomposition allows compute determi nants efﬁciently given cholesky decomposition  llcid know deta  detl detlcid  detl  triangular matrix determinant simply product diagonal entries deta  cid ii thus numerical software packages use cholesky decomposition computations efﬁcient  eigendecomposition diagonalization diagonal matrix matrix value zero offdiagonal ele ments ie form diagonal matrix              cn allow fast computation determinants powers inverses determinant product diagonal entries matrix power dk given diagonal element raised power  inverse  reciprocal diagonal elements nonzero section discuss transform matrices diagonal    deisenroth   faisal   ong published cambridge university press  diagonalizable matrix decompositions form important application basis change discussed section  eigenvalues section  recall matrices   similar deﬁnition  ex ists invertible matrix      ap  speciﬁcally look matrices similar diagonal matrices  con tain eigenvalues diagonal right nown diagonalizable deﬁnition  diagonalizable matrix similar diagonal matrix ie exists invertible matrix right nown    ap  following diagonalizing matrix right nown way expressing linear mapping basis see section  turn basis consists eigen vectors  let right nown let      λn set scalars let      pn right nown set vectors right now deﬁne        pn let  diagonal matrix diagonal entries      λn ap         λn eigenvalues      pn cor responding eigenvectors  statement holds ap  ap     pn  ap     apn     λn         pn   λp     λnpn  thus  implies ap  λp apn  λnpn  therefore columns  eigenvectors  deﬁnition diagonalization requires  right nown invertible ie  rank theorem  requires  linearly independent eigenvectors      pn ie pi form basis right now theorem  eigendecomposition square matrix factored   dp   right nown right nown  diagonal matrix diagonal entries eigenvalues  eigenvectors form basis right now draft  mathematics machine learning feedback smmlbookcom figure  intuition eigendecomposition sequential transformations topleft bottomleft   performs basis change here drawn  depicted rotationlike operation standard basis eigenbasis bottomleft bottomright  performs scaling remapped orthogonal eigenvectors depicted circle stretched ellipse bottomright topright  undoes basis change depicted reverse rotation restores original coordinate frame  eigendecomposition diagonalization   theorem  implies nondefective matrices diagonal ized columns   eigenvectors  symmetric matrices obtain stronger outcomes eigenvalue decom position theorem  symmetric matrix  right nown diagonalized theorem  follows directly spectral theorem  more over spectral theorem states ﬁnd onb eigenvectors right now makes  orthogonal matrix    cidap  remark jordan normal form matrix offers decomposition works defective matrices lang  scope book geometric intuition eigendecomposition interpret eigendecomposition matrix follows see figure  let transformation matrix linear mapping respect standard basis ei blue arrows   performs basis change standard basis eigenbasis then diagonal  scales vectors axes eigenvalues λi finally  transforms scaled vectors standardcanonical coordi nates yielding λipi example  eigendecomposition let compute eigendecomposition   cid  cid step  compute eigenvalues eigenvectors characteristic    deisenroth   faisal   ong published cambridge university press  eeppppeeppλpλpeeaeae  matrix decompositions polynomial cidcid  cidcid    deta λi  det         therefore eigenvalues     the roots characteristic polynomial associated normalized eigenvectors obtained                 ap    ap    yields      cid cid  cid cid step  check existence eigenvectors   form basis  therefore diagonalized step  construct matrix  diagonalize  collect eigen vectors  obtain      cid  cid    ap     cid  cid figure  visualizes eigendecomposition cid cid  sequence linear transformations equivalently exploiting     cid eigenvectors   example form onb cid  cid   cid  cid cid cid cid cid cidcid   cid cidcid cid cid cidcid cid cid cid  cidcid cid cid diagonal matrices  efﬁciently raised power therefore right nown eigenvalue ﬁnd matrix power matrix decomposition if exists ak   dp    dkp   computing dk efﬁcient apply operation individually diagonal element assume eigendecomposition   dp  exists then deta  detp dp   detp  detd detp  draft  mathematics machine learning feedback smmlbookcom  singular value decomposition  detd  cid dii allows efﬁcient computation determinant  eigenvalue decomposition requires square matrices useful perform decomposition general matrices sec tion introduce general matrix decomposition technique singular value decomposition  singular value decomposition singular value decomposition svd matrix central matrix decomposition method linear algebra referred fundamental theorem linear algebra strang  applied matrices square matrices exists moreover explore following svd matrix    quantiﬁes change represents linear mapping    underlying geometry vector spaces recom mend work kalman  roy banerjee  deeper overview mathematics svd theorem  svd theorem let rank   minm  svd decomposition form rmn rectangular matrix  cid orthogonal matrix orthogonal matrix  moreover   rmm column vectors ui        right nown column vectors vj          matrix σii  σi cid  σij     diagonal entries σi         called singular values ui called leftsingular vectors vj called rightsingular vectors convention singular values ordered ie  cid  cid σare cid  singular value matrix  unique requires attention rmn rectangular particular  observe  size  means  diagonal submatrix contains singular values needs additional zero padding speciﬁcally    matrix  diagonal structure row  consists    deisenroth   faisal   ong published cambridge university press  svd theorem svd singular value decomposition singular values leftsingular vectors rightsingular vectors singular value matrix cid figure  intuition svd matrix   sequential transformations topleft bottomleft  cid performs basis change  bottomleft bottomright  scales maps   ellipse bottomright lives dimension orthogonal surface elliptical disk bottomright topright performs basis change  matrix decompositions σu  cid σe σu σe cid row vectors                σn    matrix  diagonal structure column  columns consist                  σm      rmn remark svd exists matrix  geometric intuitions svd svd offers geometric intuitions transformation matrix  following discuss svd sequential linear trans formations performed bases example  apply transformation matrices svd set vectors  allows visualize effect transformation clearly svd matrix interpreted decomposition corre sponding linear mapping recall section    right rm operations figure  svd intuition follows superﬁcially simi lar structure eigendecomposition intuition figure  broadly speaking svd performs basis change  cid followed scal ing augmentation or reduction dimensionality singular draft  mathematics machine learning feedback smmlbookcom useful review basis changes section  orthogonal matrices deﬁnition  orthonormal bases section   singular value decomposition value matrix  finally performs second basis change  svd entails number important details caveats review intuition detail assume given transformation matrix linear mapping   rm respect standard bases   right rm right respectively moreover assume second basis  right  rm  matrix  performs basis change domain right  rep resented red orange vectors   topleft fig ure  standard basis   cid    performs basis change   red orange vectors aligned canonical basis bottomleft figure   having changed coordinate   scales new coordi nates singular values σi and adds deletes dimensions ie  transformation matrix  respect   rep resented red orange vectors stretched lying ee plane embedded dimension bottomright figure   performs basis change codomain rm  canoni cal basis rm represented rotation red orange vectors ee plane shown topright figure  svd expresses change basis domain codomain contrast eigendecomposition operates vector space basis change applied un done makes svd special different bases simultaneously linked singular value matrix   ﬁt box  centered origin standard basis map   example  vectors svd consider mapping square grid vectors size  vectors   σv cid cid cid   colored dots topleft panel fig start set vectors ure  arranged grid apply  cid rotated vectors shown bottomleft panel figure  map vectors singular value matrix  codomain  see bottomright panel figure  note vectors lie  rotates    deisenroth   faisal   ong published cambridge university press  matrix decompositions xx plane coordinate  vectors xx plane stretched singular values direct mapping vectors codomain  equals σv cid performs rotation transformation codomain  mapped vectors longer restricted xx plane plane shown topright panel figure  figure  svd mapping vectors represented discs panels follow anticlockwise structure figure   construction svd discuss svd exists compute detail svd general matrix shares similarities eigendecomposition square matrix remark compare eigendecomposition spd matrix   scid   dp cid draft  mathematics machine learning feedback smmlbookcom xxxxxxxxxx  singular value decomposition corresponding svd set   σv cid           svd spd matrices eigendecomposition following explore theorem  holds rmn equivalent svd constructed computing svd ﬁnding sets orthonormal bases       um        vn codomain rm domain right now respectively ordered bases construct matrices   plan start constructing orthonormal set right right now construct orthonormal set singular vectors      vn rm thereafter link leftsingular vectors      um require orthogonality vi preserved trans formation  important know images avi form set orthogonal vectors normalize images scalar factors turn singular values let begin constructing rightsingular vectors spectral theorem theorem  tells eigenvectors symmetric matrix form onb means diagonalized more over theorem  construct symmetric positive semideﬁnite matrix acida rmn thus diagonalize acida obtain right nown rectangular matrix acida   dp cid              cid  λn  orthogonal matrix composed orthonormal eigenbasis λi cid  eigenvalues acida let assume svd exists inject   yields acida  you σv cidcidyou σv cid   σcidyou cidyou σv cid    orthogonal matrices therefore cidyou  ob tain acida   σcidσv cid        cid  comparing   identi  cid   cid   λi     deisenroth   faisal   ong published cambridge university press  matrix decompositions therefore eigenvectors acida compose  rightsingular vectors  see  eigenvalues acida squared singular values  see  obtain leftsingular vectors  follow similar procedure rmm start computing svd symmetric matrix aacid instead previous acida right nown svd yields aacid  you σv cidyou σv cidcid  σv cidv σcidyou cid  cid     spectral theorem tells aacid  sdscid diagonalized ﬁnd onb eigenvectors aacid collected  orthonormal eigenvectors aacid leftsingular vectors form orthonormal basis codomain svd leaves question structure matrix  aacid acida nonzero eigenvalues see   nonzero entries  matrices svd cases same step link parts touched far orthonormal set rightsingular vectors   ﬁnish construc tion svd connect orthonormal vectors  reach goal use fact images vi orthogonal too results section  require inner product avi avj    orthogonal eigenvectors vi vj   holds avicidavj  vcid acidavj  vcid λjvj  λjvcid vj    case  cid  holds dimensional subspace rm av     avr basis  complete svd construction need leftsingular vectors orthonormal normalize images rightsingular vectors avi obtain ui  avi avi cid cid λi σi avi  avi  equality obtained   showing eigenvalues aacid  therefore eigenvectors acida know right singular vectors vi normalized images  leftsingular vectors ui form selfconsistent onbs connected singular value matrix   λi let rearrange  obtain singular value equation avi  σiui         draft  mathematics machine learning feedback smmlbookcom singular value equation cid cid  singular value decomposition equation closely resembles eigenvalue equation  vectors left righthand sides same     holds cid   says ui   however know construction or thonormal conversely     holds cid    avi   know vi form orthonormal set means svd supplies orthonormal basis kernel null space  set vectors  ax   see section  concatenating vi columns  ui columns yields av     dimensions diagonal structure rows       hence rightmultiplying  cid yields  σv cid svd  example  computing svd let ﬁnd singular value decomposition      cid cid  svd requires compute rightsingular vectors vj singular values σk leftsingular vectors ui step  rightsingular vectors eigenbasis acida start computing acida  cid       cid     compute singular values rightsingular vectors vj eigenvalue decomposition acida given          acida     dp cid  obtain rightsingular vectors columns        step  singularvalue matrix singular values σi square roots eigenvalues    deisenroth   faisal   ong published cambridge university press  matrix decompositions acida obtain straight  rka   nonzero singular values       singular value matrix size  obtain   cid cid     step  leftsingular vectors normalized image right singular vectors ﬁnd leftsingular vectors computing image right singular vectors normalizing dividing corresponding singular value obtain cid cid    cid cid   cid cid    av    av      cid       cid       cid  cid   note approach illustrated poor numerical behavior svd normally computed resorting eigenvalue decomposition acida  eigenvalue decomposition vs singular value decomposition let consider eigendecomposition   dp  svd  σv cid review core elements past sections svd exists matrix rmn eigendecomposition deﬁned square matrices right nown exists ﬁnd basis eigenvectors right now vectors eigendecomposition matrix  necessarily orthogonal ie change basis simple rotation scaling hand vectors matrices  svd orthonormal represent rotations eigendecomposition svd compositions linear mappings  change basis domain  independent scaling new basis vector mapping do main codomain  change basis codomain draft  mathematics machine learning feedback smmlbookcom  singular value decomposition star wars blade runner amelie delicatessen figure  movie ratings people movies svd decomposition key difference eigendecomposition svd svd domain codomain vector spaces different dimensions svd left rightsingular vector matrices  generally inverse they perform basis changes dif ferent vector spaces eigendecomposition basis change ma trices    inverses other svd entries diagonal matrix  real non negative generally true diagonal matrix eigendecomposition svd eigendecomposition closely related projections  leftsingular vectors eigenvectors aacid  rightsingular vectors eigenvectors acida  nonzero singular values square roots nonzero eigenvalues aacid acida right nown eigenvalue decomposition symmetric matrices svd same follows spectral theo rem  example  finding structure movie ratings consumers let add practical interpretation svd analyzing data people preferred movies consider viewers ali beatrix chandra rating different movies star wars blade runner amelie delicatessen ratings values  worst  best  shown figure  row encoded data matrix represents movie column user thus column vectors movie ratings viewer xali xbeatrix xchandra    deisenroth   faisal   ong published cambridge university press  matrix decompositions factoring svd offers way capture relationships people rate movies especially structure linking people like movies applying svd data matrix makes number assumptions  viewers rate movies consistently linear mapping  errors noise ratings  interpret leftsingular vectors ui stereotypical movies rightsingular vectors vj stereotypical viewers assumption viewers speciﬁc movie preferences expressed linear combination vj similarly movies likeability expressed linear combination ui therefore vector domain svd interpreted viewer space stereotypical viewers vector codomain svd correspondingly movie space stereotypical movies let inspect svd movieuser matrix ﬁrst leftsingular vector  large absolute values science ﬁction movies large ﬁrst singular value red shading figure  thus groups type users speciﬁc set movies science ﬁction theme similarly ﬁrst rightsingular  shows large absolute values ali beatrix high ratings science ﬁction movies green shading figure  suggests  reﬂects notion science ﬁction lover similarly  capture french art house ﬁlm theme  in dicates chandra close idealized lover movies ide alized science ﬁction lover purist loves science ﬁction movies science ﬁction lover  gives rating zero science ﬁction themedthis logic implied diagonal substructure singular value matrix  speciﬁc movie represented decomposes linearly stereotypical movies likewise person represented decompose via linear combination movie themes worth brieﬂy discuss svd terminology conventions different versions literature differences confusing mathematics remains invariant them convenience notation abstraction use svd notation svd described having square left rightsingular vector matrices nonsquare singular value matrix deﬁni tion  svd called svd authors deﬁne svd bit differently focus square sin gular matrices then rmn  cid  mn mn nn  cid nn draft  mathematics machine learning feedback smmlbookcom spaces meaningfully spanned respective viewer movie data data covers sufﬁcient diversity viewers movies svd  matrix approximation formulation called reduced svd eg datta  reduced svd svd eg press et al  alternative format changes merely matrices constructed leaves mathematical structure svd unchanged convenience alternative formulation  diagonal eigenvalue decomposition section  learn matrix approximation techniques svd called truncated svd possible deﬁne svd rankr matrix  matrix construction similar deﬁnition ensures diagonal matrix  nonzero entries diagonal main convenience alternative notation  diagonal eigenvalue decomposition restriction svd applies   matrices    practically unnecessary    svd decomposition yield  zero columns rows and consequently singular values σm     σn   matrix  diagonal matrix size     truncated svd svd variety applications machine learning leastsquares problems curve ﬁtting solving systems linear equa tions applications harness important properties svd relation rank matrix ability approximate matrices given rank lowerrank matrices substituting matrix svd advantage making calculation robust nu merical rounding errors explore section svds ability approximate matrices simpler matrices principled manner opens machine learning applications ranging dimension ality reduction topic modeling data compression clustering  matrix approximation considered svd way factorize  σv cid rmn right nown or product matrices thogonal  contains singular values main diagonal instead svd factorization investigate svd allows represent matrix sum simpler lowrank matrices ai lends matrix approximation scheme cheaper compute svd rmm  construct rank matrix ai rmn ai  uivcid formed outer product ith orthogonal column vector   figure  shows image stonehenge  outer products ai represented matrix deﬁned     deisenroth   faisal   ong published cambridge university press  figure  image processing svd  original grayscale image      matrix values  black  white bf rank matrices       corresponding singular values       gridlike structure rank matrix imposed outerproduct left rightsingular vectors rankk approximation matrix decompositions  original image                                matrix rmn rank  written sum rank matrices ai σiuivcid σiai  cid cid outerproduct matrices ai weighted ith singular value σi  holds diagonal structure singular value matrix  multiplies matching left rightsingular vectors uivcid scales corresponding singular value σi terms σijuivcid    diagonal matrix terms   vanish corresponding singular values   vanish  introduced rank matrices ai summed  in dividual rank matrices obtain rankr matrix   sum run matrices ai        intermediate value    obtain rankk approximation cidak  σiuivcid σiai cid cid rk cidak   figure  shows lowrank approximations cidak original image stonehenge shape rocks be comes increasingly visible clearly recognizable rank approx       imation original image requires   numbers rank approximation requires store ﬁve sin gular values ﬁve left rightsingular vectors               numbers dimensional each total    original measure difference error rankk approxima tion cidak need notion norm section  draft  mathematics machine learning feedback smmlbookcom cid  matrix approximation  original image   rank approximation cidac rank approximation cida figure  image reconstruction svd  original image bf image reconstruction lowrank approximation svd rankk approximation given cidak  cidk  σiai  rank approximation cidae rank approximation cidaf rank approximation cida norms vectors measure length vector analogy deﬁne norms matrices deﬁnition  spectral norm matrix  norm matrix rmn deﬁned right  spectral spectral norm cid  max cid ax cid cid cid cid introduce notation subscript matrix norm lefthand side similar euclidean norm vectors righthand side subscript  spectral norm  determines long vector  multiplied  theorem  spectral norm largest singular value  leave proof theorem exercise theorem  eckartyoung theorem eckart young  con rmn matrix rank sider matrix   cid  cidak  cidk rmn rank  let  holds  σiuivcid cidak  argminrkbk cid  σk  cidak cid cid cid cid cid cida cid  eckartyoung theorem states explicitly error intro duce approximating rankk approximation inter pret rankk approximation obtained svd projection fullrank matrix lowerdimensional space rankatmostk matrices possible projections svd minimizes error with respect spectral norm rankk approximation retrace steps understand  hold    deisenroth   faisal   ong published cambridge university press  eckartyoung theorem matrix decompositions observe difference sum remaining rank matrices cidak matrix containing cidak  cid ik σiuivcid theorem  immediately obtain σk spectral norm difference matrix let closer look  assume matrix  rkb cid  cid cid  cid cid cida cid cid cidak cid exists   implies bx   follows cid  cid  bx ax cid cid kdimensional null space  right now version cauchyschwartz inequality  en compasses norms matrices obtain ax cid cid cid cid cid  σk cid cid  cid cid however exists   dimensional subspace cid spanned rightsingular vectors vj  cid    σk  adding dimensions spaces yields number greater  nonzero vector spaces contradiction ranknullity theorem theorem  section  ax cid cid cid cid eckartyoung theorem implies use svd reduce rankr matrix rankk matrix cida principled optimal in spectral norm sense manner interpret approximation rankk matrix form lossy compression therefore lowrank approximation matrix appears machine learning applications eg image processing noise ﬁltering regularization illposed prob lems furthermore plays key role dimensionality reduction principal component analysis   example  finding structure movie ratings consumers continued coming movierating example apply con cept lowrank approximations approximate original data matrix recall ﬁrst singular value captures notion science ﬁction theme movies science ﬁction lovers thus ﬁrst singular value term rank decomposition movierating matrix obtain predicted ratings   uvcid   cid cid draft  mathematics machine learning feedback smmlbookcom  matrix phylogeny             ﬁrst rank approximation  insightful tells ali beatrix like science ﬁction movies star wars bladerunner entries values   fails capture ratings movies chandra surprising chandras type movies captured ﬁrst singular value second singular value gives better rank approximation movietheme lovers   uvcid   cid  cid second rank approximation  capture chandras ratings movie types well science ﬁction movies leads consider rank approximation cida combine ﬁrst rank approximations cida  σa  σa  cida similar original movie ratings table             suggests ignore contribution  in terpret data table evidence movie thememovielovers category means entire space moviethemesmovielovers example twodimensional space spanned science ﬁction french art house movies lovers    deisenroth   faisal   ong published cambridge university press  figure  functional phylogeny matrices encountered machine learning matrix decompositions real matrices pseudoinverse svd right nown right nowm square determinant trace basis eigenvectors defective basis eigenvectors nonsquare det   cid cid singular singular nondefective diagonalizable acida  aacid acida cid aacid normal nonnormal cid cid symmetric eigenvalues positive deﬁnite cholesky eigenvalues   diagonal identity matrix inverse matrix regular invertible columns orthogonal eigenvectors rotation orthogonal word phylogenetic describes capture relationships individuals groups derived greek words tribe source  matrix phylogeny    covered basics linear algebra analytic geometry  looked fundamental characteristics ma trices linear mappings figure  depicts phylogenetic tree relationships different types matrices black arrows indicating is subset of covered operations perform in right nowm nonsquare matrices blue consider real matrices where    svd exists saw  focus right nown determinant informs ing square matrices square matrix possesses inverse matrix ie belongs class regular invertible matrices square   matrix possesses  linearly independent eigenvectors matrix nondefective eigendecomposition exists theorem  know repeated eigen values result defective matrices diagonalized nonsingular nondefective matrices same exam ple rotation matrix invertible determinant nonzero diagonalizable real numbers eigenvalues guaranteed real numbers draft  mathematics machine learning feedback smmlbookcom cid  reading dive branch nondefective square   matrices normal condition acida  aacid holds moreover restrictive condition holds acida  aacid   called or thogonal see deﬁnition  set orthogonal matrices subset regular invertible matrices satisﬁes acid   normal matrices frequently encountered subset symmetric right nown satis   scid symmetric matrices matrices  real eigenvalues subset symmetric matrices consists pos itive deﬁnite matrices  satis condition xcidp     case unique cholesky decomposition exists theo rem  positive deﬁnite matrices positive eigenvalues invertible ie nonzero determinant right subset symmetric matrices consists diagonal matrices  diagonal matrices closed multiplication addition necessarily form group this case diagonal entries nonzero matrix invertible special diagonal matrix identity matrix   reading content  establishes underlying mathematics connects methods studying mappings heart machine learning level underpinning software so lutions building blocks machine learning theory matrix characterization determinants eigenspectra eigenspaces pro vides fundamental features conditions categorizing analyzing matrices extends forms representations data map pings involving data judging numerical stability compu tational operations matrices press et al  determinants fundamental tools order invert matrices compute eigenvalues by hand however smallest instances numerical computation gaussian elimination outperforms determinants press et al  determinants remain powerful theoretical concept eg gain intuition orientation basis based sign determinant eigenvectors perform basis changes transform data coordinates mean ingful orthogonal feature vectors similarly matrix decomposition meth ods cholesky decomposition reappear com pute simulate random events rubinstein kroese  therefore cholesky decomposition enables compute reparametrization trick want perform continuous differentiation random variables eg variational autoencoders jimenez rezende et al  kingma welling  eigendecomposition fundamental enabling extract mean ingful interpretable information characterizes linear mappings    deisenroth   faisal   ong published cambridge university press  principal component analysis fisher discriminant analysis multidimensional scaling isomap laplacian eigenmaps hessian eigenmaps spectral clustering tucker decomposition cp decomposition matrix decompositions therefore eigendecomposition underlies general class machine learning algorithms called spectral methods perform eigendecomposi tion positivedeﬁnite kernel spectral decomposition methods encompass classical approaches statistical data analysis following principal component analysis pca pearson    lowdimensional subspace explains vari ability data sought fisher discriminant analysis aims determine separating hy perplane data classiﬁcation mika et al  multidimensional scaling mds carroll chang  computational efﬁciency methods typically comes ﬁnd ing best rankk approximation symmetric positive semideﬁnite matrix contemporary examples spectral methods different origins requires computation eigenvectors eigenvalues positivedeﬁnite kernel isomap tenenbaum et al  laplacian eigenmaps belkin niyogi  hessian eigenmaps donoho grimes  spectral clustering shi malik  core computations generally underpinned lowrank matrix approximation techniques belabbas wolfe  encountered svd svd allows discover kind information eigendecomposition however svd generally applicable nonsquare matrices data tables matrix factorization meth ods relevant want identi heterogeneity data want perform data compression approximation eg in  values storing nmk values want stead storing  perform data preprocessing eg decorrelate predictor variables design matrix ormoneit et al  svd operates matrices interpret rectangular arrays indices rows columns extension matrixlike structure higherdimensional arrays called tensors turns svd special case general family decompositions operate tensors kolda bader  svdlike operations lowrank approxima tions tensors are example tucker decomposition tucker  cp decomposition carroll chang  svd lowrank approximation frequently machine learn ing computational efﬁciency reasons reduces memory operations nonzero multiplications need perform potentially large matrices data trefethen bau iii  moreover lowrank approximations operate ma trices contain missing values purposes lossy compression dimensionality reduction moonen moor  markovsky  draft  mathematics machine learning feedback smmlbookcom exercises  compute determinant laplace expansion using ﬁrst row sarrus rule  compute following determinant efﬁciently exercises       cid cid   cid cid          compute eigenspaces  compute eigenspaces  diagonalizability matrix unrelated invertibility determine following matrices diagonalizable andor invert ible cid cid cid cid cid cid cid cid  compute eigenspaces following transformation matrices diagonalizable    deisenroth   faisal   ong published cambridge university press  matrix decompositions  following matrices diagonalizable yes determine diagonal form basis respect transformation matrices di agonal no reasons diagonalizable cid  cid              cid cid   cid  cid cid cid   max cidaxcid cidxcid    svd matrix singular value decomposition  rank approximation   rmn matrices acida aacid possess nonzero eigenvalues   cid  theorem  holds ie  largest singular value  rmn draft  mathematics machine learning feedback smmlbookcom vector calculus algorithms machine learning optimize objective function respect set desired model parameters control model explains data finding good parameters phrased opti mization problem see sections   examples include  lin ear regression see   look curveﬁtting problems optimize linear weight parameters maximize likelihood ii neuralnetwork autoencoders dimensionality reduction data com pression parameters weights biases layer minimize reconstruction error repeated application chain rule iii gaussian mixture models see   modeling data distributions optimize location shape parameters mixture component maximize likelihood model figure  illustrates problems typically solve optimization algorithms exploit gradient information section  figure  gives overview concepts chap ter related connected  book central  concept function function  quantity relates quantities other book rd targets function values   quantities typically inputs  assume realvalued stated otherwise rd domain   function values   imagecodomain    regression problem parameters curve explains observations crosses well  density estimation gaussian mixture model means covariances data dots explained well material published cambridge university press mathematics machine learning marc peter deisenroth  aldo faisal cheng soon ong  version free view download personal use only redistribution resale use derivative works by   deisenroth   faisal   ong  smmlbookcom domain imagecodomain figure  vector calculus plays central role  regression curve ﬁtting  density estimation ie modeling data distributions xytrainingdatamlexx  figure  mind map concepts introduced parts book difference quotient vector calculus   regression   optimization partial derivatives   dimensionality reduction   probability jacobian hessian   density estimation     use taylor series   classiﬁcation section  provides detailed discussion context linear functions write   rd cid speci function  speciﬁes  mapping rd   speciﬁes explicit assignment input  function value   function  assigns input  exactly function value     example  recall dot product special case inner product section  previous notation function    xcidx  speciﬁed    cid       discuss compute gradients functions essential facilitate learning machine learning models gradient points direction steepest ascent therefore draft  mathematics machine learning feedback smmlbookcom  differentiation univariate functions figure  average incline function     δx incline secant blue      δx given δyδx deﬁnition  difference quotient difference quotient difference quotient vector calculus fundamental mathematical tools need machine learning book assume functions differentiable additional technical deﬁnitions cover here approaches presented extended subdifferentials functions continuous differentiable certain points look extension case functions constraints    differentiation univariate functions following brieﬂy revisit differentiation univariate function familiar high school mathematics start difference quotient univariate function       subsequently use deﬁne derivatives δy δx    δx δx   computes slope secant line points graph   figure  points xcoordinates    δx difference quotient considered average slope     δx assume  linear function limit δx  obtain tangent    differentiable tangent derivative   deﬁnition  derivative formally    derivative   deﬁned limit df dx secant figure  tangent      lim   derivative  points direction steepest ascent      deisenroth   faisal   ong published cambridge university press  derivative δyδxxyδx  vector calculus example  derivative polynomial want compute derivative    xn  know answer nxn want derive result deﬁnition derivative limit difference quotient deﬁnition derivative  obtain df dx  lim  lim  lim       hn cidn cidn   xn cidxnihi xn xn  cidn obtain cidxnh starting sum  xnterm cancels cidxnihi cidn cid xnihi df dx  lim  lim  lim cidn cid cid cidcid cid xn  xnihi cid cid cid cid cid cidcid   cid xn  nxn  taylor polynomial deﬁne        taylor series taylor series representation function  inﬁnite sum terms terms determined derivatives  evaluated  deﬁnition  taylor polynomial taylor polynomial degree       deﬁned tnx  cid  kx xk   kx kth derivative   which assume exists  kx coefﬁcients polynomial taylor series deﬁnition  taylor series smooth function  taylor series   deﬁned       draft  mathematics machine learning feedback smmlbookcom  differentiation univariate functions tx  cid  kx xk     obtain maclaurin series special instance taylor series    tx  called analytic remark general taylor polynomial degree  approximation function need polynomial taylor poly nomial similar  neighborhood  however taylor polynomial degree  exact representation polynomial  degree  cid  derivatives     vanish    means  continuously differentiable inﬁnitely times maclaurin series analytic example  taylor polynomial consider polynomial seek taylor polynomial  evaluated    start com puting coefﬁcients                    cid    cidcid                   therefore desired taylor polynomial tx  cid  kx xk     multiplying rearranging yields               tx                              ie obtain exact representation original function    deisenroth   faisal   ong published cambridge university press  figure  taylor polynomials original function    sinx  cosx black solid approximated taylor polynomials dashed    higherorder taylor polynomials approximate function  better globally similar    vector calculus example  taylor series consider function figure  given    sinx  cosx     seek taylor series expansion     maclaurin series expansion   obtain following derivatives    sin  cos    cid  cos sin    cidcid  cos        sin  cos      cos  sin  sin pattern here coefﬁcients taylor series  since sin   occurs twice switching one furthermore      therefore taylor series expansion     given tx  cid  kx xk     xk        cid  cosx  sinx                      cid    xk draft  mathematics machine learning feedback smmlbookcom xyftttt  differentiation univariate functions power series representations power series representation cosx  sinx  cid cid   xk     xk  figure  shows corresponding ﬁrst taylor polynomials tn       remark taylor series special case power series    cid akx ck ak coefﬁcients  constant special form deﬁnition   differentiation rules following brieﬂy state basic differentiation rules denote derivative   cid product rule  xgxcid   cidxgx   xgcidx cid    xgcidx  cidxgx gx cidcid quotient rule sum rule chain rule gx    gxcid   cidx  gcidx cidgf xcidcid  denotes function composition       cidx  gcidf xf cidx cid cid gf  here  example  chain rule let compute derivative function hx     chain rule hx      gf          gf      obtain derivatives    cidx    gcidf         deisenroth   faisal   ong published cambridge university press  vector calculus derivative  given hcidx  gcidf  cidx                chain rule  substituted deﬁnition   gcidf   partial differentiation gradients differentiation discussed section  applies functions   following consider general case scalar variable  right now eg function  depends variables        generalization derivative functions sev eral variables gradient ﬁnd gradient function  respect  varying variable time keeping constant gradient collection partial derivatives deﬁnition  partial derivative function   right    cid right  variables      xn deﬁne partial derivatives            xn    lim xn collect row vector  lim       xn xn     cid   df dx xf  gradf     number variables  dimension image rangecodomain   here deﬁned column vector        xncid right now row vector  called gradient  jacobian rn     xn    cid generalization derivative section  remark deﬁnition jacobian special case general deﬁnition jacobian vectorvalued functions collection partial derivatives section  example  partial derivatives chain rule        obtain partial derivatives                draft  mathematics machine learning feedback smmlbookcom partial derivative gradient jacobian use results scalar differentiation partial derivative derivative respect scalar  partial differentiation gradients              yy  chain rule  compute partial derivatives remark gradient row vector uncommon literature deﬁne gradient vector column vector following conven tion vectors generally column vectors reason deﬁne gradient vector row vector twofold first consistently generalize gradient vectorvalued functions   right rm then gradient matrix second immediately apply multivariate chain rule paying attention dimension gradient discuss points section  example  gradient      tives  respect     xx    partial derivatives ie deriva        xx       xx gradient cid    df dx cid     cidxx       xx cid    basic rules partial differentiation right now basic differentiation rules multivariate case  know school eg sum rule product rule chain rule section  apply however compute derivatives re right need pay attention gradients spect vectors  involve vectors matrices matrix multiplication commuta tive section  ie order matters general product rule sum rule chain rule product rule sum rule cidf xgxcid  cidf   gxcid  gx       deisenroth   faisal   ong published cambridge university press  product rule  gcid   cidg   gcid sum rule   gcid   cid  gcid chain rule gf cid  gcidf  cid intuition mathematically correct partial derivative fraction vector calculus chain rule    cidgf xcid  let closer look chain rule chain rule  resem bles degree rules matrix multiplication said neighboring dimensions match matrix multiplication de ﬁned section  left right chain rule exhibits similar properties  shows denominator ﬁrst factor numerator second factor multiply factors to gether multiplication deﬁned ie dimensions  match  cancels gx remains  chain rule  variables   furthermore consider function    xt xt functions  compute gradient  respect  need apply chain rule  multivariate functions df dt cid  cid cid cid xt xt  denotes gradient  partial derivatives      sin    cos  example  consider      df dt  sin    sin  cos    sin     cos  corresponding derivative  respect   sin    sin tcos     function   xs  xs  functions variables   chain rule yields partial derivatives draft  mathematics machine learning feedback smmlbookcom chain rule written matrix multiplication gradient checking  gradients vectorvalued functions gradient obtained matrix multiplication df ds    cid  cid cidcid cid cid cid cid cidcid   compact way writing chain rule matrix multiplication makes sense gradient deﬁned row vector otherwise need start transposing gradients matrix dimensions match straightforward long gradient vector matrix however gradient tensor we discuss following transpose longer triviality remark veriing correctness gradient implementation deﬁnition partial derivatives limit corresponding dif ference quotient see  exploited numerically checking correctness gradients programs compute gradients implement them use ﬁnite differences numer ically test computation implementation choose value  small eg    compare ﬁnitedifference approxima tion  analytic implementation gradient error small gradient implementation probably correct small idhidfi idhidfi   dhi ﬁnitedifference mean approximation dfi analytic gradient  respect ith variable xi cid cid cid  gradients vectorvalued functions far discussed partial derivatives gradients functions   right  mapping real numbers following generalize concept gradient vectorvalued functions vector ﬁelds   right rm  cid     function   right rm vector        xncid right now corresponding vector function values given    fmx rm    writing vectorvalued function way allows view vector rm vector functions      fmcid valued function   right fi  right  map  differentiation rules fi exactly ones discussed section     deisenroth   faisal   ong published cambridge university press  vector calculus rm respect xi xi   xi fm xi therefore partial derivative vectorvalued function   right        given vector limh xixihxixn limh fmxxixihxixnfmx rm     know gradient  respect vector row vector partial derivatives  partial derivative  xi column vector therefore obtain gradient   right collecting partial derivatives right rm respect  df  dx cid   fmx cid   xn    xn fmx xn       rmn  deﬁnition  jacobian collection ﬁrstorder partial deriva rm called jacobian tives vectorvalued function   right  matrix deﬁne arrange follows jacobian     xf  df  dx cid   xn    fmx xn           ji   cid   xn fmx xn fi xj special case  function   right vector  row vector matrix dimension  right scalar eg    cidn  maps  xi possesses jacobian   remark book use numerator layout derivative ie derivative df dx   matrix elements  deﬁne rows elements  deﬁne columns corresponding jacobian  rm respect  right  draft  mathematics machine learning feedback smmlbookcom jacobian gradient function   right  rm matrix size    numerator layout figure  determinant jacobian  compute magniﬁer blue orange area denominator layout  gradients vectorvalued functions   exists denominator layout transpose numerator layout book use numerator layout jacobian changeofvariable method probability distributions section  scaling transformation variable provided determinant section  saw determinant compute area parallelogram given vectors    cid    cid sides unit square blue figure  area square cid cid cid cid det cidcid    cidcidcid cid cid cid     cid    cid parallelogram sides    orange figure  area given absolute value deter minant see section  cid cid cid cid det cidcid   cidcidcid cid cid cid      ie area exactly times area unit square ﬁnd scaling factor ﬁnding mapping transforms unit square square linear algebra terms effectively perform variable transformation     case mapping linear absolute value determinant mapping gives exactly scaling factor looking for approaches identi mapping first ex ploit mapping linear use tools   identi mapping second ﬁnd mapping partial derivatives tools discussing    approach  started linear algebra approach bases  see section  identi recap effectively perform change basis     looking transformation matrix implements basis change results section  identi desired basis change matrix     cid cid           absolute value determi    deisenroth   faisal   ong published cambridge university press  vector calculus nant   yields scaling factor looking for given   ie area square spanned   times detj  greater area spanned   approach  linear algebra approach works linear trans formations nonlinear transformations which relevant sec tion  follow general approach partial derivatives approach consider function     performs variable transformation example  maps coordinate represen  respect   coordinate tation vector   respect   want identi representation  mapping compute area or volume changes transformed   this need ﬁnd   changes modi  bit question exactly answered jacobian matrix df  write dx            obtain functional relationship   allows partial derivatives           compose jacobian     cid cid   jacobian represents coordinate transformation looking for exact coordinate transformation linear as case  recovers exactly basis change matrix  co ordinate transformation nonlinear jacobian approximates non linear transformation locally linear one absolute value detj  jacobian determinant factor areas volumes scaled coordinates transformed case yields   jacobian determinant variable transformations relevant section  transform random variables prob ability distributions transformations extremely relevant ma chine learning context training deep neural networks reparametrization trick called inﬁnite perturbation analysis detj   encountered derivatives functions figure  sum  gradient marizes dimensions derivatives    simply scalar topleft entry   rd row vector topright entry    column vector   rd  gradient  re gradient  gradient   matrix draft  mathematics machine learning feedback smmlbookcom geometrically jacobian determinant gives magniﬁcation scaling factor transform area volume jacobian determinant figure  dimensionality partial derivatives x  gradients vectorvalued functions example  gradient vectorvalued function given    ax    rm  rm    right  compute gradient df dx ﬁrst determine dimension df dx   right rm   second compute gradient determine partial derivatives  respect xj rm  follows df dx fix  aijxj   aij cid fi xj collect partial derivatives jacobian obtain gradient df dx fm xn fm xn   an               rm    example  chain rule consider function     ht   gt            expxx cidx compute gradient  respect        cid cidt cos   sin   gt    cid  note         desired gradient computed applying chain rule dh dt cid  cid  cidexpxx   expxx xx cidx  expxx cos     cos     sin   cidcos   sin  cid sin    cos  cid  sin   xxsin    cos tcid     deisenroth   faisal   ong published cambridge university press  discuss model   context linear regression need derivatives leastsquares loss  respect parameters  leastsquares loss vector calculus example  gradient leastsquares loss linear model let consider linear model   φθ  rd parameter vector  right corresponding observations deﬁne functions right  input features seek  leastsquares loss function   use chain rule purpose  called start calculation determine dimensionality gradient   le  cid cid φθ  eθ     rd  chain rule allows compute gradient dth element given dldtheta  npeinsum nnd dldededtheta       cid know cid   ecide see section  determine cid  ecid rn  furthermore obtain desired derivative right   ecidφ  ycid cid θcidφcid cid cidcid cidcidcidcid   rd  remark obtained result chain rule immediately looking function lθ  cid φθ    cid φθcidy φθ  approach practical simple functions like  impractical deep function compositions draft  mathematics machine learning feedback smmlbookcom figure  visualization gradient computation matrix respect vector interested computing gradient   respect vector    know gradient da dx   follow equivalent approaches arrive there  collating partial derivatives jacobian tensor  ﬂattening matrix vector computing jacobian matrix reshaping jacobian tensor  gradients matrices partial derivatives     collate da dx     approach  compute partial derivative     matrix col late      tensor       dx  da dx  reshape gradient reshape  approach  reshape ﬂatten   vec tor    then compute gradient   dx   obtain gradient tensor reshaping gradient illustrated above  gradients matrices encounter situations need gradients matrices respect vectors or matrices results multidimen sional tensor think tensor multidimensional array think tensor multidimensional array    deisenroth   faisal   ong published cambridge university press  matrices transformed vectors stacking columns matrix ﬂattening vector calculus collects partial derivatives example compute gradient  matrix  resulting jacobian  ie fourdimensional tensor   entries given jijkl  aijbkl  matrix respect  matrices represent linear mappings exploit fact vectorspace isomorphism linear invertible mapping space rmn   matrices space rmn mn vectors therefore reshape matrices vectors lengths mn pq respectively gradient mn vectors results jacobian size mn pq figure  visualizes approaches practical ap plications desirable reshape matrix vector continue working jacobian matrix chain rule  boils simple matrix multiplication case jacobian tensor need pay attention dimensions need sum out example  gradient vectors respect matrices let consider following example   ax   rm  rm    right seek gradient df da let start determining dimension gradient df da  rm     deﬁnition gradient collection partial derivatives df da   fm fi   rm    compute partial derivatives helpful explicitly write matrix vector multiplication fi  aijxj         cid partial derivatives given fi aiq  xq  allows compute partial derivatives fi respect row  given fi ai  xcid rn  draft  mathematics machine learning feedback smmlbookcom  gradients matrices fi akcidi  cid rn pay attention correct dimensionality fi maps  row size    sized tensor partial derivative fi respect row    obtain  stack partial derivatives  desired gradient fi rm    cid cid xcid cid cid example  gradient matrices respect matrices consider matrix  rm    rm  right     rcidare   right   seek gradient dkdr solve hard problem let ﬁrst write know gradient dimensions tensor moreover dk dr  rn      dkpq dr  rm            kpq  qth entry     de noting ith column  ri entry  given dot product columns  ie kpq  rcid  rq  rmprmq  cid compute partial derivative kpq rij obtain kpq rij cid rij rmprmq  pqij     deisenroth   faisal   ong published cambridge university press  vector calculus pqij  riq rip riq                    know desired gradient dimension                         single entry tensor given pqij  useful identities computing gradients following list useful gradients frequently required machine learning context petersen pedersen  here use tr  determinant see section    inverse   assuming exists  trace see deﬁnition  det  xcid  trf   tr cidcid cid   cid   cid detf   detf xtr           cidabcidx cid cid     cid acidx  xcida acidx acidxb xcidbx  acid  acid  abcid  xcidb  bcid ascidw  as  ascidw symmetric  remark book cover traces transposes matrices however seen derivatives higherdimensional ten sors case usual trace transpose deﬁned cases trace   dimensional matrix special case tensor contraction similarly  tensor  draft  mathematics machine learning feedback smmlbookcom cid cid  backpropagation automatic differentiation transpose tensor mean swapping ﬁrst dimensions specif ically   require tensorrelated computations work multivariate functions    compute derivatives respect matrices and choose vectorize discussed section   backpropagation automatic differentiation machine learning applications ﬁnd good model parameters performing gradient descent section  relies fact compute gradient learning objective respect parameters model given objective function obtain gradient respect model parameters calculus applying chain rule section  taste section  looked gradient squared loss respect parameters linear regression model consider function cid      expx  cos cidx  expxcid  application chain rule noting differentiation linear compute gradient    expx cidx  expx  sin cidx  expxcid cidx   expxcid df dx cid   cidx  expx  sin cidx  expxcid cid  expxcid  cid writing gradient explicit way impractical results lengthy expression derivative practice means that careful implementation gradient signiﬁcantly expensive computing function imposes unnecessary overhead training deep neural network mod els backpropagation algorithm kelley  bryson  dreyfus  rumelhart et al  efﬁcient way compute gradient error function respect parameters model  gradients deep network area chain rule extreme deep learning function value  computed manylevel function composition   fk fk   fkfk               inputs eg images  observations eg class labels function fi        possesses parameters    deisenroth   faisal   ong published cambridge university press  good discussion backpropagation chain rule available blog tim vieira stinyurl comycfmyrw backpropagation figure  forward pass multilayer neural network compute loss  function inputs  parameters ai bi discuss case activation functions identical layer unclutter notation indepth discussion gradients neural networks justin domkes lecture notes stinyurl comyalcxgtv vector calculus           ak bk ak bk neural networks multiple layers functions fixi  σaixi  bi ith layer xi output layer  activation function logistic sigmoid ex  tanh rectiﬁed linear unit relu order train models require gradient loss function  respect model parameters aj bj         requires compute gradient  respect inputs layer example inputs  observations  network structure deﬁned       σiaif   bi          figure  visualization interested ﬁnding aj bj          squared loss lθ  cid  kθ  cid minimized         ak bk obtain gradients respect parameter set  require partial derivatives  respect parameters θj  layer         partial derivatives  chain rule allows determine aj bj θk θk θk       θk       θk             θk θi                θi orange terms partial derivatives output layer respect inputs blue terms partial derivatives output layer respect parameters assuming computed partial derivatives lθi com putation reused compute lθi additional terms draft  mathematics machine learning feedback smmlbookcom  backpropagation automatic differentiation           ak bk ak bk need compute indicated boxes figure  visualizes gradients passed backward network  automatic differentiation turns backpropagation special case general technique numerical analysis called automatic differentiation think au tomatic differentation set techniques numerically in contrast symbolically evaluate exact up machine precision gradient function working intermediate variables applying chain rule automatic differentiation applies series elementary arithmetic operations eg addition multiplication elementary functions eg sin cos exp log applying chain rule operations gradient complicated functions computed automatically automatic differentiation applies general programs forward reverse modes baydin et al  great overview automatic differentiation machine learning figure  shows simple graph representing data ﬂow in puts  outputs  intermediate variables   compute derivative dydx apply chain rule obtain dy db intuitively forward reverse mode differ order multipli cation associativity matrix multiplication choose da dx dy dx db da dy dx dy dx cid dy db db da cid db da dy db cid da dx cid da dx figure  backward pass multilayer neural network compute gradients loss function figure  simple graph illustrating ﬂow data   intermediate variables   automatic differentiation automatic differentiation different symbolic differentiation numerical approximations gradient eg ﬁnite differences general case work jacobians vectors matrices tensors equation  reverse mode gradients prop agated backward graph ie reverse data ﬂow equa tion  forward mode gradients ﬂow data left right graph reverse mode forward mode    deisenroth   faisal   ong published cambridge university press  vector calculus following focus reverse mode automatic differentia tion backpropagation context neural networks input dimensionality higher dimensionality labels reverse mode computationally signiﬁcantly cheaper forward mode let start instructive example example  consider function cid      expx  cos cidx  expxcid  implement function  computer able save computation intermediate variables      expa             cosc        intermediate variables figure  computation graph inputs  function values   intermediate variables      exp cos kind thinking process occurs applying chain rule note preceding set equations requires fewer operations direct implementation function   deﬁned  corresponding computation graph figure  shows ﬂow data computations required obtain function value   set equations include intermediate variables thought computation graph representation widely imple mentations neural network software libraries directly compute derivatives intermediate variables respect corre sponding inputs recalling deﬁnition derivative elementary functions obtain following    expa draft  mathematics machine learning feedback smmlbookcom  backpropagation automatic differentiation    sinc    looking computation graph figure  compute   working backward output obtain note implicitly applied chain rule obtain   substi tuting results derivatives elementary functions   sinc expa            thinking derivatives variable observe computation required calculating derivative similar complexity computation function itself counter intuitive mathematical expression derivative    signiﬁcantly complicated mathematical expression function    automatic differentiation formalization example  let      xd input variables function xd     xd intermediate variables xd output variable computation graph expressed follows           xi  gixpaxi     deisenroth   faisal   ong published cambridge university press  autodifferentiation reverse mode requires parse tree vector calculus  elementary functions xpaxi parent nodes gi variable xi graph given function deﬁned way use chain rule compute derivative function step bystep fashion recall deﬁnition   xd xd    variables xi apply chain rule xi cid xj xj xi cid xj gj xi xj xipaxj  xj xipaxj  paxj set parent nodes xj computation graph equation  forward propagation function  backpropagation gradient computation graph neural network training backpropagate error prediction respect label automatic differentiation approach works function expressed computation graph ele mentary functions differentiable fact function mathematical function program however com puter programs automatically differentiated eg ﬁnd differential elementary functions programming structures loops statements require care well  higherorder derivatives far discussed gradients ie ﬁrstorder derivatives some times interested derivatives higher order eg want use newtons method optimization requires secondorder derivatives nocedal wright  section  discussed taylor series approximate functions polynomials mul tivariate case exactly same following exactly this let start notation consider function     variables   use following notation higherorder partial derivatives and gradients  second partial derivative  respect  nf xn nth partial derivative  respect  yx   entiating respect  respect  xy partial derivative obtained ﬁrst partial differentiating   cid partial derivative obtained ﬁrst partial differ cid  hessian hessian collection secondorder partial derivatives draft  mathematics machine learning feedback smmlbookcom  linearization multivariate taylor series figure  linear approximation function original function  linearized    ﬁrstorder taylor series expansion    twice continuously differentiable function ie order differentiation matter corresponding hessian matrix hessian matrix xy yx   xy xy symmetric hessian denoted   right  hessian  curvature function locally   remark hessian vector field   right hessian  ntensor right xyf   generally   matrix hessian measures rm vector ﬁeld  linearization multivariate taylor series  function  locally linear approxi gradient mation         xf xx   xf  gradient  respect  evaluated  figure  illustrates linear approximation function  input  original function approximated straight line approx imation locally accurate farther away  worse approximation gets equation  special case mul tivariate taylor series expansion   consider ﬁrst terms discuss general case following allow better approximations    deisenroth   faisal   ong published cambridge university press  xxx  vector calculus  given vector    obtain outer product       δδcid   matrix figure  visualizing outer products outer products vectors increase dimensionality array  term  outer product vectors results matrix  outer product vectors yields thirdorder tensor  outer product          results thirdorder tensor three dimensional matrix ie array indexes deﬁnition  multivariate taylor series consider function multivariate taylor series smooth  deﬁne difference vector    multivariate taylor series   deﬁned   rd cid     rd     cid dk xf  δk  dk uated  xf  kth total derivative  respect  eval taylor polynomial deﬁnition  taylor polynomial taylor polynomial degree    contains ﬁrst    components series  deﬁned tnx  cid dk xf  δk    slightly sloppy notation δk rd       note xf δk kth order tensors ie kdimensional arrays deﬁned vectors  dk  times cidcid cid ddd obtained kfold outer product cid kthorder tensor δk denoted  vector       δδcid  rd example δi   δiδj draft  mathematics machine learning feedback smmlbookcom vector implemented onedimensional array matrix twodimensional array npeinsum iidfd npeinsum ijij dfdd npeinsum ijkijk dfddd  linearization multivariate taylor series      δi    δiδjδk  figure  visualizes outer products general obtain terms dk xf xδk  dk xf xi     ikδi δik    cid     cid ik taylor series dk xf xδk contains kth order polynomials deﬁned taylor series vector ﬁelds let explicitly xf xδk taylor series expansion write ﬁrst terms dk            xf xδ                   xf xδ  xf  cid cid cidcid xf xδ  trcid hx cid cidcid cid dd cid   cidcidcidcid cidcidcidcid δcid cidcidcidcid xf xiδi cid  δcidhxδ cid cid hi jδiδj      xf xδ  xf xi  kδiδjδk cid cid cid here hx hessian  evaluated  example  taylor series expansion function vari ables consider function       xy    want compute taylor series expansion       start let discuss expect function  polynomial degree  looking taylor series expansion linear combination polynomials therefore expect taylor series expansion contain terms fourth higher order express thirdorder polynomial means sufﬁcient determine ﬁrst terms  exact alterna tive representation  determine taylor series expansion start constant term ﬁrstorder derivatives given         deisenroth   faisal   ong published cambridge university press  vector calculus                    therefore obtain xyf    xyf    cid     cid     cid cid xyf     cid cid cidx cid note  mials        xyf   contains linear terms ie ﬁrstorder polyno secondorder partial derivatives given         yx xy                 yx xy            cid  yx cid xy cid cid      cid cid     collect secondorder partial derivatives obtain hes sian therefore term taylorseries expansion given xyf     δcidh    cidx   cid   xyf   contains quadratic terms ie secondorder poly       cid cidx   cid cid here  nomials draft  mathematics machine learning feedback smmlbookcom  linearization multivariate taylor series thirdorder derivatives obtained cid xyf  cid    xyf     xyf     cid  xyx cid  yx yx cid cid xy xy yxy secondorder partial derivatives hessian  constant nonzero thirdorder partial derivative           higherorder derivatives mixed derivatives degree  eg   xy  vanish xyf       xyf     cid cid    cid cid    xyf        collects cubic terms taylor series overall exact taylor series expansion               xyf    xyf     xyf           cid       xy             cid                         case obtained exact taylor series expansion polyno mial  ie polynomial  identical original polynomial  particular example result sur prising original function thirdorder polynomial expressed linear combination constant terms ﬁrstorder secondorder thirdorder polynomials     deisenroth   faisal   ong published cambridge university press  extended kalman ﬁlter unscented transform laplace approximation vector calculus  reading details matrix differentials short review required linear algebra magnus neudecker  automatic differentiation long history refer griewank walther  griewank walther  elliott  references therein machine learning and disciplines need compute expectations ie need solve integrals form cid exf    xpxdx  px convenient form eg gaussian integral gen erally solved analytically taylor series expansion  cidµ σcid way ﬁnding approximate solution assuming px  gaussian ﬁrstorder taylor series expansion  locally linearizes nonlinear function   linear functions compute mean and covariance exactly px gaussian distributed see section  property heavily exploited extended kalman ﬁlter maybeck  online state estimation nonlinear dynami cal systems also called statespace models deterministic ways approximate integral  unscented transform julier uhlmann  require gradients laplace approximation mackay  bishop  murphy  uses secondorder taylor series expansion requiring hessian local gaussian approximation px mode  compute derivative  cidx  compute derivative  cidx logistic sigmoid exercises    logx sinx       expx  compute derivative  cidx function    exp           constants     consider following functions  compute taylor polynomials tn            sinx  cosx   sinx cosx        xcidy     right   xxcid    right draft  mathematics machine learning feedback smmlbookcom exercises  dimensions fi  compute jacobians    differentiate  respect   respect     sinlogtcidt    rd gx  traxb   rde   ref    rf   tr denotes trace  compute derivatives df dx following functions chain rule provide dimensions single partial derivative steps detail    log      xcidx    rd    sinz    ax     red   rd   sin applied element   compute derivatives df dx following functions  use chain rule provide dimensions single partial deriva steps detail tive    exp      gy  ycidsy   hx        rd   rdd    trxxcid  σi    rd tra trace  ie sum diagonal elements aii hint explicitly write outer product  use chain rule provide dimensions single partial deriva tive need compute product partial derivatives explicitly   tanhz  rm   ax     right   rm     rm  here tanh applied component   deﬁne gz   log px   log qz    tcid  differentiable functions      rd   re   rf  cid  rg chain rule compute gradient dν gz      deisenroth   faisal   ong published cambridge university press  random variable probability distribution probability distributions probability loosely speaking concerns study uncertainty probabil ity thought fraction times event occurs degree belief event like use probability mea sure chance occurring experiment mentioned   quanti uncertainty data uncertainty machine learning model uncertainty predictions produced model quantiing uncertainty requires idea random variable function maps outcomes random experiments set properties interested in associated random variable function measures probability particular outcome or set outcomes occur called probability distribution probability distributions building block con cepts probabilistic modeling section  graphical models sec tion  model selection section  section present concepts deﬁne probability space the sample space events probability event related fourth concept called random variable presentation deliber ately slightly hand wavy rigorous presentation occlude intuition concepts outline concepts presented  shown figure   construction probability space theory probability aims deﬁning mathematical structure random outcomes experiments example tossing single coin determine outcome large num ber coin tosses observe regularity average outcome mathematical structure probability goal perform automated reasoning sense probability generalizes logical reasoning jaynes   philosophical issues constructing automated reasoning systems classical boolean logic allow express certain forms plausible reasoning consider material published cambridge university press mathematics machine learning marc peter deisenroth  aldo faisal cheng soon ong  version free view download personal use only redistribution resale use derivative works by   deisenroth   faisal   ong  smmlbookcom  construction probability space mean variance bayes theorem summary statistics product rule sum rule   regression property transformations random variable  distribution example gaussian exa ple   dimensionality reduction independence similarity sufﬁcient statistics inner product exponential family bernoulli beta   density estimation following scenario observe false ﬁnd  plausible conclusion drawn classical logic observe  true plausible use form reasoning daily waiting friend consider possibilities  time  delayed trafﬁc  abducted aliens observe friend late logically rule  tend consider  likely logically required so finally consider  possible continue consider unlikely conclude  plausible answer seen way probability theory considered generalization boolean logic context machine learning applied way formalize design automated reasoning systems arguments probability theory foundation reasoning systems pearl  philosophical basis probability related think true in logical sense studied cox jaynes  way think precise common sense end constructing probabilities   jaynes  identiﬁed mathematical criteria apply plausibilities  degrees plausibility represented real numbers  numbers based rules common sense    deisenroth   faisal   ong published cambridge university press  figure  mind map concepts related random variables probability distributions described for plausible reasoning necessary extend discrete true false values truth continuous plausibilities jaynes  probability distributions  resulting reasoning consistent following meanings word consistent  consistency noncontradiction result reached different means plausibility value cases  honesty available data taken account  reproducibility state knowledge problems same assign degree plausibility them coxjaynes theorem proves plausibilities sufﬁcient deﬁne universal mathematical rules apply plausibility  transformation arbitrary monotonic function crucially rules rules probability remark machine learning statistics major interpre tations probability bayesian frequentist interpretations bishop  efron hastie  bayesian interpretation uses probabil ity speci degree uncertainty user event referred subjective probability degree belief frequentist interpretation considers relative frequencies events total number events occurred probability event deﬁned relative frequency event limit inﬁnite data machine learning texts probabilistic models use lazy notation jargon confusing text exception multiple distinct concepts referred probability distribution reader disentangle meaning context trick help sense probability distributions check trying model categorical  discrete random variable some thing continuous  continuous random variable kinds questions tackle machine learning closely related con sidering categorical continuous models  probability random variables distinct ideas confused discussing probabilities idea probability space allows quanti idea probability however work directly basic probability space instead work random variables the second idea transfers probability convenient often numerical space idea idea distribution law associated random variable introduce ﬁrst ideas section expand idea section  modern probability based set axioms proposed kolmogorov draft  mathematics machine learning feedback smmlbookcom  construction probability space grinstead snell  jaynes  introduce con cepts sample space event space probability measure prob ability space models realworld process referred experiment random outcomes sample space  sample space sample space set possible outcomes experiment usually denoted  example successive coin tosses   denotes heads  sample space denotes tails event space hh tt ht th event space space potential results experiment event space subset sample space  event space end experiment observe particular outcome   event space obtained considering collection subsets  discrete probability distributions section  power set  probability  event  associate number   measures probability degree belief event occur   called probability  probability    book refer probability single event lie interval   total probability outcomes sample space   ie     given probability space     want use model realworld phenomenon machine learning avoid explic itly referring probability space instead refer probabilities quantities interest denote target space refer elements states introduce function    takes element  an outcome returns particular quantity  value  associationmapping called random variable example case tossing coins counting number heads random variable  maps possible outcomes xhh   xht   xth   xtt   particular case  probabilities interested in ﬁnite sample space  elements  function corresponding random variable essentially ﬁnite lookup table subset    the probability particular event occurring corresponding random variable  example  provides concrete illustration terminol ogy  associate pxs      remark aforementioned sample space  unfortunately referred different names different books common  state space jacod protter  state space reserved referring states dynamical hasselblatt    deisenroth   faisal   ong published cambridge university press  target space random variable random variable great source misunderstanding random variable function toy example essentially biased coin ﬂip example probability distributions katok  names  are sample description space possibility space event space example  assume reader familiar computing probabilities intersections unions sets events gentler introduction probability examples   walpole et al  consider statistical experiment model funfair game con sisting drawing coins bag with replacement coins usa denoted  uk denoted  bag draw coins bag outcomes total state space sample space  experiment         let assume composition bag coins draw returns random  probability  event interested total number times repeated draw returns  let deﬁne random variable  maps sample space   denotes number times draw  bag preceding sample space zero    random variable   function    lookup table represented table like following                  return ﬁrst coin draw drawing second implies draws independent other discuss section  note experimental outcomes map event draws returns  therefore probability mass function section   given                                                                draft  mathematics machine learning feedback smmlbookcom  construction probability space   subset  calculation equated different concepts probability output  probability samples  example          consider random variable    outcome head obtained tossing coins let   preimage   ie set elements  map    way understand transformation probability events  random variable  associate probability preimage  jacod protter    notation for example single element   xω     pxs              xω   lefthand  probability set possible outcomes eg number    interested in random variable  maps states outcomes righthand  probability set states in  property eg   random variable  distributed according particular probability distribution px deﬁnes probability mapping event probability outcome   random variable words function px equivalently  law distribution random variable  random variable  remark target space is range random variable indicate kind probability space ie ﬁnite countably inﬁnite called discrete random variable section  continuous random variables section  consider    rd  statistics probability theory statistics presented together con cern different aspects uncertainty way contrasting kinds problems considered probability consider model process underlying uncertainty captured random variables use rules probability derive happens statistics observe happened try ﬁgure underlying process explains observations sense machine learning close statistics goals construct model adequately represents process generated data use rules probability obtain bestﬁtting model data aspect machine learning systems interested generalization error see   means actually interested performance instances observe future identical instances    deisenroth   faisal   ong published cambridge university press  law distribution probability distributions seen far analysis future performance relies probability statistics presented  interested reader encouraged look books boucheron et al  shalevshwartz bendavid  statistics    discrete continuous probabilities let focus attention ways probability event introduced section  depending target space dis crete continuous natural way refer distributions different discrete speci probability target space random variable  takes particular value   denoted     expression     discrete random variable  known probability mass function target space continuous eg real line  natural speci probability random variable  interval denoted   cid  cid    con vention speci probability random variable  particular value  denoted   cid  expression   cid  continuous random variable  known cumulative distribution function discuss continuous random variables section  revisit nomenclature contrast discrete continuous random variables section    remark use phrase univariate distribution refer distribu tions single random variable whose states denoted nonbold  refer distributions random variable multivariate distributions usually consider vector random variables whose states denoted bold   discrete probabilities target space discrete imagine probability distri bution multiple random variables ﬁlling multidimensional array numbers figure  shows example target space joint probability cartesian product target spaces random variables deﬁne joint probability entry values jointly    xi   yj  nij nij number events state xi yj  total number events joint probability probability intersec   yj tion events is    xi   yj     xi figure  illustrates probability mass function pmf discrete prob ability distribution random variables    probability draft  mathematics machine learning feedback smmlbookcom probability mass function cumulative distribution function univariate multivariate joint probability probability mass function  discrete continuous probabilities ci cid cidcidcid nij cid rj      figure  visualization discrete bivariate probability mass function random variables    diagram adapted bishop        lazily written px  called joint probability think probability function takes state   returns real number reason write px  marginal probability  takes value  irrespective value marginal probability random variable  lazily written px write  px denote random variable  distributed according px consider instances    fraction instances the conditional probability    written lazily py conditional probability example  consider random variables     ﬁve possible states  possible states shown figure  denote nij number events state   xi   yj denote  total number events value ci sum individual frequencies ith column is ci  cid  nij similarly value rj row sum is rj  cid  nij deﬁnitions compactly express distribution    probability distribution random variable marginal probability seen sum row column    xi  ci cid  nij    yj  rj ci rj ith column jth row probability table respectively convention discrete random variables ﬁnite number events assume probabilties sum one is  nij cid cid cid    xi      yj    conditional probability fraction row column par    deisenroth   faisal   ong published cambridge university press  probability distributions ticular cell example conditional probability  given  nij ci conditional probability  given    xi     yj    xi   yj  nij rj categorical variable machine learning use discrete probability distributions model categorical variables ie variables ﬁnite set unordered val ues categorical features degree taken uni versity predicting salary person categorical la bels letters alphabet handwriting recognition discrete distributions construct probabilistic models combine ﬁnite number continuous distributions    continuous probabilities consider realvalued random variables section ie consider target spaces intervals real line  book pretend perform operations real random variables dis crete probability spaces ﬁnite states however simpliﬁcation precise situations repeat inﬁnitely often want draw point interval ﬁrst situation arises discuss generalization errors machine learning chap ter  second situation arises want discuss continuous distributions gaussian section  purposes lack precision allows briefer introduction probability section  behaved enough remark continuous spaces additional technicalities counterintuitive first set subsets used deﬁne event space needs restricted behave set complements set intersections set unions second size set which discrete spaces obtained counting elements turns tricky size set called measure example cardinality discrete sets length interval  volume region rd mea sures sets behave set operations additionally topology called borel σalgebra betancourt details careful con struction probability spaces set theory bogged technicalities stinyurlcomybtmfd pre cise construction refer billingsley  jacod protter book consider realvalued random variables cor draft  mathematics machine learning feedback smmlbookcom measure borel σalgebra  discrete continuous probabilities responding borel σalgebra consider random variables values rd vector realvalued random variables deﬁnition  probability density function function   rd called probability density function pdf  rd    cid   integral exists cid rd  xdx    probability density function pdf probability mass functions pmf discrete random variables integral  replaced sum  observe probability density function function  nonnegative integrates one associate random variable  function    cid  cid    xdx  cid     outcomes continuous random vari rd deﬁned analogously considering vector  association  called law distribution law   able  states  random variable  remark contrast discrete random variables probability con tinuous random variable  taking particular value     zero like trying speci interval    deﬁnition  cumulative distribution function cumulative distribu tion function cdf multivariate realvalued random variable  states  rd given x    cid      xd cid xd         xdcid        xdcid righthand represents probability random variable xi takes value smaller equal xi cdf expressed integral probability density function   x        zddz dzd     cid  cid xd     remark reiterate fact distinct concepts talking distributions idea pdf denoted   nonnegative function sums one second law random variable  is association random variable  pdf      deisenroth   faisal   ong published cambridge university press      set measure zero cumulative distribution function cdfs corresponding pdfs figure  examples  discrete  continuous uniform distributions example  details distributions uniform distribution actual values states meaningful here deliberately chose numbers drive home point want use and ignore ordering states probability distributions  discrete distribution  continuous distribution book use notation   x need distinguish pdf cdf however need careful pdfs cdfs section   contrasting discrete continuous distributions recall section  probabilities positive total prob ability sums one discrete random variables see  implies probability state lie interval   however continuous random variables normalization see  imply value density equal  values illustrate figure  uniform distribution discrete continuous random variables example  consider examples uniform distribution state equally likely occur example illustrates differences discrete continuous probability distributions let  discrete uniform random variable states           probability mass function represented table probability values     alternatively think graph figure  use fact states located xaxis yaxis represents probability particular state yaxis figure  deliberately extended figure  let  continuous random variable taking values range  cid  cid  represented figure  observe height draft  mathematics machine learning feedback smmlbookcom zpzzxpx  sum rule product rule bayes theorem point probability interval probability type discrete     probability mass function continuous px applicable   cid  probability density function cumulative distribution function table  nomenclature probability distributions density greater  however needs hold cid  pxdx    remark additional subtlety regards discrete prob ability distributions states      zd principle structure ie usually way compare them example   red   green   blue however machine learning applications discrete states numerical values eg               discrete states as sume numerical values particularly useful consider expected values section  random variables unfortunately machine learning literature uses notation nomen clature hides distinction sample space  target  random variable  value  set possible space outcomes random variable  ie  ability random variable  outcome  discrete random variables written     known probabil ity mass function pmf referred distribution continuous variables px called probability density function often referred density muddy things further cumulative distribution function   cid  referred distribu tion  use notation  refer univariate multivariate random variables denote states   re spectively summarize nomenclature table    remark expression probability distribution discrete probability mass functions continuous proba bility density functions technically incorrect line machine learning literature rely context distinguish different uses phrase probability distribution  sum rule product rule bayes theorem think probability theory extension logical reasoning discussed section  rules probability presented follow    deisenroth   faisal   ong published cambridge university press   px denotes prob think outcome  argument results probability px probability distributions naturally fulﬁlling desiderata jaynes    prob abilistic modeling section  provides principled foundation de signing machine learning methods deﬁned probability dis tributions section  corresponding uncertainties data problem turns fundamental rules sum rule product rule recall  px  joint distribution ran dom variables   distributions px py correspond ing marginal distributions py  conditional distribution  given  given deﬁnitions marginal conditional probability discrete continuous random variables section  present fundamental rules probability theory ﬁrst rule sum rule states px  cid yy cid px   discrete px ydy  continuous states target space random variable   means sum or integrate out set states  random variable   sum rule known marginalization property sum rule relates joint distribution marginal distribution general joint distribution contains random vari ables sum rule applied subset random variables resulting marginal distribution potentially random variable concretely        xdcid obtain marginal cid pxi  px     xddxi repeated application sum rule integratesum random variables xi indicated  reads all remark computational challenges probabilistic modeling application sum rule variables discrete variables states sum rule boils per forming highdimensional sum integral performing highdimensional sums integrals generally computationally hard sense known polynomialtime algorithm calculate exactly second rule known product rule relates joint distribution conditional distribution px   py xpx  product rule interpreted fact joint distribu tion random variables factorized written product draft  mathematics machine learning feedback smmlbookcom rules arise naturally jaynes requirements discussed section  sum rule marginalization property product rule  sum rule product rule bayes theorem distributions factors marginal distribu tion ﬁrst random variable px conditional distribution second random variable given ﬁrst py  ordering random variables arbitrary px  product rule implies ypy precise  expressed terms px   px probability mass functions discrete random variables continuous random variables product rule expressed terms probability density functions section  machine learning bayesian statistics interested making inferences unobserved latent random variables given observed random variables let assume prior knowledge px unobserved random variable  rela tionship py   second random variable  observe observe  use bayes theorem draw conclusions  given observed values  bayes theorem also bayes rule bayes law direct consequence product rule  px cid cidcid cid posterior prior cid cidcid cid px likelihood cid cidcid cid py py cidcidcidcid evidence px   px ypy px   py xpx bayes theorem bayes rule bayes law px ypy  py xpx px   py xpx py  px prior encapsulates subjective prior knowledge unobserved latent variable  observing data choose prior makes sense us critical ensure prior nonzero pdf or pmf plausible  rare likelihood py  describes   related case discrete probability distributions probability data  know latent variable  note likelihood distribution   py  likelihood  given  probability  given  likelihood  mackay  posterior px  quantity bayesian statistics expresses exactly interested in ie know  having observed  prior likelihood likelihood called measurement model posterior    deisenroth   faisal   ong published cambridge university press  marginal likelihood evidence bayes theorem called probabilistic inverse probabilistic inverse quantity probability distributions py  py xpxdx  expy cid marginal likelihoodevidence righthand  uses expectation operator deﬁne section  deﬁnition marginal likelihood integrates numerator  respect latent variable  therefore marginal likelihood independent  ensures posterior px  normalized marginal likelihood interpreted expected likelihood expectation respect prior px normalization posterior marginal likelihood plays important role bayesian model selection discuss section  integration  evidence hard compute bayes theorem  allows invert relationship   given likelihood therefore bayes theorem called probabilistic inverse discuss bayes theorem section  remark bayesian statistics posterior distribution quantity encapsulates available information prior data instead carrying posterior around possible focus statistic posterior maximum posterior discuss section  however focusing statistic posterior leads loss information think bigger con text posterior decisionmaking system having posterior extremely useful lead decisions robust disturbances example context modelbased re inforcement learning deisenroth et al  posterior distribution plausible transition functions leads fast datasample efﬁcient learning focusing maximum posterior leads consistent failures therefore having pos terior useful downstream task   continue discussion context linear regression  summary statistics independence interested summarizing sets random variables com paring pairs random variables statistic random variable de terministic function random variable summary statistics distribution provide useful view random variable behaves suggests provide numbers summarize charac terize distribution mean variance well known summary statistics discuss ways compare pair random variables ﬁrst random variables inde pendent second compute inner product them draft  mathematics machine learning feedback smmlbookcom  summary statistics independence  means covariances mean covariance useful properties probabil ity distributions expected values spread section  useful family distributions called exponential fam ily statistics random variable capture possible infor mation concept expected value central machine learning foundational concepts probability derived expected value whittle  deﬁnition  expected value expected value function     univariate continuous random variable  px given expected value exgx  gxpxdx  cid cid xx correspondingly expected value function  discrete random variable  px given exgx  gxpx  variable  set possible outcomes the target space random section consider discrete random variables numerical outcomes seen observing function  takes real numbers inputs remark consider multivariate random variables  ﬁnite vector univariate random variables      xdcid multivariate random variables deﬁne expected value element wise exgx  rd  exgx   exd gxd subscript exd indicates taking expected value respect dth element vector  deﬁnition  deﬁnes meaning notation ex operator indicating integral respect probabil ity density for continuous distributions sum states for discrete distributions deﬁnition mean deﬁnition  special case expected value obtained choosing  iden tity function deﬁnition  mean mean random variable  states mean    deisenroth   faisal   ong published cambridge university press  expected value function random variable referred law unconscious statistician casella berger  section  median mode rd average deﬁned probability distributions exx  exx   exd xd rd  exdxd  cid cid xix xdpxddxd  continuous random variable xipxd  xi  discrete random variable         subscript  indicates corresponding di mension  integral sum states target space random variable  dimension intuitive notions average median mode median middle value sort values ie  values greater median  smaller median idea generalized contin uous values considering value cdf deﬁnition   distributions asymmetric long tails median provides estimate typical value closer human intuition mean value furthermore median robust outliers mean generalization median higher dimensions nontrivial obvious way sort dimen sion hallin et al  kong mizera  mode frequently occurring value discrete random variable mode deﬁned value  having highest frequency occurrence continuous random variable mode deﬁned peak density px particular density px mode fur thermore large number modes highdimensional distributions therefore ﬁnding modes distribution computationally challenging example  consider twodimensional distribution illustrated figure  cid    cid    cid cid px   cid   cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cidµ σcid section  deﬁne gaussian distribution shown corresponding marginal distribution dimension ob serve distribution bimodal has modes draft  mathematics machine learning feedback smmlbookcom  summary statistics independence marginal distributions unimodal has mode horizontal bi modal univariate distribution illustrates mean median different other tempting deﬁne two dimensional median concatenation medians di mension fact deﬁne ordering twodimensional points makes difﬁcult cannot deﬁne ordering mean way deﬁne relation  cid cid cid cid figure  illustration mean mode median twodimensional dataset marginal densities remark expected value deﬁnition  linear operator ex ample given realvalued function    agx  bhx   rd obtain cid exf    xpxdx cid cid agx  bhxpxdx gxpxdx   hxpxdx cid  aexgx  bexhx  random variables wish characterize correspon    deisenroth   faisal   ong published cambridge university press  meanmodesmedian covariance terminology covariance multivariate random variables covx  referred crosscovariance covariance referring covx  variance standard deviation covariance probability distributions dence other covariance intuitively represents notion dependent random variables another deﬁnition  covariance univariate covariance  given expected product univariate random variables   deviations respective means ie covxy    exy cidx exxy ey ycid  remark random variable associated expectation covariance clear arguments subscript suppressed for example exx written ex linearity expectations expression deﬁnition  rewritten expected value product minus product expected values ie covx   exy exey  covariance variable covx  called variance denoted vxx square root variance called standard deviation denoted σx notion covariance generalized multivariate random variables deﬁnition  covariance multivariate consider multivari respec ate random variables   states  tively covariance   deﬁned rd  covx   exycid exeycid  covy xcid rde  deﬁnition  applied multivariate random vari able arguments results useful concept intuitively captures spread random variable multivariate random variable variance describes relation individual dimen sions random variable variance deﬁnition  variance variance random variable  states  rd mean vector  rd deﬁned vxx  covxx   exx µx µcid  exxxcid covx  covx  covx  covx  covxd        covx xd    covx xd       covxd xd exxexxcid  covariance matrix marginal  matrix  called covariance matrix mul tivariate random variable  covariance matrix symmetric pos itive semideﬁnite tells spread data diagonal covariance matrix contains variances marginals draft  mathematics machine learning feedback smmlbookcom  summary statistics independence figure  twodimensional datasets identical means variances axis colored lines different covariances crosscovariance    negatively correlated    positively correlated cid pxi  px     xddxi     denotes all variables  offdiagonal entries crosscovariance terms covxi xj          remark book generally assume covariance matrices positive deﬁnite enable better intuition discuss corner cases result positive semideﬁnite lowrank covariance ma trices want compare covariances different pairs random variables turns variance random variable affects value covariance normalized version covariance called correlation deﬁnition  correlation correlation random vari ables   given correlation corrx   covx  cidvxvy  correlation matrix covariance matrix standardized random variables xσx words random variable divided standard deviation the square root variance correlation matrix    covariance and correlation indicate random variables related figure  positive correlation corrx  means  grows  expected grow negative correlation means  increases  decreases  empirical means covariances deﬁnitions section  called population mean covariance refers true statistics population ma chine learning need learn empirical observations data con sider random variable  conceptual steps population mean covariance    deisenroth   faisal   ong published cambridge university press  xyxycid probability distributions population statistics realization empirical statistics first use fact ﬁnite dataset of size   construct empirical statistic function ﬁnite number identical random variables      xn  second observe data is look realiza tion      xn random variables apply empirical statistic speciﬁcally mean deﬁnition  given particular dataset obtain estimate mean called empirical mean sample mean holds empirical covariance deﬁnition  empirical mean covariance empirical mean vec tor arithmetic average observations variable deﬁned   cid xn  empirical covariance similar empirical mean empirical covariance matrix  xn rd matrix   cid xn xxn xcid compute statistics particular dataset use realizations observations      xn use   them pirical covariance matrices symmetric positive semideﬁnite see sec tion  empirical mean sample mean empirical mean book use empirical covariance biased estimate unbiased sometimes called corrected covariance factor    denominator instead   derivations exercises end  expressions variance focus single random variable  use preceding them pirical formulas derive possible expressions variance following derivation population variance need care integrals standard deﬁnition variance cor responding deﬁnition covariance deﬁnition  expec tation squared deviation random variable  expected value  ie vxx  exx   expectation  mean   exx computed us ing  depending  discrete continuous random variable variance expressed  mean new random variable    estimating variance  empirically need resort twopass algorithm pass data calculate mean   second pass estimate ˆµ calculate draft  mathematics machine learning feedback smmlbookcom  summary statistics independence variance turns avoid passes rearranging terms formula  converted socalled rawscore formula variance rawscore formula variance terms  huge approximately equal suffer unnecessary loss numerical precision ﬂoatingpoint arithmetic vxx  exx exx  expression  remembered the mean square minus square mean calculated empirically pass data accumulate xi to calculate mean  simultaneously xi ith observation unfortunately imple mented way numerically unstable rawscore version variance useful machine learning eg deriving biasvariance decomposition bishop  way understand variance sum pairwise dif ferences pairs observations consider sample      xn realizations random variable  compute squared differ ence pairs xi xj expanding square sum   pairwise differences empirical variance observations   cid ij xi xj   cid cid cid cid   xi  twice rawscore expression  means express sum pairwise distances of   them sum deviations mean of   ge ometrically means equivalence pairwise distances distances center set points computational perspective means computing mean  terms summation computing variance again  terms summation obtain expression lefthand    terms  sums transformations random variables want model phenomenon explained textbook distributions we introduce sections   perform simple manipulations random variables such adding random variables consider random variables   states   rd then ex    ex  ey ex ey   ex vx    vx  vy  covx   covy  vx covy     vx  vy covx     deisenroth   faisal   ong published cambridge university press  probability distributions mean covariance exhibit useful properties comes afﬁne transformation random variables consider random variable  mean  covariance matrix  deterministic afﬁne transformation   ax     random variable mean vector covariance matrix given ey   exax    aexx    aµ    vy   vxax    vxax  avxxacid  aσacid  respectively furthermore covx   exax  bcid exeax  bcid µbcid µµcidacid µµcidcidacid  exbcid  exxcidacid  µbcid  σacid  µbcid  cidexxcid   exxcid µµcid covariance  shown directly deﬁnition mean covariance statistical independence deﬁnition  independence random variables   statis tically independent  statistical independence px   pxpy  intuitively random variables   independent value  once known add additional information  and vice versa   statistically independent   py   px py px vxy     vxx  vy  covxy     point hold converse ie random variables covariance zero statistically independent understand why recall covariance measures linear dependence therefore random variables nonlinearly dependent covariance zero example  consider random variable  zero mean exx   exx   let    hence  dependent  consider covariance     gives covx   exy exey  ex    draft  mathematics machine learning feedback smmlbookcom  summary statistics independence machine learning consider problems mod eled independent identically distributed iid random variables      xn  random variables word indepen dent deﬁnition  usually refers mutually independent random variables subsets independent see pollard  chap ter  jacod protter    phrase identically distributed means random variables distri bution concept important machine learning conditional independence deﬁnition  conditional independence random variables   conditionally independent given  px    px zpy   denote  conditionally independent  given  set states random variable  write  deﬁnition  requires relation  hold true value  interpretation  understood given knowledge  distribution   factorizes independence cast special case conditional independence write  lefthand  obtain  product rule probability  expand   px    px  zpy   comparing righthand   py appears px    px   equation  provides alternative deﬁnition conditional indepen dence ie   alternative presentation provides inter pretation given know  knowledge  change knowledge   inner products random variables recall deﬁnition inner products section  deﬁne inner product random variables brieﬂy section uncorrelated random variables    vx    vx  vy  variances measured squared units looks like pythagorean theorem right triangles      following ﬁnd geometric interpreta tion variance relation uncorrelated random variables     deisenroth   faisal   ong published cambridge university press  independent identically distributed iid conditionally independent inner products multivariate random variables treated similar fashion figure  geometry random variables random variables  uncorrelated orthogonal vectors corresponding vector space pythagorean theorem applies covx        covαx      covx   covz     probability distributions random variables considered vectors vector space deﬁne inner products obtain geometric properties random vari ables eaton  deﬁne   cid cid  covx  zero mean random variables    obtain inner product covariance symmetric positive deﬁnite linear argument length random variable cid cid cid covx   vx  σx  cid ie standard deviation longer random variable uncertain is random variable length  deterministic look angle  random variables      cos   cid cid cid covx  cidvxvy cid cid cid correlation deﬁnition  random vari ables means think correlation cosine angle random variables consider geometri cally know deﬁnition     case means   orthogonal covx    ie uncorrelated figure  illustrates relationship  cid   cid remark tempting use euclidean distance constructed draft  mathematics machine learning feedback smmlbookcom cidvarycidvarxcidvarxycidvarxvaryacb  gaussian distribution figure  gaussian distribution random variables  preceding deﬁnition inner products compare probability distributions unfortunately best way obtain distances be tween distributions recall probability mass or density posi tive needs add  constraints mean distributions live called statistical manifold study space probability distributions called information geometry computing dis tances distributions kullbackleibler diver gence generalization distances account properties statistical manifold like euclidean distance special case metric section  kullbackleibler divergence special case general classes divergences called bregman divergences  divergences study divergences scope book refer details recent book amari  founders ﬁeld information geometry  gaussian distribution gaussian distribution wellstudied probability distribution continuousvalued random variables referred normal distribution importance originates fact com putationally convenient properties discussing fol lowing particular use deﬁne likelihood prior linear regression   consider mixture gaussians density estimation   areas machine learning beneﬁt gaussian distribution example gaussian processes variational inference reinforcement learning widely ap plication areas signal processing eg kalman ﬁlter control eg linear quadratic regulator statistics eg hypothesis testing normal distribution gaussian distribution arises naturally consider sums independent identically distributed random variables known central limit theorem grinstead snell     deisenroth   faisal   ong published cambridge university press  xxpxx figure  gaussian distributions overlaid  samples  one dimensional case  twodimensional case multivariate gaussian distribution mean vector covariance matrix known multivariate normal distribution standard normal distribution probability distributions  univariate onedimensional gaussian red cross shows mean red line shows extent variance  multivariate twodimensional gaus sian viewed top red cross shows mean colored lines con tour lines density univariate random variable gaussian distribution den sity given px    πσ exp cid cid multivariate gaussian distribution fully characterized mean vector  covariance matrix  deﬁned px  exp cid µcidσx       cidx rd write px  µcid  cidµ σcid fig ure  shows bivariate gaussian mesh corresponding con tour plot figure  shows univariate gaussian bivariate gaussian corresponding samples special case gaussian zero mean identity covariance is       referred standard normal distribution  σcid    gaussians widely statistical estimation machine learn ing closedform expressions marginal conditional dis tributions   use closedform expressions extensively linear regression major advantage modeling gaussian ran dom variables variable transformations section  needed gaussian distribution fully speciﬁed mean covariance obtain transformed distribution applying transformation mean covariance random variable  marginals conditionals gaussians gaussians following present marginalization conditioning gen eral case multivariate random variables confusing ﬁrst read ing reader advised consider univariate random variables in stead let   multivariate random variables draft  mathematics machine learning feedback smmlbookcom xpxmeansampleσxxmeansample   gaussian distribution different dimensions consider effect applying sum rule probability effect conditioning explicitly write gaus sian distribution terms concatenated states xcid ycid px   cid cidcidµx µy cidσxx σxy σyx σyy cidcid σxx  covx  σyy  covy  marginal covari ance matrices   respectively σxy  covx  cross covariance matrix   conditional distribution px  gaussian illustrated fig ure  given derived section  bishop  px cid   cidµx   σx   µx    µx  σxyς yy  σxyς yy σyx  σx    σxx µy note computation mean  yvalue observation longer random remark conditional gaussian distribution shows places interested posterior distributions kalman ﬁlter kalman  central algorithms state estimation signal processing computing gaussian conditionals joint distributions deisenroth ohlsson  sarkka  gaussian processes rasmussen williams  prac tical implementation distribution functions gaussian pro cess assumptions joint gaussianity random variables gaussian conditioning observed data determine poste rior distribution functions latent linear gaussian models roweis ghahramani  mur phy  include probabilistic principal component analysis ppca tipping bishop  look ppca de tail section  marginal distribution px joint gaussian distribution px  see  gaussian computed applying sum rule  given cid px  px ydy  cidx µx σxx cid  corresponding result holds py obtained marginaliz ing respect  intuitively looking joint distribution  ignore ie integrate out interested in illustrated figure     deisenroth   faisal   ong published cambridge university press  example  probability distributions figure   bivariate gaussian  marginal joint gaussian distribution gaussian  conditional distribution gaussian gaussian  bivariate gaussian  marginal distribution  conditional distribution consider bivariate gaussian distribution illustrated figure  cidcid cid px   cidcid cid compute parameters univariate gaussian conditioned    applying   obtain mean vari ance respectively numerically µx               therefore conditional gaussian given     px     cid cid  marginal distribution px contrast obtained apply ing  essentially mean variance random variable  giving px  cid cid  draft  mathematics machine learning feedback smmlbookcom xxxxpxmeanσxpxxmeanσ derivation exercise end  gaussian distribution  product gaussian densities linear regression   need compute gaussian likeli hood furthermore wish assume gaussian prior section  apply bayes theorem compute posterior results mul tiplication likelihood prior is multiplication  bcid gaussian densities product gaussians  ccid gaussian distribution scaled  cidx  given   acid cidx cidx        caa  bb    exp cid    bcida  ba bcid     scaling constant  written form gaussian density  inﬂated covariance matrix   ie    scid remark notation convenience use functional form gaussian density  random variable preceding demonstration wrote   bcid    bcid cidx cida cidb   cida   bcid  cidb   bcid  here  random variables however writing  way compact   sums linear transformations   independent gaussian random variables ie joint distri cid bution given px   pxpy px  cid    gaussian distributed given py  µx σx µy σy cidx cidy px    cidµx  µy σx  σy cid  knowing px   gaussian mean covariance matrix determined immediately results   property important consider iid gaussian noise acting random variables case linear regression chap ter  example  expectations linear operations obtain weighted sum independent gaussian random variables pax  by  cidaµx  bµy aσx  bσy cid     deisenroth   faisal   ong published cambridge university press  probability distributions remark case useful   weighted sum gaussian densities different weighted sum gaussian random variables theorem  random variable  density mixture densities px px weighted  theorem generalized multivariate random variable case linearity expectations holds multivariate random variables however idea squared random variable needs replaced xxcid theorem  consider mixture univariate gaussian densities px  αpx   αpx  scalar      mixture weight px px univariate gaussian densities equation  different parameters ie      mean mixture density px given weighted sum means random variable ex  αµ   αµ  variance mixture density px given vx  cidασ    ασ cid cidcidαµ    cid αµ αµ   αµcid proof mean mixture density px given weighted sum means random variable apply deﬁnition mean deﬁnition  plug mixture  yields cid  cid  cid  cid  cid  ex  xpxdx αxpx   αxpx dx   xpxdx   xpxdx cid   αµ   αµ  compute variance use rawscore version vari ance  requires expression expectation squared random variable use deﬁnition expectation function the square random variable deﬁnition  ex  xpxdx cidαxpx   αxpxcid dx draft  mathematics machine learning feedback smmlbookcom cid  gaussian distribution   xpxdx   xpxdx cid   αµ       cid     αµ   equality rawscore version variance  giving   ex  rearranged expectation squared random variable sum squared mean variance therefore variance given subtracting   vx  ex  αµ  cidασ ex    αµ      αµ αµ       cidcidαµ    cid ασ αµ cid αµ   αµcid remark preceding derivation holds density gaussian fully determined mean variance mixture den sity determined closed form mixture density individual components considered conditional distributions conditioned component identity equation  example conditional variance formula known law total variance generally states ran dom variables   holds vxx  ey vxx ie total variance  expected conditional variance plus variance conditional mean yvy exx consider example  bivariate standard gaussian random variable  performed linear transformation ax it outcome gaussian random variable mean zero covariance aacid ob serve adding constant vector change mean distribu tion affecting variance is random variable    gaussian mean  identity covariance hence linearafﬁne transformation gaussian random variable gaussian distributed consider gaussian distributed random variable  cidµ σcid given matrix appropriate shape let  random variable   ax transformed version  compute mean  exploiting expectation linear operator  follows ey  eax  aex  aµ    similarly variance   vy  vax  avxacid  aσacid  means random variable  distributed according cidy aµ aσacidcid py     deisenroth   faisal   ong published cambridge university press  law total variance linearafﬁne transformation gaussian random variable gaussian distributed probability distributions let consider reverse transformation know random variable mean linear transformation rm    cid   random variable given rank matrix let  rm gaussian random variable mean ax ie py  cidy ax σcid  corresponding probability distribution px invert ible write   ay apply transformation previous paragraph however general invertible use approach similar pseudoinverse  is pre multiply sides acid invert acida symmetric positive deﬁnite giving relation   ax acidaacidy    hence  linear transformation  obtain px  cidx acidaacidy acidaacidσaacidacid   sampling multivariate gaussian distributions explain subtleties random sampling computer interested reader referred gentle  case mul tivariate gaussian process consists stages ﬁrst need source pseudorandom numbers provide uniform sample interval  second use nonlinear transformation boxmuller transform devroye  obtain sample univari ate gaussian third collate vector samples obtain sample multivariate standard normal cid icid general multivariate gaussian is mean non zero covariance identity matrix use proper ties linear transformations gaussian random variable assume interested generating samples xi        multivariate gaussian distribution mean  covariance matrix  like construct sample sampler provides samples multivariate standard normal cid icid obtain samples multivariate normal cidµ σcid use properties linear transformation gaussian random variable cid icid   ax   aacid   gaussian dis tributed mean  covariance matrix  convenient choice use cholesky decomposition section  covariance matrix   aacid cholesky decomposition beneﬁt triangular leading efﬁcient computation   draft  mathematics machine learning feedback smmlbookcom compute cholesky factorization matrix required matrix symmetric positive deﬁnite section  covariance matrices possess property  conjugacy exponential family  conjugacy exponential family probability distributions with names ﬁnd statis tics textbooks discovered model particular types phenomena example seen gaussian distribution section  distributions related complex ways leemis mcqueston  beginner ﬁeld overwhelming ﬁgure distribution use addition distribu tions discovered time statistics computation pencil paper natural ask meaningful concepts computing age efron hastie  previous section saw operations required inference conve niently calculated distribution gaussian worth recalling point desiderata manipulating probability distributions machine learning context  closure property applying rules probability eg bayes theorem closure mean applying particular operation returns object type  collect data need parameters  interested learning data want parameter es distribution timation behave nicely turns class distributions called exponential family provides right balance generality retaining favorable compu tation inference properties introduce exponential fam ily let members named probability distributions bernoulli example  binomial example  beta exam ple  distributions computers job description exponential family example  bernoulli distribution distribution single binary random bernoulli variable  state  rameter  distribution berµ deﬁned  governed single continuous pa   represents probability    bernoulli     distribution px   µx µx       ex    vx     ex vx mean variance binary random variable  example bernoulli distribution interested modeling probability heads ﬂipping coin    deisenroth   faisal   ong published cambridge university press  probability distributions figure  examples binomial distribution         binomial distribution remark rewriting bernoulli distribution use boolean variables numerical   express exponents trick machine learning textbooks oc curence expressing multinomial distribution example  binomial distribution binomial distribution generalization bernoulli distribution distribution integers illustrated figure  particular binomial probability observing  oc currences    set  samples bernoulli distribution px       binomial distribution binn  deﬁned pm cid cid    em     vm      µm µn   em vm mean variance  respectively example binomial want probability observing  heads  coinﬂip experiments probability observing head single experiment  beta distribution example  beta distribution wish model continuous random variable ﬁnite interval beta distribution distribution continuous random variable   represent probability binary event eg parameter governing bernoulli distribution beta draft  mathematics machine learning feedback smmlbookcom numbermofobservationsxinnexperimentspmµµµ  conjugacy exponential family distribution betaα  illustrated figure  governed parameters       deﬁned pµ    eµ  γα   γαγβ    µα µβ vµ  αβ   βα      gamma function deﬁned γt  xt exp xdx     cid  γt    tγt  note fraction gamma functions  normalizes beta distribution figure  examples beta distribution different values  intuitively  moves probability mass   moves prob ability mass  special cases murphy       obtain uniform distribution     bimodal distribution spikes       distribution unimodal        distribution unimodal symmetric centered interval   ie modemean      remark zoo distributions names related different ways leemis mcqueston  worth keeping mind named distribution created particular reason applications knowing reason creation particular distribution allows insight best use it introduced preceding distributions    deisenroth   faisal   ong published cambridge university press  µpµαβαβαβαβαβαβ conjugate prior conjugate probability distributions able illustrate concepts conjugacy section  exponen tial families section   conjugacy according bayes theorem  posterior proportional product prior likelihood speciﬁcation prior tricky reasons first prior encapsulate knowl edge problem data difﬁcult describe second possible compute posterior distribu tion analytically however priors computationally convenient conjugate priors deﬁnition  conjugate prior prior conjugate likelihood function posterior formtype prior conjugacy particularly convenient algebraically cal culate posterior distribution updating parameters prior distribution remark considering geometry probability distributions con jugate priors retain distance structure likelihood agarwal daume iii  introduce concrete example conjugate priors ex ample  binomial distribution deﬁned discrete random vari ables beta distribution deﬁned continuous random vari ables example  betabinomial conjugacy consider binomial random variable  cid cid px    binn  µx µn             probability ﬁnding  times outcome heads  coin ﬂips  probability head place beta prior pa rameter  is  betaα  pµ    γα   γαγβ µα µβ  observe outcome    is  heads  coin ﬂips compute posterior distribution  pµ        µpµ   µn hµα px µh  µhα µn hβ µβ draft  mathematics machine learning feedback smmlbookcom  conjugacy exponential family likelihood bernoulli binomial gaussian gaussian multinomial dirichlet conjugate prior beta beta gaussianinverse gamma gaussianinverse gamma gaussianinverse wishart gaussianinverse wishart posterior beta beta dirichlet table  examples conjugate priors common likelihood functions betah        ie posterior distribution beta distribution prior ie beta prior conjugate parameter  binomial likelihood function following example derive result similar betabinomial conjugacy result beta distribu tion conjugate prior bernoulli distribution   example  betabernoulli conjugacy   let  parameter    θx px bution parameters   is pθ distributed according bernoulli distribution    expressed   is px   θx let  distributed according beta distri θα θβ   multiplying beta bernoulli distributions pθ θpθ     px θx  θαx   θxθα θβx θβ       pθ   line beta distribution parameters       table  lists examples conjugate priors parameters standard likelihoods probabilistic modeling distributions multinomial inverse gamma inverse wishart dirichlet statistical text described bishop  example beta distribution conjugate prior parameter  binomial bernoulli likelihood gaussian likelihood func tion place conjugate gaussian prior mean reason gaussian likelihood appears twice table need distinguish univariate multivariate case univariate scalar case inverse gamma conjugate prior variance multivariate case use conjugate inverse wishart distribution prior covariance matrix dirichlet distribution conju gamma prior conjugate precision inverse variance univariate gaussian likelihood wishart prior conjugate precision matrix inverse covariance matrix multivariate gaussian likelihood    deisenroth   faisal   ong published cambridge university press  sufﬁcient statistics probability distributions gate prior multinomial likelihood function details refer bishop   sufﬁcient statistics    recall statistic random variable deterministic function random variable example        xn cid vector cidµ σcid univariate gaussian random variables is xn   sample mean ˆµ    xn  statistic sir ronald fisher dis covered notion sufﬁcient statistics idea statistics contain available information inferred data corresponding distribution consideration words suf ﬁcient statistics carry information needed inference population is statistics sufﬁcient repre sent distribution    set distributions parametrized  let  random variable distribution px  given unknown  vector φx statistics called sufﬁcient statistics  contain possible informa tion  formal contain possible information means probability  given  factored depend  depends  φx fisherneyman factorization theorem formalizes notion state theorem  proof px   hxgθφx  hx distribution independent  gθ captures depen dence  sufﬁcient statistics φx px  depend  φx trivially sufﬁcient statistic function  interesting case px  dependent φx  itself case φx sufﬁcient statistic machine learning consider ﬁnite number samples distribution imagine simple distributions such bernoulli example  need small number samples estimate parameters distributions consider opposite problem set data  sample unknown distribution distribution gives best ﬁt natural question ask is observe data need parameters  de scribe distribution turns answer yes general studied nonparametric statistics wasserman  converse question consider class distributions ﬁnitedimensional draft  mathematics machine learning feedback smmlbookcom fisherneyman theorem theorem  fisherneyman theorem  lehmann casella  let  probability density function px  statistics φx sufﬁcient  px  written form  conjugacy exponential family sufﬁcient statistics number parameters needed increase arbitrarily answer exponential family dis tributions described following section  exponential family possible levels abstraction con sidering distributions of discrete continuous random variables level the concrete end spectrum particu lar named distribution ﬁxed parameters example univariate cid cid zero mean unit variance machine learning gaussian use second level abstraction is ﬁx paramet ric form the univariate gaussian infer parameters data cidµ σcid unknown mean example assume univariate gaussian  unknown variance  use maximum likelihood ﬁt deter best parameters   example considering linear regression   level abstraction consider families distributions book consider ex ponential family univariate gaussian example member exponential family widely statistical models includ ing named models table  members exponential family uniﬁed concept brown  remark brief historical anecdote like concepts mathemat ics science exponential families independently discovered time different researchers years  edwin pitman tasmania georges darmois paris bernard koopman new york independently showed exponential families families enjoy ﬁnitedimensional sufﬁcient statistics repeated independent sampling lehmann casella  exponential family family probability distributions parame terized  rd form px   hx exp   φx cid φx vector sufﬁcient statistics general inner prod uct section   concreteness use  θcidφx note form  φx standard dot product  cid cid exponential family essentially particular expression gθφx fisherneyman theorem theorem  aθ  cid  factor hx absorbed dot product term adding entry log hx vector sufﬁcient statistics φx constraining corresponding parameter    term aθ normalization constant ensures distribution sums inte grates called logpartition function good intuitive no tion exponential families obtained ignoring terms    deisenroth   faisal   ong published cambridge university press  exponential family logpartition function natural parameters probability distributions considering exponential families distributions form px exp cidθcidφxcid  form parametrization parameters  called natural parameters ﬁrst glance exponential families mun dane transformation adding exponential function result dot product however implications allow conve nient modeling efﬁcient computation based fact capture information data φx example  gaussian exponential family consider univariate gaussian distribution cidµ σcid let φx  cid cid  deﬁnition exponential family setting px expθx  θx    cidcid cid  substituting  obtain cid µx px exp   cid cid   cid exp therefore univariate gaussian distribution member expo nential family sufﬁcient statistic φx   natural parame cid cid  ters given   example  bernoulli exponential family recall bernoulli distribution example  px   µx µx       written exponential family form   exp cidlog cidµx µxcidcid px  exp  log     exp  log  cid  log    log  log  exp  log   log cid line  identiﬁed exponential family form  observing hx   draft  mathematics machine learning feedback smmlbookcom sigmoid  conjugacy exponential family   log  φx   aθ  relationship   invertible log   log  expθ     exp relation  obtain right equality     remark relationship original bernoulli parameter  natural parameter  known sigmoid logistic function ob  sigmoid function serve  squeezes real value range   property useful ma chine learning example logistic regression bishop  section  nonlinear activation functions neural net works goodfellow et al    obvious ﬁnd parametric form conjugate distribution particular distribution for example table  exponential families provide convenient way ﬁnd conjugate pairs distributions consider random variable  member expo nential family  px   hx exp   φx cid member exponential family conjugate prior brown aθ  cid  pθ   hcθ exp cidcidcidγ cid cid cidcid aθ cid acγ cid cidγ   dimension dimθ   sufﬁcient statistics conjugate prior  knowledge general form conjugate priors exponential families derive functional forms conjugate priors corresponding particular distributions cid cid aθ example  recall exponential family form bernoulli distribution  px   exp cid  log cid  log    deisenroth   faisal   ong published cambridge university press  probability distributions canonical conjugate prior form pµ    cid  log exp     log acγ deﬁned      αcid hcµ   tion  simpliﬁes pµ    exp   log     log acα   putting nonexponential family form yields cid  equa pµ   µα µβ  identi beta distribution  example  assumed beta distribution conjugate prior bernoulli distribution showed conjugate prior example derived form beta distribution looking canonical conjugate prior bernoulli distribution exponential fam ily form mentioned previous section main motivation expo nential families ﬁnitedimensional sufﬁcient statistics additionally conjugate distributions easy write down con jugate distributions come exponential family infer ence perspective maximum likelihood estimation behaves nicely empirical estimates sufﬁcient statistics optimal estimates pop ulation values sufﬁcient statistics recall mean covariance gaussian optimization perspective loglikelihood function concave allowing efﬁcient optimization approaches applied    change variablesinverse transform known distributions reality set distributions names limited there fore useful understand transformed random variables distributed example assuming  random variable dis cid cid tributed according univariate normal distribution distribution   example common ma chine learning is given   univariate standard normal distribution      option work distribution      calculate mean variance   combine them saw section  calculate mean variance resulting ran dom variables consider afﬁne transformations random vari draft  mathematics machine learning feedback smmlbookcom  change variablesinverse transform ables however able obtain functional form distribution transformations furthermore interested nonlinear transformations random variables closedform expressions readily available remark notation section explicit random vari ables values take hence recall use capital letters   denote random variables small letters   denote val random variables take explicitly ues target space write pmfs discrete random variables      continuous random variables  section  pdf written   cdf written x look approaches obtaining distributions transfor mations random variables direct approach deﬁnition cumulative distribution function changeofvariable approach uses chain rule calculus section  changeofvariable ap moment generating proach widely provides recipe attempting compute resulting distribution transformation ex plain techniques univariate random variables brieﬂy provide results general case multivariate random variables transformations discrete random variables understood di rectly suppose discrete random variable  pmf     section  invertible function  consider trans formed random variable    pmf     functions study transformations random variables casella berger          you         transformation inverse observe    therefore discrete random variables transformations directly change individual events with probabilities appropriately transformed  distribution function technique distribution function technique goes ﬁrst principles uses deﬁnition cdf x    cid  fact differential pdf   wasserman    random variable  function  ﬁnd pdf random variable     finding cdf      cid   differentiating cdf   pdf         dy    deisenroth   faisal   ong published cambridge university press  probability distributions need mind domain random variable changed transformation  example  let  continuous random variable probability density function  cid  cid       interested ﬁnding pdf     function  increasing function  resulting value  lies interval   obtain      cid      cid     cid      y    cid  tdt  cidtcidty       cid  cid   deﬁnition cdf transformation inverse deﬁnition cdf cdf deﬁnite integral result integration therefore cdf        cid  cid  obtain pdf differentiate cdf       dy    cid  cid  example  considered strictly monotonically increasing func tion     means compute inverse function general require function    in verse    useful result obtained considering cu mulative distribution function x random variable  transformation  leads following theorem theorem  theorem  casella berger  let  continuous random variable strictly monotonic cumulative distribu tion function x random variable  deﬁned   x uniform distribution draft  mathematics machine learning feedback smmlbookcom functions inverses called bijective functions section  probability integral transform  change variablesinverse transform theorem  known probability integral transform derive algorithms sampling distributions transforming result sampling uniform random variable bishop  algorithm works ﬁrst generating sample uniform distribu tion transforming inverse cdf assuming available obtain sample desired distribution probability integral transform hypothesis testing sample comes particular distribution lehmann romano  idea output cdf gives uniform distribution forms basis copu las nelsen   change variables distribution function technique section  derived ﬁrst principles based deﬁnitions cdfs properties in verses differentiation integration argument ﬁrst principles relies facts  transform cdf  expression cdf   differentiate cdf obtain pdf let break reasoning step step goal understand ing general changeofvariables approach theorem  remark change variables comes idea chang ing variable integration faced difﬁcult integral univariate functions use substitution rule integration cid cid  gxgcidxdx   youdu   gx  change variables probability relies changeofvariables method calculus tandra derivation rule based chain rule calculus  applying twice fundamental theorem calculus fundamental theorem calculus formalizes fact integration differentiation inverses other intuitive understanding rule obtained thinking loosely small changes differen tials equation  gx considering you  gcidxx differential  gx substituting  gx argument inside integral righthand   gx pretending you  gcidxx term du approximated du dx  obtain  consider univariate random variable  invertible function  gives random variable    assume random variable  states    deﬁnition cdf      cid      deisenroth   faisal   ong published cambridge university press  cid  cid  probability distributions interested function random variable   cid    you  cid   assume function invertible invertible function interval strictly increasing strictly decreasing case strictly increasing inverse  strictly increasing applying inverse  arguments  you  cid  obtain  you  cid    you you  cid     cid   rightmost term  expression cdf  recall deﬁnition cdf terms pdf   cid    xdx  expression cdf  terms      xdx  obtain pdf differentiate  respect     y   xdx  dy cid  dy note integral righthand respect  need integral respect  differentiating respect  particular use  substitution cid cid  you yyou cid ydy   xdx      righthand  gives    cid  dy you yyou cid ydy  recall differentiation linear operator use subscript  remind you  function   invoking fundamental theorem calculus gives    you  cid  dy cid recall assumed strictly increasing function decreas ing functions turns negative sign follow derivation introduce absolute value differential expression increasing decreasing     you  cid cid cid cid dy cid cid cid cid draft  mathematics machine learning feedback smmlbookcom changeofvariable technique  change variablesinverse transform cid cid called changeofvariable technique term cid  measures unit volume changes applying see deﬁnition jacobian section  dy  cid cid cid cid cid cid remark comparison discrete case  addi cid cid dy  tional factor cid continuous case requires care        probability density function   description probability event involving  far section studying univariate change vari ables case multivariate random variables analogous com plicated fact absolute value multivariate functions instead use determinant jacobian matrix recall  jacobian matrix partial derivatives existence nonzero determinant shows invert ja cobian recall discussion section  determinant arises differentials cubes volume transformed paral lelepipeds jacobian let summarize preceding discussion following theorem gives recipe multivariate change variables theorem  theorem  billingsley  let   value probability density multivariate continuous random variable  vectorvalued function    differentiable invertible values domain  corresponding values  probability density    given    you  det cid cid cid cid cid  cidcid cid cid cid theorem looks intimidating ﬁrst glance key point change variable multivariate random variable follows pro cedure univariate change variable need work inverse transform substitute density  calculate determinant jacobian multiply result following example illustrates case bivariate random variable example  bility density function cidcidx consider bivariate random variable  states   proba cid cidx cidcid exp cid cidx cidcid cidx cidcid use changeofvariable technique theorem  derive    deisenroth   faisal   ong published cambridge university press  probability distributions effect linear transformation section  random variable consider matrix  deﬁned cida    cid interested ﬁnding probability density function trans formed bivariate random variable  states   ax recall change variables require inverse transformation  function  consider linear transformations inverse transformation given matrix inverse see section   matrices explicitly write formula given cid cidx   cid cidy ad cid  cid cid cidy observe ad sponding probability density function given determinant section   corre     ay  cid exp  ycidaciday cid partial derivative matrix times vector respect vector matrix section  recall section  determinant inverse inverse determinant determinant jacobian matrix ay    det cid  cid ay ad able apply changeofvariable formula theo rem  multiplying   yields      det cid cid cid cid cidcid cid cid cid ay cid   ycidaciday cid exp cid ad example  based bivariate random variable al lows easily compute matrix inverse preceding relation holds higher dimensions remark saw section  density    actually standard gaussian distribution transformed density   bivariate gaussian covariance   aacid use ideas  probabilistic modeling draft  mathematics machine learning feedback smmlbookcom  reading section  introduce graphical language section  direct machine learning applications ideas    reading  terse times grinstead snell  walpole et al  provide relaxed presentations suit able selfstudy readers interested philosophical aspects probability consider hacking  approach related software engineering presented downey  overview exponential families barndorffnielsen  use probability distributions model machine learning tasks   ironically recent surge neural networks resulted broader appreciation probabilistic models example idea normalizing ﬂows jimenez rezende mohamed  relies change variables transform ing random variables overview methods variational inference applied neural networks described    book goodfellow et al  stepped large difﬁculty continuous random vari ables avoiding measure theoretic questions billingsley  pollard  assuming construction real numbers ways deﬁning sets real numbers appropriate fre quency occurrence details matter example speciﬁ cation conditional probability py  continuous random variables   proschan presnell  lazy notation hides fact want speci    which set measure zero fur thermore interested probability density function  precise notation eyf  σx expectation  test function  conditioned σalgebra  technical audience interested details probability the ory options jaynes  mackay  jacod protter  grimmett welsh  including technical discus sions shiryayev  lehmann casella  dudley  bickel doksum  cinlar  alternative way approach proba bility start concept expectation work backward derive necessary properties probability space whittle  machine learning allows model intricate distributions complex types data developer probabilistic machine learn ing models understand technical aspects ma chine learning texts probabilistic modeling focus include books mackay  bishop  rasmussen williams  bar ber  murphy     deisenroth   faisal   ong published cambridge university press  probability distributions  consider following bivariate distribution px  discrete random variables    exercises                compute  marginal distributions px py  conditional distributions pxy   pyx    consider mixture gaussian distributions illustrated figure    cid cidcid cid cidcid    cid cidcid cid cidcid  compute marginal distributions dimension  compute mean mode median marginal distribution  compute mean mode twodimensional distribution  written program compiles some times code change decide model apparent stochas ticity success vs success  compiler bernoulli distribution parameter  px    µx  µx       choose conjugate prior bernoulli likelihood compute pos terior distribution pµ       xn   bags ﬁrst bag contains mangos apples second bag contains mangos apples biased coin shows heads probability  tails probability  coin shows heads pick fruit random bag  pick fruit random bag  friend ﬂips coin you result picks fruit random corresponding bag presents mango probability mango picked bag  hint use bayes theorem  consider timeseries model xt  axt       cid qcid    cid rcid  yt  cxt      iid gaussian noise variables further assume px   cidµ  cid draft  mathematics machine learning feedback smmlbookcom exercises  form px      xt  justi answer you explicitly compute joint distribution  assume pxt       yt   cidµt σt cid  compute pxt       yt  compute pxt yt       yt  time  observe value yt  ˆy compute conditional distribution pxt       yt prove relationship  relates standard deﬁnition variance rawscore expression variance prove relationship  relates pairwise difference be tween examples dataset rawscore expression variance  express bernoulli distribution natural parameter form ex ponential family   express binomial distribution exponential family distribution express beta distribution exponential family distribution product beta binomial distribution member exponential family  derive relationship section  ways  completing square  expressing gaussian exponential family form product gaussians  cidx   acidn cidx   bcid unnormalized gaussian distribution   cidx   ccid        caa  bb           exp cid      bcida  ba  bcid  note normalizing constant  considered normalized gaussian distribution  inﬂated covariance matrix   ie    cida    bcid   cidb    bcid  iterated expectations consider random variables   joint distribution px  ex   ey cidex   ycid  here ex    denotes expected value  conditional distri bution px    manipulation gaussian random variables consider gaussian random variable    cidx  µx σx furthermore cid   rd   ax        re  red   re    cidw   qcid indepen dent gaussian noise independent implies   independent random variables  diagonal  write likelihood py    distribution py  cid py  xpxdx gaussian compute mean µy covariance σy derive result detail    deisenroth   faisal   ong published cambridge university press  probability distributions  random variable  transformed according measure ment mapping   cy      rf    rf     cidv   rcid independent gaus sian measurement noise write pz   compute pz ie mean µz covariance σz derive result detail  now value ˆy measured compute posterior distribution px  ˆy hint solution posterior gaussian ie need de termine mean covariance matrix start explicitly com puting joint gaussian px  requires compute crosscovariances covxyx  covyxy  apply rules gaussian conditioning  probability integral transformation given continuous random variable  cdf   ran dom variable     uniformly distributed theorem  draft  mathematics machine learning feedback smmlbookcom continuous optimization machine learning algorithms implemented computer mathematical formulations expressed numerical optimization meth ods  describes basic numerical methods training ma chine learning models training machine learning model boils ﬁnding good set parameters notion good de termined objective function probabilistic model examples second book given objective function ﬁnding best value optimization algorithms  covers main branches continuous optimization fig ure  unconstrained constrained optimization assume  objective function differentiable see   access gradient location space help ﬁnd optimum value convention objective functions ma chine learning intended minimized is best value minimum value intuitively ﬁnding best value like ﬁnding val leys objective function gradients point uphill idea downhill opposite gradient hope ﬁnd deepest point unconstrained optimization concept need design choices discuss section  constrained optimization need introduce concepts man age constraints section  introduce special class problems convex optimization problems section  statements reaching global optimum  function value approximately   function smooth gradients help ﬁnd min imum indicating step right left assumes correct bowl exists local minimum    recall solve stationary points function calculating derivative setting zero cidx           obtain corresponding gradient dcidx dx         material published cambridge university press mathematics machine learning marc peter deisenroth  aldo faisal cheng soon ong  version free view download personal use only redistribution resale use derivative works by   deisenroth   faisal   ong  smmlbookcom consider data models rd optimization problems face continuous optimization problems opposed combinatorial optimization problems discrete variables local minimum stationary points real roots derivative is points zero gradient consider function figure  function global minimum global minimum continuous optimization figure  mind map concepts related optimization presented main ideas gradient descent convex optimization continuous optimization stepsize unconstrained optimization gradient descent momentum constrained optimization   dimension reduc stochastic gradient descent lagrange multipliers   density estimation convex convex optimization  duality linear programming convex conjugate quadratic programming   classiﬁcation cubic equation general solutions set zero example minimums maximum around    check stationary point minimum maximum need derivative second time check second derivative positive negative stationary point case second derivative dcidx dx        substituting visually estimated values   observe expected middle point maximum stationary points minimums   cid cid dcidx dx   note avoided analytically solving values  previous discussion loworder polynomials pre ceding so general unable ﬁnd analytic solu tions need start value    follow negative gradient negative gradient indicates draft  mathematics machine learning feedback smmlbookcom  optimization gradient descent figure  example objective function negative gradients indicated arrows global minimum indicated dashed blue line right far this called stepsize furthermore started right eg    negative gradient led wrong minimum figure  illustrates fact    negative gradient points minimum right ﬁgure larger objective value section  learn class functions called convex functions exhibit tricky dependency starting point optimization algorithm convex functions local minimums global minimum turns machine learning objective functions designed convex ex ample   discussion  far onedimensional func tion able visualize ideas gradients descent direc tions optimal values rest  develop ideas high dimensions unfortunately visualize con cepts dimension concepts generalize directly higher dimensions care needs taken reading according abelrufﬁni theorem general algebraic solution polynomials degree  abel  convex functions local minima global minimum  optimization gradient descent consider problem solving minimum realvalued function min       deisenroth   faisal   ong published cambridge university press  valueofparameterobjectivexxxx use convention row vectors gradients continuous optimization  objective function captures machine   rd learning problem hand assume function  differentiable unable analytically ﬁnd solution closed form gradient descent ﬁrstorder optimization algorithm ﬁnd local minimum function gradient descent takes steps propor tional negative gradient function current point recall section  gradient points direction steepest ascent useful intuition consider set lines function certain value     value  known contour lines gradient points direction orthogonal contour lines function wish optimize let consider multivariate functions imagine surface described function   ball starting particular location  ball released downhill direction steepest de scent gradient descent exploits fact   decreases fastest  xcid moves  direction negative gradient   assume book functions differentiable refer reader general settings section  then     xcid small stepsize  cid    cid   note use transpose gradient dimensions work out observation allows deﬁne simple gradient descent algo rithm want ﬁnd local optimum   function   right   optimize iterate according   start initial guess  parameters wish cid xi  xi γi  xicid  suitable stepsize γi sequence   cid   cid    converges local minimum example  consider quadratic function dimensions cidcid cidcidx cidx cidcid cid   cid cid cidx cid cidcid cidx cid gradient cidcid cidcidx cidx cidcid cid cid   cidcid cid cid iteratively apply  starting initial location    obtain sequence estimates converge minimum value draft  mathematics machine learning feedback smmlbookcom figure  gradient descent twodimensional quadratic surface shown heatmap example  description stepsize called learning rate  optimization gradient descent illustrated figure  both ﬁgure plug ging      negative gradient  points  cid repeating argument north east leading    gives    cid on remark gradient descent relatively slow close minimum asymptotic rate convergence inferior methods us ing ball rolling hill analogy surface long valley problem poorly conditioned trefethen bau iii  poorly conditioned convex problems gradient descent increasingly zigzags gradients point nearly orthogonally shortest di rection minimum point figure   stepsize mentioned earlier choosing good stepsize important gradient descent stepsize small gradient descent slow stepsize chosen large gradient descent overshoot fail con verge diverge discuss use momentum section method smoothes erratic behavior gradient up dates dampens oscillations adaptive gradient methods rescale stepsize iteration de pending local properties function simple heuris tics toussaint  function value increases gradient step stepsize large undo step decrease stepsize function value decreases step larger try increase stepsize    deisenroth   faisal   ong published cambridge university press  xx  continuous optimization undo step waste resources heuristic guarantees monotonic convergence example  solving linear equation system solve linear equations form ax   practice solve    approximately ﬁnding  minimizes squared error ax use euclidean norm gradient  respect  ax cid cid   ax bcidax   ax bcida  use gradient directly gradient descent algorithm how ever particular special case turns analytic solution setting gradient zero solving squared error problems   remark applied solution linear systems equations ax   gradient descent converge slowly speed convergence gra dient descent dependent condition number   σamax σamin ratio maximum minimum singular value section   condition number essentially measures ratio curved direction versus curved direction corresponds imagery poorly conditioned problems long valleys curved direction ﬂat other instead di rectly solving ax   instead solve  ax     called preconditioner goal design     better condition number time   easy com pute information gradient descent preconditioning convergence refer boyd vandenberghe     gradient descent momentum illustrated figure  convergence gradient descent slow curvature optimization surface regions poorly scaled curvature gradient descent steps hops walls valley approaches optimum small steps proposed tweak improve convergence gradient descent memory gradient descent momentum rumelhart et al  method introduces additional term remember happened previous iteration memory dampens oscillations smoothes gradient updates continuing ball analogy momentum term emulates phenomenon heavy ball reluctant change di rections idea gradient update memory implement draft  mathematics machine learning feedback smmlbookcom condition number preconditioner goh  wrote intuitive blog post gradient descent momentum  optimization gradient descent moving average momentumbased method remembers update xi iteration determines update linear combi nation current previous gradients xi  xi xi  xi γi xi  αxi  xicid  αxi γi  xicid    know gradient approxi mately cases momentum term useful averages different noisy estimates gradient particularly useful way obtain approximate gradient stochastic approximation discuss next  stochastic gradient descent computing gradient time consuming however possible ﬁnd cheap approximation gradient approximating gradient useful long points roughly direction true gradient stochastic gradient descent often shortened sgd stochastic ap proximation gradient descent method minimizing objective function written sum differentiable functions word stochastic refers fact acknowledge know gradient precisely instead know noisy approxima tion it constraining probability distribution approximate gradients theoretically guarantee sgd converge machine learning given         data points consider objective functions sum losses ln incurred example  mathematical notation form lθ  lnθ  cid  vector parameters interest ie want ﬁnd  minimizes  example regression   negative log likelihood expressed sum loglikelihoods individual examples lθ  cid log pyn xn   xn parameters regression model rd training inputs yn training targets  standard gradient descent introduced previously batch opti mization method ie optimization performed training set    deisenroth   faisal   ong published cambridge university press  stochastic gradient descent continuous optimization updating vector parameters according θi  θi γi lθicid  θi γi cid lnθicid suitable stepsize parameter γi evaluating sum gradient re quire expensive evaluations gradients individual functions ln training set enormous andor simple formulas exist evaluating sums gradients expensive consider term cidn lnθi  reduce computation taking sum smaller set ln contrast batch gradient descent uses ln          randomly choose subset ln minibatch gradient descent extreme case randomly select single ln estimate gradient key insight taking subset data sensible realize gradient descent converge require gradient unbiased estimate true gradient fact term cidn lnθi  empirical estimate expected value section  gradient therefore unbiased empirical estimate ex pected value example subsample data sufﬁce convergence gradient descent remark learning rate decreases appropriate rate sub ject relatively mild assumptions stochastic gradient descent converges surely local minimum bottou  consider approximate gradient major rea son practical implementation constraints size central processing unit cpugraphics processing unit gpu memory limits computational time think size subset esti mate gradient way thought size sample estimating empirical means section  large minibatch sizes provide accurate estimates gradient reducing variance parameter update furthermore large minibatches advantage highly optimized matrix operations vectorized implementations cost gradient reduction variance leads stable conver gence gradient calculation expensive contrast small minibatches quick estimate minibatch size small noise gradient estimate allow bad local optima stuck in machine learning optimization methods training min imizing objective function training data overall goal improve generalization performance   goal machine learning necessarily need precise estimate min imum objective function approximate gradients minibatch approaches widely used stochastic gradient descent effective largescale machine learning problems bottou et al  draft  mathematics machine learning feedback smmlbookcom  constrained optimization lagrange multipliers figure  illustration constrained optimization unconstrained problem indicated contour lines minimum right indicated circle box constraints  cid  cid   cid  cid  require optimal solution box resulting optimal value indicated star training deep neural networks millions images dean et al  topic models hoffman et al  reinforcement learning mnih et al  training largescale gaussian process models hensman et al  gal et al   constrained optimization lagrange multipliers previous section considered problem solving min imum function min      rd section additional constraints is realvalued         consider constrained functions gi  rd optimization problem see figure  illustration   min subject gix cid          worth pointing functions  gi nonconvex general consider convex case section obvious practical way converting constrained problem  unconstrained use indicator function jx     gix  cid    deisenroth   faisal   ong published cambridge university press  xx lagrange multiplier lagrangian continuous optimization  inﬁnite step function   cid  cid  gives inﬁnite penalty constraint satisﬁed provide solution however inﬁnite step function equally difﬁcult optimize overcome difﬁculty introduc ing lagrange multipliers idea lagrange multipliers replace step function linear function associate problem  lagrangian introducing la grange multipliers λi cid  corresponding inequality constraint re spectively boyd vandenberghe    lx      λigix cid     λcidgx  line concatenated constraints gix vector gx lagrange multipliers vector  rm introduce idea lagrangian duality general duality optimization idea converting optimization problem set variables  called primal variables optimization problem different set variables  called dual variables introduce different approaches duality section discuss lagrangian duality section  discuss legendrefenchel duality deﬁnition  problem    min subject gix cid         primal problem lagrangian dual problem known primal problem corresponding primal variables  associated lagrangian dual problem given max λrm dλ subject  cid    dual variables dλ  minxrd lx  remark discussion deﬁnition  use concepts independent boyd vandenberghe  minimax inequality minimax inequality says function arguments ϕx  maximin minimax ie max min ϕx  cid min max ϕx   draft  mathematics machine learning feedback smmlbookcom  constrained optimization lagrange multipliers inequality proved considering inequality   min ϕx  cid max ϕx   note taking maximum  lefthand  main tains inequality inequality true  similarly minimum  righthand  obtain  second concept weak duality uses  weak duality primal values greater equal dual values de scribed  recall difference jx  lagrangian  relaxed indicator function linear func tion therefore  cid  lagrangian lx  lower bound jx hence maximum lx  respect  recall original problem minimizing jx jx  max λcid lx   min xrd max λcid lx   minimax inequality  follows swapping order minimum maximum results smaller value ie min xrd max λcid lx  cid max λcid min xrd lx   known weak duality note inner right weak duality hand dual objective function dλ deﬁnition follows contrast original optimization problem constraints minxrd lx  unconstrained optimization problem given value  solving minxrd lx  easy overall problem easy solve observing  lx  afﬁne respect  minxrd lx  pointwise min imum afﬁne functions  dλ concave    nonconvex outer problem maximization  maximum concave function efﬁciently computed  differentiable ﬁnd lagrange dual problem differentiating lagrangian respect  setting differential zero solving optimal value discuss concrete examples sections     convex assuming    gi  gi  gi remark equality constraints consider  additional equality constraints   min subject gix cid  hjx                      deisenroth   faisal   ong published cambridge university press  convex set deﬁnition  set   cid  cid  convex set   scalar   continuous optimization model equality constraints replacing inequality constraints equality constraint hjx   equivalently replace constraints hjx cid  hjx cid  turns resulting lagrange multipliers unconstrained therefore constrain lagrange multipliers corresponding inequality constraints  nonnegative leave la grange multipliers corresponding equality constraints unconstrained  convex optimization focus attention particularly useful class optimization prob lems guarantee global optimality    convex function constraints involving     convex sets called convex optimization problem setting strong duality optimal solution dual problem opti mal solution primal problem distinction convex func tions convex sets strictly presented machine learning literature infer implied meaning context θx   θy   convex sets sets straight line connecting ele ments set lie inside set figures   illustrate convex nonconvex sets respectively convex functions functions straight line points function lie function figure  shows non convex function figure  shows convex function convex function shown figure  deﬁnition  let function   rd  function domain convex set function  convex function   domain   scalar   cid  cid   θx   θy cid θf    θf   remark concave function negative convex function constraints involving      truncate functions scalar value resulting sets relation convex functions convex sets consider set obtained ﬁlling in convex function convex function bowllike object imagine pouring water ﬁll up resulting ﬁlledin set called epigraph convex function convex set function   right  differentiable speci convexity draft  mathematics machine learning feedback smmlbookcom convex optimization problem strong duality figure  example convex set figure  example nonconvex set convex function concave function epigraph  convex optimization figure  example convex function xf  section  function   convex terms gradient points   holds xf xcidy   cid    know function   twice differentiable is hessian  exists values domain  function xf  positive semideﬁnite boyd   convex vandenberghe    example  negative entropy     log  convex    visualization function shown figure  function convex illustrate previous deﬁnitions convexity let check calculations points       note prove convexity   need check points  recall deﬁnition  consider point midway points that    lefthand    righthand  log    log        deﬁnition satisﬁed    log       differentiable alternatively use  calculating derivative   obtain xx log    log     log    loge  loge  test points       lefthand  given     righthand    cid               loge       deisenroth   faisal   ong published cambridge university press  xyyxx  continuous optimization figure  negative entropy function which convex tangent    check function set convex ﬁrst principles recalling deﬁnitions practice rely operations pre serve convexity check particular function set convex al details vastly different idea closure introduced   vector spaces example  nonnegative weighted sum convex functions convex observe  convex function  cid  nonnegative scalar function αf convex multiplying  sides equation deﬁnition  recalling multiplying nonnegative number change inequality   convex functions deﬁnition fθx   fθx   θy cid θ   θy cid θ   θ θ  summing sides gives fθx   cid θ   θy  fθx   θy θ  θ   righthand rearranged θ  θ     θ    completing proof sum convex functions convex combining preceding facts α  β convex   cid  closure property extended sim ilar argument nonnegative weighted sums convex functions draft  mathematics machine learning feedback smmlbookcom xxlogxtangentatx  convex optimization remark inequality  called jensens inequality fact class inequalities taking nonnegative weighted sums convex functions called jensens inequality summary constrained optimization problem called convex opti jensens inequality convex optimization problem mization problem min   subject gix cid  hjx                   functions   gix convex functions hjx   convex sets following classes convex optimization problems widely understood  linear programming consider special case preceding functions linear ie min xrd ccidx subject ax cid   variables  linear constraints lagrangian given rm known linear program  rmd  lx   ccidx  λcidax   ranging terms corresponding  yields rm vector nonnegative lagrange multipliers rear lx     acidλcidx λcidb  taking derivative lx  respect  setting zero gives   acidλ    λcidb recall like therefore dual lagrangian dλ  maximize dλ addition constraint derivative lx  zero fact  cid  resulting following dual optimization problem linear program linear programs widely approaches industry max λrm  bcidλ subject   acidλ    cid   convention minimize primal maximize dual linear program  variables choice solving primal  dual  program depending    deisenroth   faisal   ong published cambridge university press  continuous optimization   larger recall  number variables  number constraints primal linear program example  linear program consider linear program min xr  cid cid cidcid cidx cid cidx cid subject variables program shown figure  objective function linear resulting linear contour lines constraint set standard form translated legend optimal value lie shaded feasible region indicated star figure  illustration linear program unconstrained problem indicated contour lines minimum right side optimal value given constraints shown star draft  mathematics machine learning feedback smmlbookcom xxxxxxxxxx  convex optimization  quadratic programming consider case convex quadratic objective function con straints afﬁne ie min xrd xcidqx  ccidx subject ax cid   rmd  rm  rdd positive deﬁnite objective function convex known quadratic program observe  variables  linear constraints rd square symmetric matrix  example  quadratic program consider quadratic program cidx subject min xr cidcid cid    cid cidx cid cid cid cidx cid cid cidcid cidx variables program illustrated figure  objec tive function quadratic positive semideﬁnite matrix  resulting elliptical contour lines optimal value lie shaded feasi ble region indicated star lagrangian given lx   xcidqx  ccidx  λcidax xcidqx    acidλcidx λcidb  rearranged terms taking derivative lx  respect  setting zero gives assuming  invertible qx    acidλ      qc  acidλ  substituting  primal lagrangian lx  dual lagrangian dλ    acidλcidqc  acidλ λcidb     deisenroth   faisal   ong published cambridge university press  continuous optimization therefore dual optimization problem given max λrm  subject  cid     acidλcidqc  acidλ λcidb application quadratic programming machine learning    legendrefenchel transform convex conjugate let revisit idea duality section  considering constraints useful fact convex set equiva lently described supporting hyperplanes hyperplane called supporting hyperplane convex set intersects convex set convex set contained it recall ﬁll convex function obtain epigraph convex set therefore convex functions terms supporting hyper planes furthermore observe supporting hyperplane touches convex function fact tangent function point recall tangent function   given point  evaluation gradient function point df  dx summary convex sets equivalently described sup porting hyperplanes convex functions equivalently described function gradient legendre transform formalizes concept cid cid cidxx begin general deﬁnition unfortunately counterintuitive form look special cases relate deﬁnition intuition described preceding paragraph legendrefenchel transform transformation in sense fourier transform convex differentiable function   function depends xf  worth stressing transformation tangents sx  function    variable  function evaluated  legendrefenchel transform known convex conjugate for reasons soon closely related duality hiriarturruty lemarechal    supporting hyperplane legendre transform physics students introduced legendre transform relating lagrangian hamiltonian classical mechanics legendrefenchel transform convex conjugate convex conjugate deﬁnition  convex conjugate function   rd function   deﬁned    sup xrd   cid cid     note preceding convex conjugate deﬁnition need function  convex differentiable deﬁnition  general inner product section  rest section draft  mathematics machine learning feedback smmlbookcom derivation easiest understand drawing reasoning progresses classical legendre transform deﬁned convex differentiable functions rd  convex optimization cid  scidx avoid technical details consider standard dot product ﬁnitedimensional vectors cid   understand deﬁnition  geometric fashion consider nice simple onedimensional convex differentiable function example     note looking onedimensional problem hyperplanes reduce line consider line   sxc recall able convex functions supporting hyperplanes let try function   supporting lines fix gradi  point    graph   ﬁnd ent line  minimum value  line intersects    note minimum value  place line slope  just touches function     line passing    gradient  given    sx   yintercept line   sx   intersects graph  sx    minimum  sx     inf   preceding convex conjugate convention deﬁned nega tive this reasoning paragraph rely fact chose onedimensional convex differentiable function holds  nonconvex nondifferentiable   rd remark convex differentiable functions example     nice special case need supremum onetoone correspondence function legendre trans form let derive ﬁrst principles convex differentiable function know  tangent touches      sx    recall want convex function   terms xf  rearrange expres gradient sion xf     obtain note think function   changes     sx       sx    comparing  deﬁnition   special case without supremum conjugate function nice properties example convex functions applying legendre transform gets orig inal function way slope    slope      deisenroth   faisal   ong published cambridge university press  continuous optimization  following examples common uses convex conjugates machine learning example  convex conjugates illustrate application convex conjugates consider quadratic function    ycidk  based positive deﬁnite matrix  variable  right dual variable  applying deﬁnition  obtain function right now right nown denote primal    sup   yright cid cid  ycidk   function differentiable ﬁnd maximum taking derivative respect  setting zero  cid   cid  ycidk ycid gradient zero     yields   cid  λk ycid  kα substituting    αcidkα kα   kα αcidkα  cid  cidcid cid  cid example  machine learning use sums functions example ob jective function training set includes sum losses ex ample training set following derive convex conjugate  illustrates appli sum losses cidt cid    ciditi cation convex conjugate vector case let then   cidn   sup   tright cid cid  cid ciditi cid ziti ciditi ziti sup tright ciditi  sup tright cid deﬁnition dot product draft  mathematics machine learning feedback smmlbookcom  convex optimization cid cid zi  deﬁnition conjugate recall section  derived dual optimization problem lagrange multipliers furthermore convex optimization problems strong duality solutions primal dual problem match legendrefenchel transform described derive dual optimization problem furthermore function convex differentiable supremum unique investi gate relation approaches let consider linear equality constrained convex optimization problem example  let   gx convex functions real matrix appropriate dimensions ax   min  ax  gx  min axy    gx introducing lagrange multiplier constraints ax   min axy    gx  min xy max    gx  ax ycidyou  max min xy    gx  ax ycidyou  step swapping max min fact   gx convex functions splitting dot product term collecting   recall convex conjugate deﬁnition  fact dot prod cid cid max min xy cid  max min    max min      gx  ax ycidyou cid ycidyou    axcidyou  gx cid cid ycidyou    cid cid xcidacidyou  gx cid min min ucts symmetric max min    you  max acidyou  therefore shown cid ycidyou    cid min cid xcidacidyou  gx min  ax  gx  max  you acidyou     deisenroth   faisal   ong published cambridge university press  general inner products acid replaced adjoint  continuous optimization legendrefenchel conjugate turns useful ma chine learning problems expressed convex optimization problems particular convex loss functions apply independently example conjugate loss convenient way derive dual problem  reading continuous optimization active area research try provide comprehensive account recent advances gradient descent perspective major weaknesses set literature ﬁrst challenge fact gradient descent ﬁrstorder algorithm use infor mation curvature surface long valleys gradient points perpendicularly direction interest idea momentum generalized general class acceleration meth ods nesterov  conjugate gradient methods avoid issues faced gradient descent taking previous directions account shewchuk  secondorder methods newton methods use hessian provide information curvature choices choos ing stepsizes ideas like momentum arise considering curvature objective function goh  bottou et al  quasinewton methods lbfgs try use cheaper computational methods ap proximate hessian nocedal wright  recently metrics computing descent directions result ing approaches mirror descent beck teboulle  natural gradient toussaint  second challenge handle nondifferentiable functions gradi ent methods deﬁned kinks function cases subgradient methods shor  fur ther information algorithms optimizing nondifferentiable func tions refer book bertsekas  vast literature different approaches numerically solving continuous optimization problems including algorithms constrained optimization problems good starting points appreciate literature books luenberger  bonnans et al  recent survey con tinuous optimization provided bubeck  modern applications machine learning mean size datasets prohibit use batch gradient descent stochastic gradient descent current workhorse largescale machine learning methods recent surveys literature include hazan  bot tou et al  duality convex optimization book boyd vanden berghe  includes lectures slides online mathematical treatment provided bertsekas  recent book draft  mathematics machine learning feedback smmlbookcom hugo goncalves blog good resource easier introduction legendrefenchel transforms stinyurl comydaalhj exercises key researchers area optimization nesterov  con vex optimization based convex analysis reader interested foundational results convex functions referred rock afellar  hiriarturruty lemarechal  borwein lewis  legendrefenchel transforms covered afore mentioned books convex analysis beginnerfriendly pre sentation available zia et al  role legendrefenchel transforms analysis convex optimization algorithms surveyed polyak   consider univariate function exercises           stationary points indicate maximum mini mum saddle points  consider update equation stochastic gradient descent equation  write update use minibatch size one  consider following statements true false  intersection convex sets convex  union convex sets convex  difference convex set convex set  convex  consider following statements true false  sum convex functions convex  difference convex functions convex  product convex functions convex  maximum convex functions convex  express following optimization problem standard linear program matrix notation max xr ξr pcidx   subject constraints  cid   cid   cid   consider linear program illustrated figure  min xr cid cid cidcid cidx     cid cidx cid subject derive dual linear program lagrange duality    deisenroth   faisal   ong published cambridge university press  continuous optimization  consider quadratic program illustrated figure  cidcid cidx cid cid cidx cidx min xr cid cid cidcid cid   cid cidx cid subject derive dual quadratic program lagrange duality  consider following convex optimization problem min wrd wcidw subject wcidx cid   cid    xd log xd  derive lagrangian dual introducing lagrange multiplier   consider negative entropy   rd derive convex conjugate function   assuming standard dot product hint gradient appropriate function set gradient zero  consider function    xcidax  bcidx    strictly positive deﬁnite means invertible derive convex conjugate   hint gradient appropriate function set gradient zero  hinge loss which loss support vector machine given lα  max     interested applying gradient methods lbfgs want resort subgradient methods need smooth kink hinge loss compute convex conjugate hinge loss lβ  dual variable add cid proximal term compute conjugate resulting function  given hyperparameter lβ    draft  mathematics machine learning feedback smmlbookcom ii central machine learning problems material published cambridge university press mathematics machine learning marc peter deisenroth  aldo faisal cheng soon ong  version free view download personal use only redistribution resale use derivative works by   deisenroth   faisal   ong  smmlbookcom models meet data ﬁrst book introduced mathematics form foundations machine learning methods hope reader able learn rudimentary forms language mathematics ﬁrst part use discuss machine learning second book introduces pillars machine learning regression   dimensionality reduction   density estimation   classiﬁcation   main aim book illustrate mathematical concepts introduced ﬁrst book design machine learning algorithms solve tasks remit pillars intend introduce advanced machine learning concepts instead provide set practical methods allow reader apply knowledge gained ﬁrst book provides gateway wider machine learning literature readers familiar mathematics  data models learning worth point pause consider problem ma chine learning algorithm designed solve discussed   major components machine learning system data models learning main question machine learning what mean good models word model subtleties model revisit multiple times  entirely obvious objectively deﬁne word good guiding principles machine learning good models perform unseen data requires deﬁne performance metrics accu racy distance ground truth ﬁguring ways performance metrics  covers necessary bits pieces mathematical statistical language commonly material published cambridge university press mathematics machine learning marc peter deisenroth  aldo faisal cheng soon ong  version free view download personal use only redistribution resale use derivative works by   deisenroth   faisal   ong  smmlbookcom models meet data table  example data ﬁctitious human resource database numerical format aditya bob chloe daisuke elisabeth gender degree msc phd becon bsc mba postcode wbg ecaba swabh seat seaa age annual salary data assumed tidy format wickham  codd  talk machine learning models so brieﬂy out line current best practices training model resulting predictor data seen mentioned   different senses use phrase machine learning algorithm training prediction ideas  idea selecting different models introduce framework empirical risk minimization section  principle maximum likelihood section  idea probabilistic models section  brieﬂy outline graphical language speciing probabilistic models sec tion  ﬁnally discuss model selection section  rest section expands main components machine learning data models learning  data vectors assume data read computer represented ade quately numerical format data assumed tabular figure  think row table representing particular in stance example column particular feature recent years machine learning applied types data obviously come tabular numerical format example genomic se quences text image contents web social media graphs discuss important challenging aspects identiing good features aspects depend domain expertise re quire careful engineering and recent years umbrella data science stray  adhikari denero  data tabular format choices obtain numerical representation example table  gender column  categorical variable converted numbers  representing male  representing female alternatively gen   respectively as shown der represented numbers table  furthermore important use domain knowledge constructing representation knowing university degrees progress bachelors masters phd realizing postcode provided string characters actually encodes area london table  converted data table  numerical format postcode represented numbers draft  mathematics machine learning feedback smmlbookcom  data models learning gender id degree latitude in degrees longitude in degrees age annual salary in thousands table  example data ﬁctitious human resource database see table  converted numerical format latitude longitude numerical data potentially directly read machine learning algorithm carefully con sidered units scaling constraints additional information shift scale columns dataset empirical mean  empirical variance  purposes book assume domain expert converted data ap propriately ie input xn ddimensional vector real numbers called features attributes covariates consider dataset form illustrated table  observe dropped column table  new numerical representation main reasons desirable  expect iden tiﬁer the name informative machine learning task  wish anonymize data help protect privacy employees book use  denote number exam ples dataset index examples lowercase          assume given set numerical data represented array vectors table  row particular individual xn referred example data point machine learning subscript  refers fact nth example total  exam ples dataset column represents particular feature example index features         recall data represented vectors means example each data point ddimensional vector orientation table originates database community machine learning algorithms eg   convenient represent examples col umn vectors let consider problem predicting annual salary age based data table  called supervised learning problem label yn the salary associated example xn the age label yn names including target re sponse variable annotation dataset written set example       xn yn     xn  yn   table examples label pairs right  fig      xn concatenated written  ure  illustrates dataset consisting rightmost columns table    age   salary use concepts introduced ﬁrst book formalize    deisenroth   faisal   ong published cambridge university press  feature attribute covariate example data point label figure  toy data linear regression training data xn yn pairs rightmost columns table  interested salary person aged    illustrated vertical dashed red line training data feature map kernel models meet data machine learning problems previous paragraph representing data vectors xn allows use concepts linear al gebra introduced   machine learning algorithms need additionally able compare vectors    computing similarity distance ex amples allows formalize intuition examples similar fea tures similar labels comparison vectors requires construct geometry explained   allows optimize resulting learning problem techniques   vector representations data manipulate data ﬁnd potentially better representations it discuss ﬁnding good representations ways ﬁnding lowerdimensional approximations original feature vector nonlinear higherdimensional combinations original feature vector   example ﬁnding lowdimensional approximation original data space ﬁnding principal components finding principal components closely related concepts eigenvalue singular value decomposi tion introduced   highdimensional representation explicit feature map   allows represent in puts xn higherdimensional representation φxn main mo tivation higherdimensional representations construct new features nonlinear combinations original features turn learning problem easier discuss feature map section  feature map leads kernel section  recent years deep learning methods goodfellow et al  shown promise data learn new good fea tures successful areas vision speech recognition natural language processing cover neural networks book reader referred draft  mathematics machine learning feedback smmlbookcom xy figure  example function black solid diagonal line prediction    ie     predictor  data models learning section  mathematical description backpropagation key concept training neural networks  models functions data appropriate vector representation business constructing predictive function known predictor   language precise models concepts ﬁrst book introduce model means present major approaches book predictor function predictor probabilistic model de scribe subsection predictor function that given particular input example in case vector features produces output now consider output single number ie realvalued scalar output written   rd input vector  ddimensional has  features func tion  applied written   returns real number fig ure  illustrates possible function compute value prediction input values  book consider general case functions involve need functional analysis instead consider special case linear functions    θcidx   unknown   restriction means contents chap ters   sufﬁce precisely stating notion predictor nonprobabilistic in contrast probabilistic view described next    deisenroth   faisal   ong published cambridge university press  xy  models meet data figure  example function black solid diagonal line predictive uncertainty    drawn gaussian view machine learning linear functions strike good balance generality problems solved back ground mathematics needed  models probability distributions consider data noisy observations true underlying effect hope applying machine learning identi signal noise requires language quanti ing effect noise like predictors express sort uncertainty eg quanti conﬁdence value prediction particular test data point seen   probability theory provides language quan tiing uncertainty figure  illustrates predictive uncertainty function gaussian distribution instead considering predictor single function con sider predictors probabilistic models ie models describing dis tribution possible functions limit book spe cial case distributions ﬁnitedimensional parameters allows probabilistic models needing stochastic processes random measures special case think prob abilistic models multivariate probability distributions allow rich class models introduce use concepts probability   deﬁne machine learning models section  introduce graphical language describing probabilistic models compact way sec tion  draft  mathematics machine learning feedback smmlbookcom xy  data models learning  learning finding parameters goal learning ﬁnd model corresponding parame ters resulting predictor perform unseen data conceptually distinct algorithmic phases discussing machine learning algorithms  prediction inference  training parameter estimation  hyperparameter tuning model selection prediction phase use trained predictor previously un seen test data words parameters model choice ﬁxed predictor applied new vectors representing new input data points outlined   previous subsection consider schools machine learning book corresponding predictor function probabilistic model probabilistic model discussed section  predic tion phase called inference remark unfortunately agreed naming different algorithmic phases word inference mean parameter estimation probabilistic model mean prediction nonprobabilistic models training parameter estimation phase adjust pre dictive model based training data like ﬁnd good predic tors given training data main strategies so ﬁnding best predictor based measure quality sometimes called ﬁnding point estimate bayesian inference finding point estimate applied types predictors bayesian inference requires probabilistic models nonprobabilistic model follow principle empirical risk minimization section  empirical risk minimiza tion directly provides optimization problem ﬁnding good parame ters statistical model principle maximum likelihood maximum likelihood ﬁnd good set parameters section  additionally model uncertainty parameters probabilistic model look section  empirical risk minimization use numerical methods ﬁnd good parameters ﬁt data training methods thought hillclimbing approaches ﬁnd maximum objective example maximum likeli hood apply hillclimbing approaches use gradients described   implement numerical optimization approaches chap ter  mentioned   interested learning model based data performs future data    deisenroth   faisal   ong published cambridge university press  convention optimization minimize objectives hence extra minus sign machine learning objectives crossvalidation abduction good movie title ai abduction hyperparameter model selection nested crossvalidation models meet data model ﬁt training data well predictor needs per form unseen data simulate behavior predictor future unseen data crossvalidation section   achieve goal performing unseen data need balance ﬁtting training data ﬁnding simple explanations phenomenon tradeoff achieved us ing regularization section  adding prior section  philosophy considered induction deduction called abduction according stanford encyclopedia philosophy abduction process inference best explanation douven need highlevel modeling decisions struc ture predictor number components use class probability distributions consider choice number components example hyperparameter choice af fect performance model signiﬁcantly problem choosing different models called model selection section  nonprobabilistic models model selection nested crossvalidation described section  use model selection choose hyperparameters model remark distinction parameters hyperparameters some arbitrary driven distinction numerically optimized versus needs use search techniques way consider distinction consider parameters explicit parameters probabilistic model consider hyperparam eters higherlevel parameters parameters control distribution explicit parameters following sections look ﬂavors machine learn ing empirical risk minimization section  principle maximum likelihood section  probabilistic modeling section   empirical risk minimization having mathematics belt posi tion introduce means learn learning machine learning boils estimating parameters based training data section consider case predictor function consider case probabilistic models section  idea empirical risk minimization originally popularized proposal support vector machine described   however general principles widely applicable allow ask question learning explicitly constructing probabilis tic models main design choices cover following subsections draft  mathematics machine learning feedback smmlbookcom  empirical risk minimization section  set functions allow predictor take section  measure predictor performs training data section  construct predictors training data performs unseen test data section  procedure searching space mod els  hypothesis class functions rd corresponding scalar la assume given  examples xn  consider supervised learning setting obtain bels yn pairs       xn  yn  given data like estimate  parametrized  hope able ﬁnd predictor   good parameter  ﬁt data well is    rd  xn  yn          section use notation ˆyn   xn  represent output predictor remark ease presentation empirical risk mini mization terms supervised learning where labels simpliﬁes deﬁnition hypothesis class loss function common machine learning choose parametrized class functions example afﬁne functions example  introduce problem ordinary leastsquares regression illustrate empirical risk minimization comprehensive account regression given   label yn realvalued popular choice function class predictors set afﬁne functions choose compact notation afﬁne function concatenating addi  cid tional unit feature    xn ie xn    parameter vector correspondingly          θdcid allowing write predictor linear function       xd    linear predictor equivalent afﬁne model  xn   θcidxn   xn     θdxd   cid predictor takes vector features representing single example xn input produces realvalued output ie   rd    deisenroth   faisal   ong published cambridge university press  afﬁne functions referred linear functions machine learning models meet data previous ﬁgures  straight line predictor means assumed afﬁne function instead linear function wish consider nonlinear func tions predictors recent advances neural networks allow efﬁcient computation complex nonlinear function classes given class functions want search good predictor second ingredient empirical risk minimization measure predictor ﬁts training data  loss function training consider label yn particular example corresponding pre diction ˆyn based xn deﬁne means ﬁt data well need speci loss function cidyn ˆyn takes ground truth label prediction input produces nonnegative num ber referred loss representing error particular prediction goal ﬁnding good parameter vector  minimize average loss set  training examples assumption commonly machine learning set examples       xn  yn  independent identically distributed word independent section  means data points xi yi xj yj statistically depend other mean ing empirical mean good estimate population mean section  implies use empirical mean       xn  yn  loss training data given training set introduce notation example matrix        xn cid right  label vector        yn cid right  matrix notation average loss given rempf    cidyn ˆyn  cid loss function expression error mean loss independent identically distributed training set empirical risk empirical risk minimization ˆyn   xn  equation  called empirical risk de pends arguments predictor  data   general strategy learning called empirical risk minimization example  leastsquares loss continuing example leastsquares regression speci measure cost making error training squared ˆyn wish minimize empirical risk  loss cidyn ˆyn  yn draft  mathematics machine learning feedback smmlbookcom  empirical risk minimization average losses data min θrd cid yn  xn  substituted predictor ˆyn   xn  choice linear predictor  xn   θcidxn obtain optimization problem equation equivalently expressed matrix form min θrd cid yn θcidxn  min θrd  cid xθ   cid known leastsquares problem exists closedform an alytic solution solving normal equations discuss section  leastsquares problem interested predictor performs training data instead seek predictor performs has low risk unseen test data formally interested ﬁnding predictor  with parameters ﬁxed minimizes expected risk expected risk rtruef   exycidy     label   prediction based example  notation rtruef  indicates true risk access inﬁnite data expectation inﬁnite set possible data labels practical questions arise desire minimize expected risk address following subsections phrase commonly expected risk population risk change training procedure generalize well estimate expected risk ﬁnite data remark machine learning tasks speciﬁed associated performance measure eg accuracy prediction root mean squared error performance measure complex cost sensitive capture details particular application principle de sign loss function empirical risk minimization correspond directly performance measure speciﬁed machine learning task practice mismatch design loss function performance measure issues ease implementation efﬁciency optimization    deisenroth   faisal   ong published cambridge university press  test set knowing performance predictor test set leaks information blum hardt  overﬁtting regularization models meet data  regularization reduce overﬁtting section describes addition empirical risk minimization al lows generalize approximately minimizing expected risk re aim training machine learning predictor perform unseen data ie predictor generalizes well sim ulate unseen data holding proportion dataset hold set referred test set given sufﬁciently rich class functions predictor   essentially memorize training data obtain zero empirical risk great minimize loss and risk training data expect predictor generalize unseen data practice ﬁnite set data split data training test set training set ﬁt model test set not seen machine learning algorithm training evaluate generalization performance important user cycle new round training having observed test set use subscripts train test denote training test sets respectively revisit idea ﬁnite dataset evaluate expected risk section  turns empirical risk minimization lead overﬁtting ie predictor ﬁts closely training data general ize new data mitchell  general phenomenon hav ing small average loss training set large average loss test set tends occur little data complex hy pothesis class particular predictor  with parameters ﬁxed phenomenon overﬁtting occurs risk estimate train ing data rempf  train ytrain underestimates expected risk rtruef  estimate expected risk rtruef  empirical risk test set rempf  test ytest test risk larger training risk indication overﬁtting revisit idea overﬁtting section  therefore need bias search minimizer empirical risk introducing penalty term makes harder optimizer return overly ﬂexible predictor machine learning penalty term referred regularization regularization way compromise accurate solution empirical risk minimization size complexity solution example  regularized squares regularization approach discourages complex extreme solu tions optimization problem simplest regularization strategy draft  mathematics machine learning feedback smmlbookcom  empirical risk minimization replace leastsquares problem min  cid xθ   cid previous example regularized problem adding penalty term involving    xθ min    cid  cid cid  called regularizer parameter additional term  regularization parameter regularization parameter trades minimizing loss training set magnitude pa rameters  happens magnitude parameter values relatively large run overﬁtting bishop  cid cid cid regularization term called penalty term bi ases vector  closer origin idea regularization appears probabilistic models prior probability parameters recall section  posterior distribution form prior distribution prior likelihood need con jugate revisit idea section    idea regularizer equivalent idea large margin  crossvalidation assess generalization performance mentioned previous section measure generalization error estimating applying predictor test data data referred validation set validation set sub set available training data aside practical issue approach data limited ideally use data available train model require validation set small lead noisy estimate with high variance predictive performance solu tion contradictory objectives large training set large validation set use crossvalidation kfold crossvalidation effectively partitions data  chunks  chunk serves validation set similar idea outlined previously crossvalidation iterates ideally combinations assignments chunks  figure  procedure repeated  choices validation set performance model  runs averaged partition dataset sets validation set train model  training assess performance predictor   form training set overlap         deisenroth   faisal   ong published cambridge university press  regularizer regularization parameter penalty term validation set crossvalidation models meet data training validation figure  kfold crossvalidation dataset divided    chunks    serve training set blue validation set orange hatch validation set eg computing root mean square error rmse trained model validation set precisely partition  produces predictor   applied  training data  cycle validation set possible partitionings validation training sets com pute average generalization error predictor crossvalidation approximates expected generalization error  compute empirical risk rf  evrf rf    cid  risk eg rmse validation set rf  predictor   approximation sources ﬁrst ﬁnite training set results best possible   second ﬁnite validation set results inaccurate estimation risk rf   potential disadvantage kfold crossvalidation computational cost training model  times bur densome training cost computationally expensive practice sufﬁcient look direct parameters alone example need explore multiple complexity parameters eg multiple regu larization parameters direct parameters model evaluating quality model depending hyperparameters result number training runs exponential number model parameters use nested crossvalidation section  search good hyperparameters however crossvalidation embarrassingly parallel problem ie lit tle effort needed separate problem number parallel tasks given sufﬁcient computing resources eg cloud computing server farms crossvalidation require longer single performance assessment section saw empirical risk minimization based following concepts hypothesis class functions loss function regularization section  effect probability distribution replace idea loss functions regularization draft  mathematics machine learning feedback smmlbookcom embarrassingly parallel statistical learning theory tikhonov regularization  parameter estimation  reading fact original development empirical risk minimiza tion vapnik  couched heavily theoretical language subsequent developments theoretical area study called statistical learning theory vapnik  evgeniou et al  hastie et al  von luxburg scholkopf  recent machine learning textbook builds theoretical foundations develops efﬁcient learning algorithms shalevshwartz bendavid  concept regularization roots solution illposed in verse problems neumaier  approach presented called tikhonov regularization closely related constrained version called ivanov regularization tikhonov regularization deep relation ships biasvariance tradeoff feature selection buhlmann van geer  alternative crossvalidation bootstrap jackknife efron tibshirani  davidson hinkley  hall thinking empirical risk minimization section  probabil ity free incorrect underlying unknown probability distri bution px  governs data generation however approach empirical risk minimization agnostic choice distribution contrast standard statistical approaches explicitly re quire knowledge px  furthermore distribution joint distribution examples  labels  labels non deterministic contrast standard statistics need speci noise distribution labels   parameter estimation section  explicitly model problem probability distributions section use probability distribu tions model uncertainty observation process uncertainty parameters predictors section  in troduce likelihood analogous concept loss functions section  empirical risk minimization concept priors sec tion  analogous concept regularization section   maximum likelihood estimation idea maximum likelihood estimation mle deﬁne func maximum likelihood tion parameters enables ﬁnd model ﬁts data well estimation problem focused likelihood function precisely negative logarithm data represented random variable  family probability densities px  parametrized  negative loglikelihood given estimation likelihood negative loglikelihood    deisenroth   faisal   ong published cambridge university press  models meet data xθ  log px   xθ emphasizes fact parameter  varying notation data  ﬁxed drop reference  writing negative loglikelihood function  write  random variable representing uncertainty data clear context let interpret probability density px  modeling ﬁxed value  distribution models uncertainty data given parameter setting given dataset  likelihood allows express preferences different settings parameters  choose setting likely generated data complementary view consider data ﬁxed because observed vary parameters   tell us tells likely particular setting  observations  based second view maximum likelihood estimator gives likely parameter  set data rd labels yn consider supervised learning setting obtain pairs  inter       xn  yn  xn ested constructing predictor takes feature vector xn input produces prediction yn or close it ie given vec tor xn want probability distribution label yn words speci conditional probability distribution labels given examples particular parameter setting  example  ﬁrst example speci conditional probability labels given examples gaussian distribution words assume explain observation uncertainty independent gaussian noise refer section  zero mean εn   prediction means speci gaussian likelihood example label pair xn yn cid σcid assume linear model xcid   pyn xn   cidyn   σcid  xcid illustration gaussian likelihood given parameter  shown figure  section  explicitly expand preceding expression terms gaussian distribution independent identically distributed assume set examples       xn  yn  independent identically distributed iid word independent section  implies likelihood involving dataset   factorizes product likelihoods      yn      xn draft  mathematics machine learning feedback smmlbookcom recall logab  loga  logb  parameter estimation individual example       pyn xn   cid pyn xn  particular distribution which gaussian ex ample  expression identically distributed means term product  distribution share parameters easier optimization viewpoint compute functions decomposed sums simpler functions hence machine learning consider negative loglikelihood   log        log pyn xn   cid temping interpret fact  right condi xn   interpreted observed tioning pyn ﬁxed interpretation incorrect negative loglikelihood function  therefore ﬁnd good parameter vector  explains data       xn  yn  well minimize negative log likelihood  respect  remark negative sign  historical artifact convention want maximize likelihood numerical optimization literature tends study minimization functions example  continuing example gaussian likelihoods  negative loglikelihood rewritten   log pyn xn   log cidyn   σcid xcid cid cid cid cid log cid πσ cid exp yn log exp cid yn   cid xcid   cid cid log πσ yn xcid   log πσ xcid cid  given second term  constant minimizing corresponds solving leastsquares problem compare  expressed ﬁrst term turns gaussian likelihoods resulting optimization    deisenroth   faisal   ong published cambridge university press  models meet data figure  given data maximum likelihood estimate parameters results black diagonal line orange square shows value maximum likelihood prediction    figure  comparing predictions maximum likelihood estimate map estimate    prior biases slope steep intercept closer zero example bias moves intercept closer zero actually increases slope problem corresponding maximum likelihood estimation closed form solution details   figure  shows regression dataset function induced maxi mumlikelihood parameters maximum likelihood estimation suffer overﬁtting section  analogous unregularized empirical risk minimization section  likelihood functions ie model noise nongaussian distributions maximum likelihood es timation closedform analytic solution case resort numerical optimization methods discussed    maximum posteriori estimation prior knowledge distribution parameters  multiply additional term likelihood additional term prior probability distribution parameters pθ given prior draft  mathematics machine learning feedback smmlbookcom xyxymlemap  parameter estimation observing data  update distribution  words represent fact speciﬁc knowledge  observing data  bayes theorem discussed section  gives principled tool update probability distribu tions random variables allows compute posterior distribution pθ  the speciﬁc knowledge parameters  general prior statements prior distribution pθ function px links parameters  observed data  called likelihood posterior prior likelihood recall interested ﬁnding parameter  maximizes posterior distribution px depend  ignore value denominator optimization obtain pθ   px θpθ px pθ px θpθ  preceding proportion relation hides density data px difﬁcult estimate instead estimating minimum negative loglikelihood estimate minimum neg ative logposterior referred maximum posteriori estima maximum tion map estimation illustration effect adding zeromean gaussian prior shown figure  posteriori estimation map estimation example  addition assumption gaussian likelihood previous exam ple assume parameter vector distributed multivariate cid σcid  covari gaussian zero mean ie pθ  ance matrix section  note conjugate prior gaussian gaussian section  expect posterior distribution gaussian details maximum posteriori estimation   idea including prior knowledge good parameters lie widespread machine learning alternative view saw section  idea regularization introduces addi tional term biases resulting parameters close origin maximum posteriori estimation considered bridge non probabilistic probabilistic worlds explicitly acknowledges need prior distribution produces point estimate parameters remark maximum likelihood estimate θml possesses following properties lehmann casella  efron hastie  asymptotic consistency mle converges true value    deisenroth   faisal   ong published cambridge university press  models meet data mθ mθ mθ   figure  model ﬁtting parametrized class mθ models optimize model parameters  minimize distance true unknown model   limit inﬁnitely observations plus random error ap proximately normal size samples necessary achieve properties large errors variance decays    number data points especially small data regime maximum likelihood estimation lead overﬁtting principle maximum likelihood estimation and maximum pos teriori estimation uses probabilistic modeling reason uncer tainty data model parameters however taken probabilistic modeling extent section resulting train ing procedure produces point estimate predictor ie training returns single set parameter values represent best predic tor section  view parameter values treated random variables instead estimating best val ues distribution use parameter distribution making predictions  model fitting consider setting given dataset interested ﬁtting parametrized model data talk ﬁt ting typically mean optimizinglearning model parameters minimize loss function eg negative loglikelihood maximum likelihood section  maximum posteriori estima tion section  discussed commonly algorithms model ﬁtting parametrization model deﬁnes model class mθ operate example linear regression setting deﬁne relationship inputs  noisefree observations    ax     model parameters case model parameters  family afﬁne functions ie straight lines slope  offset   assume data comes   draft  mathematics machine learning feedback smmlbookcom  parameter estimation figure  fitting by maximum likelihood different model classes regression dataset overﬁtting way detect overﬁtting practice observe model low training risk high test risk cross validation section   overﬁtting  underﬁtting  fitting well model   unknown us given training dataset optimize  mθ close possible   close ness deﬁned objective function optimize eg squared loss training data figure  illustrates setting small model class indicated circle mθ data generation model   lies outside set considered models begin parameter search mθ optimization ie obtain best pos sible parameters  distinguish different cases  overﬁtting ii underﬁtting iii ﬁtting well highlevel intuition concepts mean roughly speaking overﬁtting refers situation para metrized model class rich model dataset generated   ie mθ model complicated datasets instance dataset generated linear function deﬁne mθ class seventhorder polynomials model linear func tions polynomials degree two three etc models over ﬁt typically large number parameters observation overly ﬂexible model class mθ uses modeling power reduce training error training data noisy ﬁnd useful signal noise itself cause enormous prob lems predict away training data figure  gives example overﬁtting context regression model pa rameters learned means maximum likelihood see section  discuss overﬁtting regression section  model class mθ rich enough example dataset generated sinusoidal function  parametrizes straight lines best optimization procedure close true model however optimize parameters ﬁnd best straight line models dataset figure  shows example model underﬁts insufﬁciently ﬂexible models underﬁt typ ically parameters case parametrized model class right then model ﬁts well ie overﬁts underﬁts means model class rich dataset given figure  shows model ﬁts given dataset fairly well ideally    deisenroth   faisal   ong published cambridge university press  run underﬁtting encounter opposite problem underﬁtting xytrainingdatamlexytrainingdatamlexytrainingdatamle  models meet data model class want work good generalization properties practice deﬁne rich model classes mθ pa rameters deep neural networks mitigate problem over ﬁtting use regularization section  priors section  discuss choose model class section   reading considering probabilistic models principle maximum likeli hood estimation generalizes idea leastsquares regression linear models discuss   restricting predictor linear form additional nonlinear function  applied output ie pyn xn   ϕθcidxn  consider models prediction tasks binary classiﬁcation modeling count data mccullagh nelder  alternative view consider likelihoods ex ponential family section  class models linear dependence parameters data potentially nonlin ear transformation  called link function referred generalized linear models agresti    maximum likelihood estimation rich history originally proposed sir ronald fisher  expand idea probabilistic model section  debate researchers use probabilistic models discussion bayesian fre quentist statistics mentioned section  boils deﬁnition probability recall section  consider probability generalization by allowing uncertainty logical rea soning cheeseman  jaynes  method maximum like lihood estimation frequentist nature interested reader pointed efron hastie  balanced view bayesian frequentist statistics probabilistic models maximum likelihood esti mation possible reader referred advanced sta tistical textbooks eg casella berger  approaches method moments  estimation estimating equations link function generalized linear model  probabilistic modeling inference machine learning frequently concerned interpretation analysis data eg prediction future events decision making task tractable build models generative process generates observed data generative process draft  mathematics machine learning feedback smmlbookcom  probabilistic modeling inference example outcome coinﬂip experiment heads tails steps first deﬁne parameter  describes probability heads parameter bernoulli distri bution   second sample outcome  head tail   berµ parameter  gives bernoulli distribution px depends coin used  un rise speciﬁc dataset known advance observed directly need mecha nisms learn  given observed outcomes coinﬂip experiments following discuss probabilistic modeling purpose    probabilistic models probabilistic models represent uncertain aspects experiment probability distributions beneﬁt probabilistic models offer uniﬁed consistent set tools probability theory   modeling inference prediction model selection probabilistic modeling joint distribution px  observed variables  hidden parameters  central importance en capsulates information following prior likelihood product rule section  marginal likelihood px play important role model selection section  computed taking joint dis tribution integrating parameters sum rule section  posterior obtained dividing joint marginal likelihood joint distribution property therefore probabilistic model speciﬁed joint distribution random variables  bayesian inference key task machine learning model data uncover values models hidden variables  given observed variables  section  discussed ways estimating model parameters  maximum likelihood maximum posteriori esti mation cases obtain singlebest value  key algorithmic problem parameter estimation solving optimization problem point estimates  known use predictions speciﬁcally predictive distribution px use  likelihood function discussed section  focusing solely statistic pos terior distribution such parameter  maximizes poste rior leads loss information critical    deisenroth   faisal   ong published cambridge university press  probabilistic model speciﬁed joint distribution random variables parameter estimation phrased optimization problem bayesian inference learning distribution random variables bayesian inference bayesian inference inverts relationship parameters data models meet data  decisions decisionmaking uses prediction px systems typically different objective functions likelihood squarederror loss misclassiﬁcation error therefore having posterior distribution extremely useful leads robust decisions bayesian inference ﬁnding posterior distri  parameter prior pθ bution gelman et al  dataset likelihood function posterior pθ     θpθ   cid   θpθdθ    obtained applying bayes theorem key idea exploit bayes theorem invert relationship parameters  data given likelihood obtain posterior distribution pθ implication having posterior distribution parameters propagate uncertainty parameters data speciﬁcally distribution pθ parameters predictions   px  px θpθdθ  eθpx   cid longer depend model parameters  marginalizedintegrated out equation  reveals prediction average plausible parameter values  plausibility encapsulated parameter distribution pθ having discussed parameter estimation section  bayesian in ference here let compare approaches learning parameter estimation maximum likelihood map estimation yields consistent point estimate  parameters key computational problem solved optimization contrast bayesian inference yields pos terior distribution key computational problem solved integration predictions point estimates straightforward predictions bayesian framework require solving integration problem  however bayesian inference gives principled way incorporate prior knowledge account information incorporate structural knowledge easily context parameter estimation moreover propagation parameter uncertainty prediction valuable decisionmaking systems risk assessment exploration context dataefﬁcient learn ing deisenroth et al  kamthe deisenroth  bayesian inference mathematically principled framework learning parameters making predictions prac tical challenges come integration problems need solve   speciﬁcally choose conjugate prior parameters section  integrals   analytically tractable compute pos draft  mathematics machine learning feedback smmlbookcom  probabilistic modeling inference terior predictions marginal likelihood closed form cases need resort approximations here use stochas tic approximations markov chain monte carlo mcmc gilks et al  deterministic approximations laplace ap proximation bishop  barber  murphy  variational in ference jordan et al  blei et al  expectation propaga tion minka  despite challenges bayesian inference successfully ap plied variety problems including largescale topic modeling hoff man et al  clickthroughrate prediction graepel et al  dataefﬁcient reinforcement learning control systems deisenroth et al  online ranking systems herbrich et al  largescale rec ommender systems generic tools bayesian optimiza tion brochu et al  snoek et al  shahriari et al  useful ingredients efﬁcient search meta parameters models algorithms remark machine learning literature somewhat ar bitrary separation random variables parameters parameters estimated eg maximum likelihood variables usually marginalized out book strict sep aration because principle place prior parameter integrate out turn parameter random vari able according aforementioned separation  latentvariable models practice useful additional latent variables  besides model parameters  model moustaki et al  latent variables different model parameters  parametrize model explicitly latent variables datagenerating process contributing inter pretability model simpli structure model allow deﬁne simpler richer model structures sim pliﬁcation model structure goes hand hand smaller number model parameters paquet  murphy  learning latentvariable models at maximum likelihood principled way expectation maximization them algorithm demp ster et al  bishop  examples latent variables helpful principal component analysis dimensionality reduc tion   gaussian mixture models density estimation chap ter  hidden markov models maybeck  dynamical systems ghahramani roweis  ljung  timeseries modeling meta learning task generalization hausman et al  sæ mundsson et al  introduction latent variables    deisenroth   faisal   ong published cambridge university press  latent variable models meet data model structure generative process easier learning latentvariable models generally hard   latentvariable models allow deﬁne process generates data parameters let look generative pro cess denoting data  model parameters  latent vari ables  obtain conditional distribution px   allows generate data model parameters latent vari ables given  latent variables place prior pz them models discussed previously models latent variables parameter learning inference frameworks discussed sections   facilitate learning eg means maximum likelihood estimation bayesian inference fol low twostep procedure first compute likelihood px model depend latent variables second use likelihood parameter estimation bayesian inference use exactly expressions sections   respectively likelihood function px  predictive distribution data given model parameters need marginalize latent variables cid px   px  θpzdz  px   given  pz prior latent variables note likelihood depend latent variables  function data  model parameters  likelihood  directly allows parameter estimation maximum likelihood map estimation straightforward ad ditional prior model parameters  discussed section  moreover likelihood  bayesian inference section  latentvariable model works usual way place prior pθ model parameters use bayes theorem obtain posterior distribution pθ     θpθ   model parameters given dataset  posterior  predictions bayesian inference framework  challenge latentvariable model like lihood   requires marginalization latent variables ac cording  choose conjugate prior pz   marginalization  analytically tractable px need resort approximations bishop  paquet  mur phy  moustaki et al    draft  mathematics machine learning feedback smmlbookcom likelihood function data model parameters independent latent variables  probabilistic modeling inference similar parameter posterior  compute posterior latent variables according pz     zpz   cid    θpθdθ      pz prior latent variables  integrate model parameters     requires given difﬁculty solving integrals analytically clear mar ginalizing latent variables model parameters time possible general bishop  murphy  quantity easier compute posterior distribution latent variables conditioned model parameters ie pz       θpz     pz prior latent variables      given    derive likelihood functions pca gaussian mixture models respectively moreover compute poste rior distributions  latent variables pca gaussian mixture models remark following  drawing clear distinction latent variables  uncertain model parameters  model parameters latent hidden unobserved    use latent variables  pay attention difference different types hidden variables model parameters  latent variables  exploit fact elements probabilistic model random variables deﬁne uniﬁed language representing them section  concise graphical language representing structure probabilistic models use graphical language probabilistic models subsequent   reading probabilistic models machine learning bishop  barber  murphy  provide way users capture uncertainty data predictive models principled fashion ghahramani  presents short review probabilistic models machine learning given proba bilistic model lucky able compute parameters analytically however general analytic solutions rare computational methods sampling gilks et al  brooks et al  variational inference jordan et al  blei et al    deisenroth   faisal   ong published cambridge university press  probabilistic programming directed graphical model directed graphical models known bayesian networks graphical model models meet data  used moustaki et al  paquet  provide good overview bayesian inference latentvariable models recent years programming languages proposed aim treat variables deﬁned software random variables corresponding probability distributions objective able write complex functions probability distributions hood compiler automatically takes care rules bayesian inference rapidly changing ﬁeld called probabilistic programming  directed graphical models section introduce graphical language speciing prob abilistic model called directed graphical model provides compact succinct way speci probabilistic models allows reader visually parse dependencies random variables graphical model visually captures way joint distribution random variables decomposed product factors depending subset variables section  identiﬁed joint distri bution probabilistic model key quantity comprises information prior likelihood posterior however joint distribution complicated tell structural properties probabilis tic model example joint distribution pa   tell independence relations point graphical models come play section relies concepts independence conditional independence described section  graphical model nodes random variables figure  nodes represent random variables    edges represent probabilistic relations variables eg conditional probabilities remark distribution represented particular choice graphical model discussion bishop  probabilistic graphical models convenient properties simple way visualize structure probabilistic model design motivate new kinds statistical models inspection graph gives insight properties eg con ditional independence complex computations inference learning statistical models expressed terms graphical manipulations  graph semantics directed graphical modelbayesian network directed graphical modelsbayesian networks method representing conditional dependencies probabilistic model provide visual draft  mathematics machine learning feedback smmlbookcom  directed graphical models description conditional probabilities hence providing simple lan guage describing complex interdependence modular description additional assumptions entails computational simpliﬁcation directed links arrows arrows nodes random variables indicate conditional probabilities ex indicate causal ample arrow  figure  gives conditional relationships pearl probability pb   given  figure  examples directed graphical models  fully connected  fully connected directed graphical models derived joint distributions know factorization example  consider joint distribution pa    pc  bpb apa random variables    factorization joint distribution  tells relationship random variables  depends directly   depends directly  depends   factorization  obtain directed graphical model figure  general construct corresponding directed graphical model factorized joint distribution follows  create node random variables  conditional distribution add directed link arrow graph nodes corresponding variables distribution conditioned graph layout depends choice factorization joint dis tribution discussed known factorization joint dis tribution corresponding directed graphical model now    deisenroth   faisal   ong published cambridge university press  graph layout depends factorization joint distribution models meet data exactly opposite extract joint distribution set random variables given graphical model example  looking graphical model figure  exploit proper ties joint distribution px      seek product set conditionals node graph particular example need ﬁve conditionals conditional depends parents corresponding node graph example  conditioned  properties yield desired factorization joint distribu tion px      pxpxpx xpx  xpx    general joint distribution px  px     xk given px  pxk pak  cid pak means the parent nodes xk parent nodes xk nodes arrows pointing xk conclude subsection concrete example coinﬂip experiment consider bernoulli experiment example  probability outcome  experiment heads px   berµ  repeat experiment  times observe outcomes      xn obtain joint distribution px     xn   pxn   cid expression righthand product bernoulli distribu tions individual outcome experiments indepen dent recall section  statistical independence means distribution factorizes write graphical model set ting distinction unobservedlatent variables observed variables graphically observed variables denoted shaded nodes obtain graphical model figure  single parameter  xn         outcomes xn identically distributed compact equivalent graphical model setting given figure  use draft  mathematics machine learning feedback smmlbookcom  directed graphical models xn xn figure  graphical models repeated bernoulli experiment xn  version xn explicit                  version plate notation  hyperparameters   latent  plate notation plate box repeats inside in case observations xn  times therefore graphical models equiv alent plate notation compact graphical models immedi ately allow place hyperprior  hyperprior second layer prior distributions parameters ﬁrst layer priors fig ure  places betaα  prior latent variable  treat   deterministic parameters ie random variables omit circle it plate hyperprior dseparation  conditional independence dseparation directed graphical models allow ﬁnd conditional independence sec tion  relationship properties joint distribution looking graph concept called dseparation pearl  key this consider general directed graph arbitrary nonin tersecting sets nodes whose union smaller complete set nodes graph wish ascertain particular con ditional independence statement  given conditionally independent  denoted     implied given directed acyclic graph so consider possible trails paths ignore direction arrows node  path said blocked includes node following true nodes arrows path meet head tail tail tail node node set arrows meet head head node node descendants set paths blocked joint distribution variables graph satis said dseparated        deisenroth   faisal   ong published cambridge university press  figure  types graphical models  directed graphical models bayesian networks  undirected graphical models markov random ﬁelds  factor graphs models meet data  directed graphical model undirected graphical  factor graph model example  conditional independence figure  dseparation example consider graphical model figure  visual inspection gives cid cid     directed graphical models allow compact representation proba bilistic models examples directed graphical models     representation concept con ditional independence allows factorize respective probabilistic models expressions easier optimize graphical representation probabilistic model allows visually impact design choices structure model need highlevel assumptions structure model modeling assumptions hyperparameters affect prediction performance selected directly approaches seen far discuss different ways choose structure section  draft  mathematics machine learning feedback smmlbookcom  model selection  reading introduction probabilistic graphical models bishop    extensive description different applica tions corresponding algorithmic implications book koller friedman  main types probabilistic graphical models directed graphical models bayesian networks figure  undirected graphical models markov random ﬁelds figure  factor graphs figure  graphical models allow graphbased algorithms inference learning eg local message passing applications range rank ing online games herbrich et al  vision eg image segmentation semantic labeling image denoising image restora tion kittler foglein  sucar gillies  shotton et al  szeliski et al  coding theory mceliece et al  solv ing linear equation systems shental et al  iterative bayesian state estimation signal processing bickson et al  deisenroth mohamed  topic particularly important real applications discuss book idea structured prediction bakir et al  nowozin et al  allows machine learning models tackle predictions structured example sequences trees graphs popularity neural network models allowed ﬂex ible probabilistic models used resulting useful applica tions structured models goodfellow et al    recent years renewed graphical models applications causal inference pearl  imbens rubin  peters et al  rosenbaum   model selection machine learning need highlevel modeling decisions critically inﬂuence performance model choices eg functional form likelihood inﬂuence number type free parameters model ﬂexibility expressivity model complex models ﬂexible sense datasets instance polynomial degree   line     ax linear relations inputs  observations  polynomial degree  additionally quadratic relationships inputs observations think ﬂexible models generally preferable simple models expressive general problem    deisenroth   faisal   ong published cambridge university press  directed graphical model bayesian network undirected graphical model markov random ﬁeld factor graph polynomial    axax linear functions setting    ie strictly expressive ﬁrstorder polynomial models meet data figure  nested crossvalidation perform levels kfold crossvalidation labeled data training data test data train model validation training time use training set evaluate performance model learn parameters however per formance training set interested in section  seen maximum likelihood estimation lead overﬁtting especially training dataset small ideally model also works test set which available training time therefore need mechanisms assessing model generalizes unseen test data model selection concerned exactly problem  nested crossvalidation seen approach crossvalidation section  model selection recall crossvalidation provides estimate generalization error repeatedly splitting dataset training validation sets apply idea time ie split perform round crossvalidation referred nested crossvalidation figure  inner level estimate performance particular choice model hyperparameter internal validation set outer level estimate generalization performance best choice model chosen inner loop test different model hyperparameter choices inner loop distinguish levels set estimate generalization performance called test set set choosing best model called validation set inner loop estimates expected value generalization error given model  approximating empirical error validation set ie evr        cid   empirical risk eg root mean square error   model   repeat procedure models validation set choose model performs best note crossvalidation gives expected generalization error obtain high order statistics eg standard error estimate uncertain draft  mathematics machine learning feedback smmlbookcom nested crossvalidation test set validation set standard error deﬁned  number experiments  standard deviation risk experiment  model selection evidence     mean estimate is model chosen evaluate ﬁnal performance test set  bayesian model selection approaches model selection covered section generally attempt trade model complexity data ﬁt assume simpler models prone overﬁtting complex models objective model selection ﬁnd simplest model explains data reasonably well concept known occams razor remark treat model selection hypothesis testing problem looking simplest hypothesis consistent data mur phy  consider placing prior models favors simpler models however necessary this automatic occams razor quantitatively embodied application bayesian probability smith spiegelhalter  jefferys berger  mackay  fig ure  adapted mackay  gives basic intuition complex expressive models turn probable  let think horizontal axis choice modeling given dataset  interested representing space possible datasets posterior probability pmi employ bayes theorem assuming uniform prior pm  mod els bayes theorem rewards models proportion pre dicted data occurred prediction data given model mi called evidence mi simple model  mi  predict small number datasets shown  powerful model  has eg free parameters  able  model mi given data          deisenroth   faisal   ong published cambridge university press  figure  bayesian inference embodies occams razor horizontal axis describes space possible datasets  evidence vertical axis evaluates model predicts available data pd  mi needs integrate  choose model greatest evidence adapted mackay occams razor predictions quantiﬁed normalized probability distribution  ie needs integratesum  evidence models meet data predict greater variety datasets means however  predict datasets region   suppose equal prior probabilities assigned models then dataset falls region  powerful model  probable model earlier  argued models need able explain data ie way generate data given model furthermore model appropriately learned data expect generated data similar empirical data this helpful phrase model selection hierarchical inference problem allows compute posterior distribution models let consider ﬁnite number models   model mk possesses parameters θk bayesian model selection place prior pm  set models corresponding generative process allows generate data model      mk mk θk   pm  pθ   mk θk illustrated figure  given training set theorem compute posterior distribution models  apply bayes pmk pmkp mk      note posterior longer depends model parameters θk integrated bayesian setting cid mk  θkpθk mkdθk      pθk mk prior distribution model parameters θk model mk term  referred model evidence marginal likelihood posterior  determine map estimate    arg max mk pmk     uniform prior pmk     gives model equal prior probability determining map estimate models amounts pick ing model maximizes model evidence  remark likelihood marginal likelihood important differences likelihood marginal likelihood evidence likelihood prone overﬁtting marginal likelihood typ ically model parameters marginalized ie longer ﬁt parameters furthermore marginal likeli hood automatically embodies tradeoff model complexity data ﬁt occams razor draft  mathematics machine learning feedback smmlbookcom bayesian model selection generative process figure  illustration hierarchical generative process bayesian model selection place prior pm  set models model distribution pθ    corresponding model parameters generate data  model evidence marginal likelihood posterior odds prior odds bayes factor jeffreyslindley paradox  model selection  bayes factors model comparison consider problem comparing probabilistic models    compute posteriors pm given dataset compute ratio posteriors  pm     pm   pm   cid cidcid cid posterior odds pd  mpm pd pd  mpm pd pm pm cid cidcid cid prior odds cid cid     cidcid bayes factor ratio posteriors called posterior odds ﬁrst frac tion righthand  prior odds measures prior initial beliefs favor   ratio marginal like lihoods second fraction righthandside called bayes factor measures data predicted  compared  remark jeffreyslindley paradox states bayes factor favors simpler model probability data complex model diffuse prior small murphy  here diffuse prior refers prior favor speciﬁc models ie models priori plausible prior choose uniform prior models prior odds term   ie posterior odds ratio marginal likelihoods bayes factor     bayes factor greater  choose model  model  similar way frequentist statistics guidelines size ratio consider signiﬁcance result jeffreys  remark computing marginal likelihood marginal likelihood plays important role model selection need compute bayes factors  posterior distributions models  unfortunately computing marginal likelihood requires solve integral  integration generally analytically intractable resort approximation techniques eg numerical integration stoer burlirsch  stochastic approximations monte carlo murphy  bayesian monte carlo techniques ohagan  rasmussen ghahramani  however special cases solve it section  discussed conjugate models choose conjugate parameter prior pθ compute marginal likelihood closed form chap ter  exactly context linear regression seen brief introduction basic concepts machine learning  rest book    deisenroth   faisal   ong published cambridge university press  models meet data different ﬂavors learning sections    applied pillars machine learning regression dimensionality reduction density estimation classiﬁcation  reading mentioned start section highlevel modeling choices inﬂuence performance model examples include following degree polynomial regression setting number components mixture model network architecture deep neural network type kernel support vector machine dimensionality latent space pca learning rate schedule optimization algorithm rasmussen ghahramani  showed automatic occams razor necessarily penalize number parameters model active terms complexity functions showed automatic occams razor holds bayesian nonparametric models parameters eg gaussian processes focus maximum likelihood estimate exist number heuristics model selection discourage overﬁtting called information criteria choose model largest value akaike information criterion aic akaike  log px corrects bias maximum likelihood estimator addition penalty term compensate overﬁtting complex models lots parameters here  number model parameters aic estimates relative information lost given model bayesian information criterion bic schwarz  cid log px  log px θpθdθ log px  log  exponential family distributions here  number data points  number parameters bic penalizes model complexity heavily aic parametric models number parameters related complexity model class akaike information criterion bayesian information criterion draft  mathematics machine learning feedback smmlbookcom linear regression regression following apply mathematical concepts chap ters     solve linear regression curve ﬁtting problems rd corre regression aim ﬁnd function  maps inputs   assume given set train sponding function values   ing inputs xn corresponding noisy observations yn   xncid cid iid random variable describes measurementobservation noise potentially unmodeled processes which consider   assume zeromean gaussian noise task ﬁnd function models training data generalizes predicting function values input locations training data see   il lustration regression problem given figure  typical regression setting given figure  input values xn observe noisy function values yn   xn  cid task infer function  generated data generalizes function values new input locations possible solution given figure  distributions centered function values   represent noise data regression fundamental problem machine learning regres sion problems appear diverse range research areas applica figure   dataset  possible solution regression problem  regression problem observed noisy func tion values wish infer underlying function generated data  regression solution possible function generated data blue indication measurement noise function value corresponding in puts orange distributions material published cambridge university press mathematics machine learning marc peter deisenroth  aldo faisal cheng soon ong  version free view download personal use only redistribution resale use derivative works by   deisenroth   faisal   ong  smmlbookcom xyxy normally type noise model choice ﬁx noise gaussian linear regression tions including timeseries analysis eg identiﬁcation control robotics eg reinforcement learning forwardinverse model learn ing optimization eg line searches global optimization deep learning applications eg games speechtotext translation image recognition automatic video annotation regression key ingredient classiﬁcation algorithms finding regression function re quires solving variety problems including following choice model type parametrization regres sion function given dataset function classes eg polynomi als good candidates modeling data particular parametrization eg degree polynomial choose model selection discussed section  allows compare var ious models ﬁnd simplest model explains training data reasonably well finding good parameters having chosen model regression function ﬁnd good model parameters here need look different lossobjective functions they determine good ﬁt is optimization algorithms allow minimize loss overﬁtting model selection overﬁtting problem regression function ﬁts training data too well gen eralize unseen test data overﬁtting typically occurs underly ing model or parametrization overly ﬂexible expressive section  look underlying reasons discuss ways mitigate effect overﬁtting context linear regression relationship loss functions parameter priors loss func tions optimization objectives motivated induced prob abilistic models look connection loss functions underlying prior assumptions induce losses uncertainty modeling practical setting access ﬁnite potentially large training data selecting model class corresponding parameters given ﬁnite training data cover possible scenarios want remaining parameter uncertainty obtain mea sure conﬁdence models prediction test time smaller training set important uncertainty modeling consistent mod eling uncertainty equips model predictions conﬁdence bounds following mathematical tools chap ters     solve linear regression problems discuss maximum likelihood maximum posteriori map estimation ﬁnd optimal model parameters parameter estimates brief look generalization errors overﬁtting end  discuss bayesian linear regression allows reason model parameters higher level removing problems encountered maximum likelihood map estimation draft  mathematics machine learning feedback smmlbookcom  problem formulation  problem formulation presence observation noise adopt probabilis tic approach explicitly model noise likelihood function speciﬁcally  consider regression prob lem likelihood function py   cidy   σcid  here   functional relationship   given  noisy function values targets rd inputs       cid    cid σcid independent identically distributed iid gaus cid sian measurement noise mean  variance  objective ﬁnd function close similar unknown function  generated data generalizes well  focus parametric models ie choose para metrized function ﬁnd parameters  work well modeling data time being assume noise variance  known focus learning model parameters  linear regression consider special case parameters  appear linearly model example linear regression given    py   xcidθ  cid  cidy cid xcidθ σcid cid σcid    rd parameters seek class functions de scribed  straight lines pass origin  chose parametrization    xcidθ likelihood  probability density function  evalu ated xcidθ note source uncertainty originates observation noise as   assumed known  ob servation noise relationship   deterministic  dirac delta example   linear regression model  describes straight lines   linear functions parameter  slope line fig ure  shows example functions different values  linear regression model  linear pa rameters linear inputs  figure  shows examples functions later   φcidxθ nonlinear trans formations  linear regression model linear regression    deisenroth   faisal   ong published cambridge university press  dirac delta delta function zero single point integral  considered gaussian limit    likelihood linear regression refers models linear parameters figure  linear regression example  example functions fall category  training set  maximum likelihood estimate training set figure  probabilistic graphical model linear regression observed random variables shaded deterministic known values circles xn yn         linear regression  example functions straight lines described us ing linear model   training set  maximum likelihood esti mate refers models linear parameters ie models de scribe function linear combination input features here fea ture representation φx inputs  following discuss ﬁnd good pa rameters  evaluate parameter set works well time being assume noise variance  known  parameter estimation       xn  yn  consider linear regression setting  assume given training set rd corresponding observationstargets yn           corresponding graphical model given figure  note yi yj conditionally independent given respective inputs xi xj likelihood factorizes according consisting  inputs xn       py     yn cid pyn xn        xn   cid   cidyn   σcid  xcid sets deﬁned training inputs corresponding targets respectively likelihood factors pyn xn  gaussian noise distribution      xn      yn following discuss ﬁnd optimal parameters  rd linear regression model  parameters  found predict function values parameter estimate  arbitrary test input  distribution corre sponding target  py    cidy   σcid  xcid following look parameter estimation maxi mizing likelihood topic covered degree section  draft  mathematics machine learning feedback smmlbookcom xyxyxy  parameter estimation  maximum likelihood estimation widely approach ﬁnding desired parameters θml maximum maximum likelihood likelihood estimation ﬁnd parameters θml maximize likelihood  intuitively maximizing likelihood means maximiz maximizing likelihood means ing predictive distribution training data given model param maximizing eters obtain maximum likelihood parameters predictive distribution training data given parameters θml  arg max estimation       likelihood probability distribution parameters logarithm strictly monotonically increasing function optimum function  identical optimum log   remark likelihood py   probability distribution  simply function parameters  integrate  ie unnormalized integrable respect  however likelihood  normalized probability distribution ﬁnd desired parameters θml maximize likelihood typically perform gradient ascent or gradient descent negative likelihood case linear regression consider here however closedform solution exists makes iterative gradient descent un necessary practice instead maximizing likelihood directly apply logtransformation likelihood function minimize negative loglikelihood remark logtransformation likelihood  product  gaussian distributions logtransformation useful  suffer numerical underﬂow  differentiation rules turn simpler speciﬁcally numerical underﬂow prob lem multiply  probabilities  number data points represent small numbers  furthermore logtransform turn product sum log probabilities corresponding gradient sum individual gradients instead repeated application product rule  compute gradient product  terms ﬁnd optimal parameters θml linear regression problem minimize negative loglikelihood log     log pyn xn   log pyn xn       cid cid exploited likelihood  factorizes number data points independence assumption training set linear regression model  likelihood gaussian due gaussian additive noise term arrive  yn constant includes terms independent      const  xn   log pyn xcid    deisenroth   faisal   ong published cambridge university press  linear regression negative loglikelihood  obtain ignoring constant terms   yn xcid   cid   xθcidy xθ   cid xθ   cid negative loglikelihood function called error function design matrix squared error measure distance recall section  cidxcid  xcidx choose dot product inner product right  deﬁne design matrix        xn cid right vector collection training inputs        yn cid collects training targets note nth row design matrix  corresponds training input xn  fact sum squared errors observations yn corresponding model prediction xcid   equals squared distance  xθ  concrete form negative loglikelihood function need optimize immediately  quadratic  means ﬁnd unique global solution θml mini mizing negative loglikelihood  ﬁnd global optimum computing gradient  setting  solving  results   compute gradient respect parameters ignoring possibility duplicate data points rkx    cid  ie parameters data points dθ xθcidy cid    cid ycidy dθ ycidx  θcidx cidx dθ   cid xθ rd  ycidxθ  θcidx cidxθ cid maximum likelihood estimator θml solves dl mality condition obtain dθ  cid necessary opti dθ  cid  θcid mlx cidx  ycidx θcid ml  ycidxx cidx θml   cidxx cidy  rightmultiply ﬁrst equation  cidx  cidx positive deﬁnite rkx   rkx denotes rank  remark setting gradient cid necessary sufﬁcient condition obtain global minimum hessian rdd positive deﬁnite remark maximum likelihood solution  requires solve linear equations form aθ     cidx    cidy    cidx θl draft  mathematics machine learning feedback smmlbookcom  parameter estimation example  fitting lines let look figure  aim ﬁt straight line    θx  unknown slope dataset maximum likelihood estimation examples functions model class straight lines shown figure  dataset shown figure  ﬁnd maximum likelihood estimate slope parameter   obtain maximum likelihood linear function figure  maximum likelihood estimation features far considered linear regression setting described  allowed ﬁt straight lines data maximum likelihood estimation however straight lines sufﬁciently expressive comes ﬁtting interesting data fortunately linear regression offers way ﬁt nonlinear functions linear regression framework linear regression refers linear parameters perform arbitrary nonlinear transformation φx inputs  linearly combine components transformation corre sponding linear regression model linear regression refers linearin theparameters regression models inputs undergo nonlinear transformation py      φcidxθ  cid  cidy φcidxθ σcid cid θkφkx  cid    rd φk  rd model parameters  appear linearly rk nonlinear transformation inputs   kth component feature vector  note feature vector example  polynomial regression concerned regression problem   φcidxθcid  rk transformation context φx  φx φx φkx xk rk  means lift original onedimensional input space kdimensional feature space consisting monomials xk          features model polynomials degree cid   framework linear regression polynomial degree    deisenroth   faisal   ong published cambridge university press  linear regression    θkxk  φcidxθ  cid  deﬁned         θkcid linear parameters θk rk contains feature matrix design matrix let look maximum likelihood estimation param eters  linear regression model  consider training inputs           deﬁne feature matrix xn design matrix rd targets yn   φcidx φcidxn    φx φx φxn  φkx φkx φkxn    right   φij  φjxi φj  rd          example  feature matrix secondorder polynomials secondorder polynomial  training points xn        feature matrix           xn  feature matrix  deﬁned  negative loglikelihood linear regression model  written log          φθcidy φθ  const  comparing  negative loglikelihood  fea turefree model immediately need replace     independent parameters  wish optimize arrive immediately maximum likelihood estimate θml  φcidφφcidy linear regression problem nonlinear features deﬁned  remark working features required  cidx invertible case rkx   ie columns  draft  mathematics machine learning feedback smmlbookcom maximum likelihood estimate figure  polynomial regression  dataset consisting xn yn pairs          maximum likelihood polynomial degree   parameter estimation linearly independent  require φcidφ invertible case rkφ   rkk example  maximum likelihood polynomial fit  regression dataset  polynomial degree  determined max imum likelihood estimation consider dataset figure  dataset consists    sinxn  cosxn  cid   yn  pairs xn yn xn cid cid cid   ﬁt polynomial degree  maximum likelihood estimation ie parameters θml given  maximum likelihood estimate yields function values φcidxθml test location  result shown figure  estimating noise variance far assumed noise variance  known however use principle maximum likelihood estimation obtain maximum likelihood estimator  ml noise variance this follow standard procedure write loglikelihood com pute derivative respect    set  solve loglikelihood given log         cid cid log  cid cid cid log cidyn φcidxnθ σcid logπ log   yn cid φcidxnθ yn φcidxnθ  const  cidcid cid    deisenroth   faisal   ong published cambridge university press  xyxytrainingdatamle  linear regression partial derivative loglikelihood respect   log               identi ml  cid yn φcidxnθ  therefore maximum likelihood estimate noise variance empirical mean squared distances noisefree function values φcidxnθ corresponding noisy observations yn input lo cations xn  overﬁtting linear regression discussed use maximum likelihood estimation ﬁt lin ear models eg polynomials data evaluate quality model computing errorloss incurred way compute negative loglikelihood  minimized determine maximum likelihood estimator alternatively given noise parameter  free model parameter ignore scaling  end squarederrorloss function  instead squared loss use root mean φθ cid  φθ  cid   cid cid cid cid cid cid yn φcidxnθ   allows compare errors datasets different sizes  scale units observed func tion values yn example ﬁt model maps postcodes  given latitude longitude house prices yvalues eur rmse measured eur squared error given eur choose include factor  original negative loglikelihood  end unitless objective ie preceding example objective longer eur eur model selection see section  use rmse or negative loglikelihood determine best degree polynomial ﬁnding polynomial degree  minimizes objective given polynomial degree natural number perform bruteforce search enumerate reasonable values   training set size  sufﬁcient test  cid  cid       maximum likelihood estimator unique  cid   parameters draft  mathematics machine learning feedback smmlbookcom root mean square error rmse cid cid square error rmse rmse normalized negative loglikelihood unitless  parameter estimation figure  maximum likelihood ﬁts different polynomial degrees                           data points need solve underdetermined linear equations φcidφ  longer invertible inﬁnitely possible maximum likelihood estimators figure  shows number polynomial ﬁts determined maximum likelihood dataset figure     observations notice polynomials low degree eg constants    linear    ﬁt data poorly and hence poor representations true underlying function degrees         ﬁts look plausible smoothly interpolate data higherdegree polynomials notice ﬁt data better better ex treme case       function pass single data point however highdegree polynomials oscillate wildly poor representation underlying function generated data suffer overﬁtting remember goal achieve good generalization making accurate predictions new unseen data obtain quantita tive insight dependence generalization performance polynomial degree  considering separate test set comprising  data points generated exactly procedure generate training set test inputs chose linear grid  points   choice   evaluate rmse  interval  training data test data looking test error qualitive measure gen eralization properties corresponding polynomial notice ini tially test error decreases figure  orange fourthorder polynomials test error relatively low stays relatively constant degree  however degree  onward test error increases signif icantly highorder polynomials bad generalization proper ties particular example evident corresponding    deisenroth   faisal   ong published cambridge university press  case      extreme sense null space corresponding linear equations nontrivial inﬁnitely optimal solutions linear regression problem overﬁtting note noise variance    xytrainingdatamlexytrainingdatamlexytrainingdatamlexytrainingdatamlexytrainingdatamlexytrainingdatamle  linear regression figure  training test error training error test error maximum likelihood ﬁts figure  note training error blue curve figure  increases degree polynomial in creases example best generalization the point smallest test error obtained polynomial degree    maximum posteriori map  maximum posteriori estimation saw maximum likelihood estimation prone overﬁtting observe magnitude parameter values relatively large run overﬁtting bishop  mitigate effect huge parameter values place prior distribution pθ parameters prior distribution explicitly en codes parameter values plausible before having seen data cid cid single parameter example gaussian prior pθ     encodes parameter values expected lie interval  two standard deviations mean value dataset available instead maximizing likelihood seek parameters maximize posterior distribution pθ  procedure called maximum posteriori map estimation   posterior parameters  given training data obtained applying bayes theorem section  pθ      θpθ       posterior explicitly depends parameter prior pθ prior effect parameter vector ﬁnd maximizer posterior explicitly following parameter vector θmap maximizes posterior  map estimate ﬁnd map estimate follow steps similar ﬂavor maximum likelihood estimation start logtransform compute logposterior log pθ   log     log pθ  const       draft  mathematics machine learning feedback smmlbookcom degreeofpolynomialrmsetrainingerrortesterror  parameter estimation constant comprises terms independent  logposterior  sum loglikelihood    logprior log pθ map estimate compromise prior our suggestion plausible parameter values observing data datadependent likelihood    ﬁnd map estimate θmap minimize negative logposterior distribution respect  ie solve θmap  arg min log      log pθ    gradient negative logposterior respect   log pθ dθ    log       dθ  log pθ dθ identi ﬁrst term righthand gradient negative loglikelihood  conjugate gaussian prior pθ  cid bicid parameters  negative logposterior linear regression setting  obtain negative log posterior   log pθ   φθcidy φθ    here ﬁrst term corresponds contribution loglikelihood second term originates logprior gradient log posterior respect parameters   θcidθ  const     ycidφ   θcid   log pθ dθ  θcidφcidφ ﬁnd map estimate θmap setting gradient cid solving θmap obtain  θcidφcidφ cid  θcid  θcid  cid  ycidφ  cid φcidφ  ycidφ  cid cid cid θcid φcidφ  θcid  ycidφ φcidφ   ycidφ cid cid cid θmap  φcidφ  cid φcidy  comparing map estimate  maximum likelihood es timate  difference solutions additional term   inverse matrix term ensures    deisenroth   faisal   ong published cambridge university press  map estimate by transposing sides equality φcidφ symmetric positive semi deﬁnite additional term  strictly positive deﬁnite inverse exists linear regression φcidφ    symmetric strictly positive deﬁnite ie inverse exists map estimate unique solution linear equations moreover reﬂects impact regularizer example  map estimation polynomial regression polynomial regression example section  place gaus cid icid parameters  determine map sian prior pθ  estimates according  figure  maximum likelihood map estimates polynomials degree  left degree  right prior regularizer play signiﬁcannot role lowdegree polynomial keeps function relatively smooth higherdegree polynomials map estimate push boundaries overﬁtting general solution problem need principled approach tackle overﬁtting  polynomials degree   polynomials degree   map estimation regularization instead placing prior distribution parameters  pos sible mitigate effect overﬁtting penalizing amplitude parameter means regularization regularized squares consider loss function cid φθ    cid   cid cid minimize respect  see section  here ﬁrst term dataﬁt term also called misﬁt term proportional negative loglikelihood  second term called regularizer regularization parameter  cid  controls strict ness regularization cidcid choose pnorm remark instead euclidean norm cidcidp  practice smaller values  lead sparser solutions here sparse means parameter values θd   draft  mathematics machine learning feedback smmlbookcom figure  polynomial regression maximum likelihood map estimates  polynomials degree   polynomials degree  regularization regularized squares dataﬁt term misﬁt term regularizer regularization parameter xytrainingdatamlemapxytrainingdatamlemap  bayesian linear regression useful variable selection    regularizer called lasso lasso least absolute shrinkage selection operator proposed tib shirani  regularizer    interpreted negative log gaussian prior use map estimation  specif cid bicid obtain negative ically gaussian prior pθ  loggaussian prior cid cid log pθ   cid cid   const  regularization term negative loggaussian    prior identical given regularized leastsquares loss function  consists terms closely related negative loglikelihood plus neg ative logprior surprising that minimize loss obtain solution closely resembles map estimate  speciﬁcally minimizing regularized leastsquares loss function yields θrls  φcidφ  λiφcidy  identical map estimate        noise variance  variance isotropic gaussian prior pθ  cid bicid far covered parameter estimation maximum likeli hood map estimation point estimates  op timize objective function likelihood posterior saw maximum likelihood map estimation lead overﬁtting section discuss bayesian linear regression use bayesian inference section  ﬁnd posterior distribution unknown parameters subsequently use predictions speciﬁcally predictions average plausible sets parameters instead focusing point estimate  bayesian linear regression previously looked linear regression models estimated model parameters  eg means maximum likelihood map esti mation discovered mle lead severe overﬁtting particu lar smalldata regime map addresses issue placing prior parameters plays role regularizer bayesian linear regression pushes idea parameter prior step attempt compute point estimate parameters instead posterior distribution parameters taken account making predictions means ﬁt parameters compute mean plausible parameters settings according posterior    deisenroth   faisal   ong published cambridge university press  point estimate single speciﬁc parameter value unlike distribution plausible parameter settings bayesian linear regression figure  graphical model bayesian linear regression linear regression  model bayesian linear regression consider model prior likelihood py pθ  cidm  cidy cid  φcidxθ σcid     cid  explicitly place gaussian prior pθ  turns parameter vector random variable allows write corresponding graphical model figure  parameters gaussian prior  explicit proba bilistic model ie joint distribution observed unobserved ran dom variables   respectively cidm  py    py  θpθ   prior predictions practice usually interested parameter values  themselves instead focus lies predictions parameter values bayesian setting parameter distribution average plausible parameter settings predictions speciﬁcally predictions input  integrate  obtain cid py   py  θpθdθ  eθpy    interpret average prediction    plau sible parameters  according prior distribution pθ note pre dictions prior distribution require speci input  training data model  chose conjugate gaussian prior  predictive distribution gaussian and computed cid obtain closed form prior distribution pθ  predictive distribution cidm  py   cidφcidxm φcidxsφx  σcid  exploited  prediction gaussian conjugacy see section  marginalization property gaussians see sec tion  ii gaussian noise independent vy  vθφcidxθ  vcidcid  iii  linear transformation  apply rules computing mean covariance prediction analytically   respectively  term φcidxsφx predictive variance explicitly accounts uncertainty associated draft  mathematics machine learning feedback smmlbookcom  bayesian linear regression parameters   uncertainty contribution measurement noise interested predicting noisefree function values    φcidxθ instead noisecorrupted targets  obtain cidφcidxm φcidxsφxcid  pf   differs  omission noise variance  predictive variance remark distribution functions represent distri bution pθ set samples θi sample θi gives rise   θcid  follows parameter distribution pθ function fi induces distribution pf   functions use notation  explicitly denote functional relationship parameter distribution pθ induces distribution functions example  prior functions figure  prior functions  distribution functions represented mean function black line marginal uncertainties shaded representing   conﬁdence bounds respectively  samples prior functions induced samples parameter prior  prior distribution functions  samples prior distribution functions cid  let consider bayesian linear regression problem polynomials  icid figure  degree  choose parameter prior pθ  visualizes induced prior distribution functions shaded area dark gray  conﬁdence bound light gray  conﬁdence bound induced parameter prior including function samples prior function sample obtained ﬁrst sampling parameter vector   input lo θi cations  uncertainty represented shaded area figure  solely parameter uncertainty considered noisefree predictive distribution    θcid   apply feature function  pθ computing fi far looked computing predictions parameter prior pθ however parameter posterior given train ing data  principles prediction inference hold   need replace prior pθ posterior    deisenroth   faisal   ong published cambridge university press  xyxy marginal likelihood evidence marginal likelihood expected likelihood parameter prior linear regression pθ predictions    following derive posterior distribution  posterior distribution rd corresponding observations           compute posterior parameters given training set inputs xn yn bayes theorem pθ      θpθ       collection correspond   likelihood pθ set training inputs ing training targets furthermore  parameter prior cid             θpθdθ  eθp      marginal likelihoodevidence independent parameters  ensures posterior normalized ie integrates  think marginal likelihood likelihood averaged possible parameter settings with respect prior distribution pθ theorem  parameter posterior model  parameter posterior  computed closed form cid  pθ     cidθ mn  sn sn     σφcidφ  mn  sn     σφcidy  subscript  indicates size training set proof bayes theorem tells posterior pθ tional product likelihood      prior pθ  propor posterior likelihood prior    pθ      pθ       cidθ  θpθ       φθ σicid cidy cid    instead looking product prior likelihood transform problem logspace solve mean covariance posterior completing squares sum logprior loglikelihood log cidy cidσy φθ σicid  log cidθ   cid φθcidy φθ   mcids   mcid  const draft  mathematics machine learning feedback smmlbookcom  bayesian linear regression constant contains terms independent  ignore constant following factorize  yields σycidφθ  θcidσφcidφθ  θcids   cidσycidy        mcid cidθcidσφcidφ   mcid cid     σφcidy    mcidθcid  const  constant contains black terms  inde pendent  orange terms terms linear  blue terms ones quadratic  inspecting  ﬁnd equation quadratic  fact unnormalized logposterior distribution negative quadratic form implies posterior gaussian ie pθ   explog pθ explog     log pθ        cid exp cidθcidσφcidφ     σφcidy    mcidθcidcid  expression remaining task bring unnormalized gaussian cid ie need identi mn  sn form proportional mean mn covariance matrix sn  this use concept completing squares desired logposterior cidθ log cidθ mn  sn cid  cidθcids   mn cids      mcid mcid      mn cid  mn   const completing squares here factorized quadratic form  mn  term quadratic  blue term linear  orange constant term black allows ﬁnd sn mn matching colored expressions   yields   mn cids pθ       cidmn  sn holds θmap  mn  cid   φcidσiφ   sn  σφcidφ         σφcidy   mcid mn  sn σφcidy    mcid       deisenroth   faisal   ong published cambridge university press  linear regression remark general approach completing squares given equation xcidax acidx  const  symmetric positive deﬁnite wish bring form µcidσx   const  setting const  const µcidσµ terms inside exponential  form       σa  σφcidφ    σφcidy       difﬁcult identi equations like  of helpful bring equations form  decouples quadratic term linear terms constants simpliﬁes ﬁnding desired solution  posterior predictions  computed predictive distribution  test input  parameter prior pθ principle predicting pa rameter posterior pθ  fundamentally different given conjugate model prior posterior gaussian with different parameters therefore following reasoning section  obtain posterior predictive distribution   py    py  θpθ dθ   cidy   φcidxθ σcid cidθ φcidxmn  φcidxsn φx  σcid   mn  sn ciddθ cid cid cidy term φcidxsn φx reﬂects posterior uncertainty associated parameters  note sn depends training inputs   predictive mean φcidxmn coincides predictions map estimate θmap draft  mathematics machine learning feedback smmlbookcom ey       φcidxmn  φcidxθmap  bayesian linear regression   remark marginal likelihood posterior predictive distribution replacing integral  predictive distribution equiv alently written expectation eθ   ypy   expec tation taken respect parameter posterior pθ writing posterior predictive distribution way highlights close resemblance marginal likelihood  key difference marginal likelihood posterior predictive distribution  marginal likelihood thought predicting training targets  test targets  ii marginal likelihood av erages respect parameter prior parameter poste rior remark mean variance noisefree function values cases interested predictive distribution py   noisy observation  instead like obtain distribu tion noisefree function values    φcidxθ determine corresponding moments exploiting properties means variances yields   ef    vθf        eθφcidxθ   φcidxeθθ  φcidxmn  mcid  φx    vθφcidxθ  φcidxvθθ  φcidxsn φx  φx       predictive mean predictive mean noisy observations noise mean  predictive variance differs  variance measurement noise predict noisy function values need include  source uncertainty term needed noisefree predictions here remaining uncertainty stems parameter posterior remark distribution functions fact integrate parameters  induces distribution functions sample θi pθ   alization θcid values eθf  marginal variance ie variance function   φcid  parameter posterior obtain single function re  mean function ie set expected function mean function  distribution functions mcid  given parameters induces distribution functions   sn   integrating example  posterior functions let revisit bayesian linear regression problem polynomials  icid figure  degree  choose parameter prior pθ  visualizes prior functions induced parameter prior sample functions prior cid     deisenroth   faisal   ong published cambridge university press  figure  bayesian linear regression posterior functions  training data  posterior distribution functions  samples posterior functions linear regression figure  shows posterior functions obtain bayesian linear regression training dataset shown panel  panel  shows posterior distribution functions including functions obtain maximum likelihood map estimation function obtain map estimate corresponds posterior mean function bayesian linear regression setting panel  shows plausible realizations samples functions pos terior functions  training data  samples posterior functions in duced samples parameter posterior  posterior functions rep resented marginal uncer tainties shaded showing   predictive con ﬁdence bounds maximum likelihood estimate mle map estimate map identical posterior mean function figure  shows posterior distributions functions induced parameter posterior different polynomial degrees   left panels maximum likelihood function θcid  map func tion θcid  which identical posterior mean function   predictive conﬁdence bounds obtained bayesian linear regression represented shaded areas mapφ mlφ right panels samples posterior functions here sampled parameters θi parameter posterior computed function φcidxθi single realization function posterior distribution functions loworder polynomials parameter posterior allow parameters vary much sampled functions nearly identical model ﬂexible adding parameters ie end higherorder polynomial parameters sufﬁciently constrained pos terior sampled functions easily visually separated corresponding panels left uncertainty increases especially boundaries seventhorder polynomial map estimate yields rea sonable ﬁt bayesian linear regression model additionally tells draft  mathematics machine learning feedback smmlbookcom xyxytrainingdatamlemapblrxy  bayesian linear regression  posterior distribution polynomials degree    left samples pos terior functions right  posterior distribution polynomials degree    left samples posterior functions right figure  bayesian linear regression left panels shaded areas indicate  dark gray  light gray predictive conﬁdence bounds mean bayesian linear regression model coincides map estimate predictive uncertainty sum noise term posterior parameter uncertainty depends location test input right panels sampled functions posterior distribution  posterior distribution polynomials degree    left samples pos terior functions right posterior uncertainty huge information critical use predictions decisionmaking system bad deci sions signiﬁcannot consequences eg reinforcement learning robotics    deisenroth   faisal   ong published cambridge university press  xytrainingdatamlemapblrxyxytrainingdatamlemapblrxyxytrainingdatamlemapblrxy marginal likelihood interpreted expected likelihood prior ie eθpy     linear regression  computing marginal likelihood section  highlighted importance marginal likelihood bayesian model selection following compute marginal likelihood bayesian linear regression conjugate gaussian prior parameters ie exactly setting discussing recap consider following generative process cid   σcid           marginal likelihood given cidm  cidxcid xn      yn       θpθdθ cid cid    cidy xθ σicid cidθ   ciddθ  integrate model parameters  compute marginal likelihood steps first marginal likelihood gaussian as distribution  second compute mean co variance gaussian  marginal likelihood gaussian section  know  product gaussian random variables unnormalized gaussian distribution ii linear transformation gaussian random variable gaussian distributed  require linear  σcid transformation bring   done integral solved closed form result normalizing constant product gaus sians normalizing constant gaussian shape   mean covariance compute mean covariance matrix marginal likelihood exploiting standard results means covariances afﬁne transformations random variables sec tion  mean marginal likelihood computed xθ σicid form cidy cidθ      eθcidxθ  cid  xeθθ  xm  cid σicid vector iid random variables note cid   covariance matrix given cov yx   covθcidxθ  cid  covθxθ  σi   covθθx cid  σi  xsx cid  σi  hence marginal likelihood  detxsx cid  σi       exp cid   xmcidxsx cid  σiy xmcid draft  mathematics machine learning feedback smmlbookcom  maximum likelihood orthogonal projection figure  geometric interpretation squares  dataset  maximum likelihood solution interpreted projection  regression dataset consisting noisy ob servations yn blue function values  xn input locations xn  orange dots projections noisy observations blue dots line θmlx maximum likelihood solution linear regression problem ﬁnds subspace line overall projection er ror orange lines observations mini mized cidy xm xsx cid  σicid  given close connection posterior predictive distribution see remark marginal likelihood posterior predictive distribution ear lier section functional form marginal likelihood surprising  maximum likelihood orthogonal projection having crunched algebra derive maximum likelihood map estimates provide geometric interpretation maximum likelihood estimation let consider simple linear regression setting   xθ  cid cid cid σcid    consider linear functions    origin we omit features clarity parameter  determines slope line figure  shows onedimensional dataset training data set recall results section  obtain maximum likelihood estimator slope parameter       xn  yn  θml   cidxx cidy   cidy  cidx  right         yn cid right         xn cid means training inputs  obtain optimal maximum likelihood reconstruction training targets xθml    cidy  cidx xx cid  cidx      deisenroth   faisal   ong published cambridge university press  xyxyprojectionobservationsmaximumlikelihoodestimate linear regression thought method solving systems linear equations maximum likelihood linear regression performs orthogonal projection linear regression ie obtain approximation minimum leastsquares error  xθ looking solution   xθ think linear regression problem solving systems linear equations there fore relate concepts linear algebra analytic geometry discussed    particular looking carefully  maximum likelihood estimator θml ex ample  effectively orthogonal projection  onedimensional subspace spanned  recalling results or thogonal projections section  identi xxcid xcidx projection matrix θml coordinates projection onedimensional subspace right spanned  xθml orthogonal projection  subspace therefore maximum likelihood solution provides geometri cally optimal solution ﬁnding vectors subspace spanned  closest corresponding observations  clos est means smallest squared distance function values yn xnθ achieved orthogonal projections figure  shows projection noisy observations subspace minimizes squared distance original dataset projection note xcoordinate ﬁxed corresponds maximum likelihood solution general linear regression case   φcidxθ  cid cid cid σcid   rk interpret maxi vectorvalued features φx mum likelihood result φθml  θml  φcidφφcidy projection kdimensional subspace right  spanned columns feature matrix  section  feature functions φk use construct feature ma trix  orthonormal see section  obtain special case columns  form orthonormal basis see section  φcidφ   lead projection cid  cid cid φφcidφφcidy  φφcidy  φkφcid maximum likelihood projection simply sum projections  individual basis vectors φk ie columns  further more coupling different features disappeared orthogonality basis popular basis functions signal process ing wavelets fourier bases orthogonal basis functions draft  mathematics machine learning feedback smmlbookcom  reading basis orthogonal convert set linearly inde pendent basis functions orthogonal basis gramschmidt process section  strang   reading classiﬁcation  discussed linear regression gaussian likelihoods conjugate gaussian priors parameters model al lowed closedform bayesian inference however applications want choose different likelihood function example binary classiﬁcation setting observe possible categorical outcomes gaussian likelihood inappropriate setting in stead choose bernoulli likelihood return probability predicted label  or  refer books barber  bishop  murphy  indepth introduction classiﬁ cation problems different example nongaussian likelihoods important count data counts nonnegative integers case binomial poisson likelihood better choice gaussian examples fall category generalized linear models ﬂex ible generalization linear regression allows response variables error distributions gaussian distribution glm generalized linear generalizes linear regression allowing linear model related observed values smooth invertible function  nonlinear   σf     θcidφx linear regression model  think generalized linear model terms function composition       linear regression model  activation function note talking generalized linear models outputs  longer linear parameters  logistic regression choose   interpreted logistic sigmoid σf   probability observing    bernoulli random variable    function   called transfer function activation function inverse called canonical link function perspective clear generalized linear models building blocks deep feedforward neural networks consider generalized linear model   σax   weight matrix  bias vector iden ti generalized linear model singlelayer neural network activation function   recursively compose functions transfer function activation function canonical link function ordinary linear regression activation function simply identity models building blocks deep neural networks generalized linear model expf   logistic regression logistic sigmoid   xk   kxk  kxk  σkakxk  bk           input features xk     klayer deep neural observed outputs        network therefore building blocks deep neural network    deisenroth   faisal   ong published cambridge university press  great post relation glms deep networks available stinyurl comglmdnn linear regression generalized linear models deﬁned  neural networks bishop  goodfellow et al  signiﬁcantly expressive ﬂexi ble linear regression models however maximum likelihood parame ter estimation nonconvex optimization problem marginalization parameters fully bayesian setting analytically intractable brieﬂy hinted fact distribution parameters in duces distribution regression functions gaussian processes ras mussen williams  regression models concept distribution function central instead placing distribution parameters gaussian process places distribution directly space functions detour parameters so gaussian process exploits kernel trick scholkopf smola  allows compute inner products function values  xi  xj looking corresponding input xi xj gaus sian process closely related bayesian linear regression sup port vector regression interpreted bayesian neural network single hidden layer number units tends inﬁnity neal  williams  excellent introductions gaussian processes mackay  rasmussen williams cid rd training set small size  focused gaussian parameter priors discussions chap ter allow closedform inference linear regression mod els however regression setting gaussian likelihoods choose nongaussian prior consider setting inputs  means regression problem underdetermined case choose parameter prior enforces sparsity ie prior tries set parameters  possible variable selection prior provides stronger regularizer gaussian prior leads in creased prediction accuracy interpretability model laplace prior example frequently purpose linear re gression model laplace prior parameters equivalent linear regression  regularization lasso tibshirani  laplace distribution sharply peaked zero its ﬁrst derivative discon tinuous concentrates probability mass closer zero gaussian distribution encourages parameters  therefore nonzero parameters relevant regression problem reason speak variable selection gaussian process kernel trick variable selection lasso draft  mathematics machine learning feedback smmlbookcom dimensionality reduction principal component analysis working directly highdimensional data images comes difﬁculties hard analyze interpretation difﬁcult visualiza tion nearly impossible from practical point view storage data vectors expensive however highdimensional data properties exploit example highdimensional data overcomplete ie dimensions redundant ex plained combination dimensions furthermore dimensions highdimensional data correlated data possesses intrinsic lowerdimensional structure dimensionality reduction exploits structure correlation allows work compact rep resentation data ideally losing information think dimensionality reduction compression technique similar jpeg mp compression algorithms images music  discuss principal component analysis pca algorithm linear dimensionality reduction pca proposed pearson  hotelling   years commonly techniques data compres sion data visualization identiﬁcation simple patterns latent factors structures highdimensional data  dataset   coordinates  compressed dataset  coor dinate relevant material published cambridge university press mathematics machine learning marc peter deisenroth  aldo faisal cheng soon ong  version free view download personal use only redistribution resale use derivative works by   deisenroth   faisal   ong  smmlbookcom    pixel color image data point milliondimensional space pixel responds dimensions color channel red green blue principal component analysis pca dimensionality reduction figure  illustration dimensionality reduction  original dataset vary direction  data  represented xcoordinate nearly loss xxxx karhunenloeve transform dimensionality reduction principal component analysis signal processing community pca known karhunenloeve transform  derive pca ﬁrst principles drawing understanding basis basis change sections   projections section  eigenvalues section  gaussian distribu tions section  constrained optimization section  dimensionality reduction generally exploits property highdimen sional data eg images lies lowdimensional subspace figure  gives illustrative example dimensions data figure  lie line data vary xdirection express line  nearly loss figure  data figure  xcoordinate required data lies onedimensional subspace   problem setting pca interested ﬁnding projections xn data points xn similar original data points possible sig niﬁcantly lower intrinsic dimensionality figure  gives illustration look like concretely consider iid dataset      xn rd mean  possesses data covariance matrix   xn data covariance matrix furthermore assume exists lowdimensional compressed rep resentation code   xnxcid   cid zn  bcidxn rm xn deﬁne projection matrix        bm  rdm  columns      bm  form basis  dimensional subspace projected data   bbcidx  rd live   bcid assume columns  orthonormal deﬁnition  bcid bi   seek  dimensional bj   rd dimyou      project data subspace  coordinates with respect denote projected data xn basis vectors      bm  zn aim ﬁnd projections rd or equivalently codes zn basis vectors      bm  xn similar original data xn minimize loss compression example  coordinate representationcode consider  canonical basis    cid    cid draft  mathematics machine learning feedback smmlbookcom cid  problem setting original rd reconstructed rd compressed rm figure  graphical illustration pca pca ﬁnd compressed version  original data  compressed data reconstructed  lives original data space intrinsic lowerdimensional representation  represented linear combina   know  tion basis vectors eg cid cid      however consider vectors form   cid cid   written   ze represent vectors sufﬁcient rememberstore coordinatecode   respect  vector precisely set  vectors with standard vector addition scalar multiplication forms vector subspace see section  dimyou     spane dimension vector space corresponds number basis vectors see section  section  ﬁnd lowdimensional representations re tain information possible minimize compression loss alternative derivation pca given section   be looking minimizing squared reconstruction error tween original data xn projection xn xn cid xn cid figure  illustrates setting consider pca  repre sents lowerdimensional representation compressed data  plays role bottleneck controls information ﬂow   pca consider linear relationship original data  lowdimensional code    bcidx   bz suitable matrix  based motivation thinking pca data compression technique interpret arrows figure  pair operations representing encoders decoders linear mapping represented  thought decoder rm original data maps lowdimensional code  space rd similarly bcid thought encoder encodes original data  lowdimensional compressed code   use mnist digits dataset re    deisenroth   faisal   ong published cambridge university press  figure  examples handwritten digits mnist dataset  yannlecun comexdbmnist dimensionality reduction principal component analysis occurring example contains  examples handwritten digits   digit grayscale image size   ie contains  pixels interpret image dataset vector  examples digits shown figure   maximum variance perspective figure  gave example twodimensional dataset represented single coordinate figure  chose ig nore xcoordinate data add in formation compressed data similar original data figure  chosen ignore xcoordinate compressed data dissimilar original data information data lost interpret information content data space ﬁlling dataset is information contained data looking spread data section  know variance indicator spread data derive pca dimensionality reduction algorithm maximizes variance lowdimensional representation data retain information possible figure  illustrates this considering setting discussed section  aim ﬁnd matrix  see  retains information possible compressing data projecting subspace spanned columns      bm  retaining information data com pression equivalent capturing largest variance lowdimensional code hotelling  remark centered data data covariance matrix  assumed centered data assumption loss gen erality let assume  mean data properties variance discussed section  obtain vzz  vxbcidx   vxbcidx bcidµ  vxbcidx  ie variance lowdimensional code depend mean data therefore assume loss generality data mean  remainder section assumption mean lowdimensional code  ezz  exbcidx  bcidexx   draft  mathematics machine learning feedback smmlbookcom  maximum variance perspective  direction maximal variance maximize variance lowdimensional code sequential rd maximizes approach start seeking single vector  variance projected data ie aim maximize variance ﬁrst coordinate   rm   vz  cid maximized exploited iid assumption data deﬁned zn ﬁrst coordinate lowdimensional representation zn rd note ﬁrst component zn given rm xn zn  bcid  xn  ie coordinate orthogonal projection xn one dimensional subspace spanned  section  substitute   yields   bcid  xn  bcid  xnxcid   cid cid cid cid cid  bcid xnxcid   bcid  sb   data covariance matrix deﬁned   fact dot product vectors symmetric respect arguments is bcid  xn  xcid notice arbitrarily increasing magnitude vector  in creases  is vector  times longer result  potentially times larger therefore restrict solutions    results constrained optimization problem cid cid seek direction data varies most   restriction solution space unit vectors vector  points direction maximum variance    deisenroth   faisal   ong published cambridge university press  figure  pca ﬁnds lowerdimensional subspace line maintains variance spread data possible data blue projected subspace orange vector  ﬁrst column matrix  ﬁrst  orthonormal basis vectors span lowerdimensional subspace cidbcid    cidbcid   dimensionality reduction principal component analysis constrained optimization problem max bcid  sb subject     cid cid following section  obtain lagrangian lb   bcid  sb   bcid   solve constrained optimization problem partial derivatives  respect    bcid   λbcid     bcid    respectively setting partial derivatives  gives relations sb  λb  bcid      comparing deﬁnition eigenvalue decomposition section   eigenvector data covariance matrix  lagrange multiplier  plays role correspond ing eigenvalue eigenvector property  allows rewrite variance objective    bcid  sb  λbcid      ie variance data projected onedimensional subspace equals eigenvalue associated basis vector  spans subspace therefore maximize variance lowdimensional code choose basis vector associated largest eigenvalue data covariance matrix eigenvector called ﬁrst principal component determine effectcontribution principal com ponent  original data space mapping coordinate zn data space gives projected data point xn  bzn  bbcid  xn rd original data space remark xn ddimensional vector requires single coordinate zn represent respect basis vector  rd   dimensional subspace maximal variance assume ﬁrst  eigenvectors  associated largest   eigenvalues  symmetric spectral theorem theorem  states use eigenvectors construct orthonormal eigenbasis  principal components  draft  mathematics machine learning feedback smmlbookcom quantity called loading unit vector  represents standard deviation data accounted principal subspace spanb principal component matrix ˆx  ˆx     ˆxn   rdn  contains information data compressed  maximum variance perspective dimensional subspace rd generally mth principal com ponent subtracting effect ﬁrst   principal components      bm data trying ﬁnd principal components compress remaining information arrive new data matrix ˆx   bibcid    bmx  cid rdn contains data points column projection matrix projects        xn  vectors bm  cidm  bibcid subspace spanned      bm remark notation  follow con vention collecting data      xn rows data matrix deﬁne columns  means data ma  matrix  matrix instead conventional  trix   reason choice algebra operations work smoothly need transpose matrix redeﬁne vectors row vectors leftmultiplied matrices ﬁnd mth principal component maximize variance vm  vzm  mn  bcid  ˆxn  bcid ˆsbm  cid cid bm cid    followed steps  subject cid deﬁned ˆs data covariance matrix transformed dataset  previously looked ﬁrst principal component alone solve constrained optimization problem dis cover optimal solution bm eigenvector ˆs associated largest eigenvalue ˆs ˆx     ˆxn turns bm eigenvector  generally sets eigenvectors  ˆs identical  ˆs sym metric ﬁnd onb eigenvectors spectral theorem  ie exist  distinct eigenvectors  ˆs next eigenvector  eigenvector ˆs assume eigenvectors      bm ˆs consider eigenvector bi  ie sbi  λibi general ˆsbi  cid ˆx ˆx bi    bmxx bmxcidbi sbm bms  bmsbmbi  distinguish cases cid  ie bi eigenvector  principal components bi orthogo  principal components bmbi     ie  principal components bi basis vector ﬁrst  nal ﬁrst  bi ﬁrst     deisenroth   faisal   ong published cambridge university press  dimensionality reduction principal component analysis principal subspace bm projects      bm onb principal subspace obtain bmbi  bi cases summarized follows bmbi  bi    bmbi   cid   case cid    obtain ˆsbi   bmsbi  sbi  λibi  ie bi eigenvector ˆs eigen value λi speciﬁcally ˆsbm  sbm  λmbm  equation  reveals bm eigenvector  ˆs speciﬁcally λm largest eigenvalue ˆs λm mth largest eigenvalue  associated eigenvector bm case     obtain ˆsbi   bms  bmsbmbi    bi sbm means      bm eigenvectors ˆs as sociated eigenvalue       bm span null space ˆs overall eigenvector  eigenvector ˆs however  dimensional principal eigenvectors   subspace associated eigenvalue ˆs  relation  bcid mbm   variance data pro jected mth principal component vm  bcid msbm  λmbcid mbm  λm  means variance data projected   dimensional subspace equals sum eigenvalues associ ated corresponding eigenvectors data covariance matrix example  eigenvalues mnist   eigenvalues sorted descending order data covariance matrix digits  mnist training set  variance captured principal compo nents draft  mathematics machine learning feedback smmlbookcom derivation shows intimate connection  dimensional subspace maximal variance eigenvalue decomposition revisit connection section  figure  properties training data mnist   eigenvalues sorted descending order  variance captured principal components associated largest eigenvalues indexeigenvaluenumberofprincipalcomponentscapturedvariance  projection perspective figure  illustration projection approach subspace line minimizes length difference vector projected orange original blue data taking digits  mnist training data compute eigen values data covariance matrix figure  shows  largest eigenvalues data covariance matrix value differs signiﬁcantly  therefore variance projecting data subspace spanned cor responding eigenvectors captured principal components shown figure  overall ﬁnd  dimensional subspace rd retains information possible pca tells choose columns matrix    eigenvectors data covariance matrix  associated  largest eigenvalues maximum variance pca capture ﬁrst  principal components vm  λm  cid λm  largest eigenvalues data covariance matrix  consequently variance lost data compression pca jm  λj  vd vm  cid jm  instead absolute quantities deﬁne relative variance captured vm vd  relative variance lost compression  vm vd  projection perspective following derive pca algorithm directly mini mizes average reconstruction error perspective allows in terpret pca implementing optimal linear autoencoder draw heavily    previous section derived pca maximizing variance projected space retain information possible    deisenroth   faisal   ong published cambridge university press  figure  simpliﬁed projection setting  vector    red cross shall projected onedimensional subspace   spanned   shows difference vectors  candidates  vectors   vectors plane  dimensionality plane  vectors coordinates respect standard basis dimensionality reduction principal component analysis  setting  differences   xi  different xi shown red lines following look difference vectors original data xn reconstruction xn minimize distance xn xn close possible figure  illustrates setting  setting objective assume ordered orthonormal basis onb        bd rd ie bcid bj      otherwise section  know basis      bd rd  rd written linear combination basis vectors rd ie   ζdbd  ζmbm  ζjbj cid cid cid jm  suitable coordinates ζd interested ﬁnding vectors  rd live lower rd dimyou     dimensional subspace   cid zmbm rd similar  possible note point need assume coordinates zm  ζm  identical following use exactly kind representation  ﬁnd optimal coordinates  basis vectors      bm  sim ilar original data point  possible ie aim minimize euclidean distance  figure  illustrates setting cid loss generality assume dataset cid      xn    zeromean assump rd centered  ie  xn draft  mathematics machine learning feedback smmlbookcom xxbuxxbu  projection perspective tion arrive exactly solution notation substantially cluttered interested ﬁnding best linear projection lower dimensional subspace rd dimyou    orthonormal basis vectors      bm  subspace principal subspace projections data points denoted xn  zmnbm  bzn rd  cid principal subspace rm coordinate vector xn zn  zn     zm ncid respect basis      bm  speciﬁcally interested having xn similar xn possible similarity measure use following squared distance    deﬁne ob euclidean norm jective minimizing average squared euclidean distance reconstruction reconstruction error error pearson  cid cid jm  cid  cid xn xn   cid explicit dimension subspace project data   order ﬁnd optimal linear projection need ﬁnd orthonormal basis principal subspace coordinates zn rm projections respect basis ﬁnd coordinates zn onb principal subspace follow twostep approach first optimize coordinates zn given onb      bm  second ﬁnd optimal onb  finding optimal coordinates let start ﬁnding optimal coordinates zn     zm  projec tions xn          consider figure  principal subspace spanned single vector  geometrically speaking ﬁnding optimal coordinates  corresponds ﬁnding representation linear projection  respect  minimizes distance  figure  clear orthogonal projection following exactly this assume onb      bm  rd ﬁnd optimal co ordinates zm respect basis require partial derivatives  xn zin jm zin jm  xn jm  xn xn xncid rd     deisenroth   faisal   ong published cambridge university press  figure  optimal projection vector    onedimensional subspace continuation figure   distances cidx  xcid     orthogonal projection optimal coordinates coordinates optimal projection xn respect basis vectors      bm coordinates orthogonal projection xn principal subspace dimensionality reduction principal component analysis  distances cidx  xcid   zb   spanb panel  setting  vector  minimizes distance panel  orthogonal projection  coordinate projection  respect basis vector  spans factor need scale  order reach   xn zin zin cid  cid cid zmnbm  bi         obtain jm zin xn xncidbi cid xn cid cidcid zmnbm bi onb xcid  bi zinbcid bi  xcid  bi zin  bcid optimal coordinates bi   setting partial derivative  yields immediately zin  xcid  bi  bcid xn                 means optimal co ordinates zin projection xn coordinates orthogonal projection see section  original data point xn one dimensional subspace spanned bi consequently optimal linear projection xn xn orthogonal projection coordinates xn respect basis      bm  coordinates orthogonal projection xn principal sub space orthogonal projection best linear mapping given objec tive  coordinates ζm   coordinates zm   draft  mathematics machine learning feedback smmlbookcom xkxxkxxbux bcid   coordinate orthogonal projection  subspace spanned bj   projection perspective identical           spanbm      bd orthogonal complement see section   spanb     bm  remark orthogonal projections orthonormal basis vectors let brieﬂy recap orthogonal projections section       bd orthonormal basis rd   bjbcid  bjbcid    bjbcid   rd orthogonal projection  subspace spanned jth ba sis vector zj  bcid   coordinate projection respect basis vector bj spans subspace zjbj   figure  illustrates setting generally aim project  dimensional subspace rd obtain orthogonal projection   dimensional subspace orthonormal basis vectors      bm   bbcidb cid cidcid cid bcidx  bbcidx  rdm  coordinates deﬁned        bm  projection respect ordered basis      bm    bcidx discussed section  think coordinates representation projected vector new coordinate deﬁned      bm  note al rd need  coordinates      zm represent vector   coordinates respect basis vectors bm      bd  far shown given onb ﬁnd optimal coordinates  orthogonal projection principal subspace following determine best basis is  finding basis principal subspace determine basis vectors      bm principal subspace rephrase loss function  results far easier ﬁnd basis vectors reformulate loss func tion exploit results obtain xn  zmnbm xcid  bmbm  cid exploit symmetry dot product yields cid cid xn  bmbcid xn  cid  cid    deisenroth   faisal   ong published cambridge university press  dimensionality reduction principal component analysis figure  orthogonal projection displacement vectors projecting data points xn blue subspace  obtain xn orange displacement vector xn  xn lies completely orthogonal complement  generally write original data point xn linear combi nation basis vectors holds xn  zdnbd xcid  bdbd  cid cid bdbcid xn cid  cid cid cid bmbcid xn  cid  cid jm  bjbcid xn  cid cid  cid split sum  terms sum  sum  terms result ﬁnd displacement vector xn ie difference vector original data point xn projection xn xn  cid  cid cid bjbcid xn jm  cid jm  xcid  bjbj  means difference exactly projection data point orthogonal complement principal subspace identi ma trix cidd   projection matrix performs xn lies subspace projection displacement vector xn orthogonal principal subspace illustrated figure  jm  bjbcid remark lowrank approximation  saw projec tion matrix projects   given cid bmbcid   bbcid  construction sum rankone matrices bmbcid  bbcid draft  mathematics machine learning feedback smmlbookcom xxuu  projection perspective symmetric rank   therefore average squared reconstruction error written xn cid  cid cid xn   cid cid cid cid cidxn bbcidxn cid cid cid cid cid cidi bbcidxn cid cid cid finding orthonormal basis vectors      bm  minimize differ ence original data xn projections xn equivalent ﬁnding best rankm approximation bbcid identity matrix see section  pca ﬁnds best rankm approximation identity matrix tools reformulate loss function  jm  cid  cid xn xn   cid cid cid jm  cid cid cid cid cid cid cid cid cid cid bcid  xnbj explicitly compute squared norm exploit fact bj form onb yields jm  cid cid jm  cid cid jm  bcid  xn   xnbcid bcid  xn cid cid jm  bcid  xnxcid  bj  exploited symmetry dot product step write bcid  bj swap sums obtain  xn  xcid xnxcid bj  bcid  sbj jm  cid bcid jm  cid cid cid cidcid cid cid cid jm  cid jm  trbcid  sbj  trsbjbcid    tr cid jm  cid cid  cid bjbcid cid cid jm  cidcid projection matrix cid cid  see  exploited property trace operator tr linear invariant cyclic permutations arguments assumed dataset centered ie     identi  data covariance matrix projection matrix  con structed sum rankone matrices bjbcid  rank    equation  implies formulate average squared reconstruction error equivalently covariance matrix data    deisenroth   faisal   ong published cambridge university press  minimizing average squared reconstruction error equivalent minimizing projection data covariance matrix orthogonal complement principal subspace minimizing average squared reconstruction error equivalent maximizing variance projected data figure  embedding mnist digits  blue  orange twodimensional principal subspace pca embeddings digits   principal subspace highlighted red corresponding original digit dimensionality reduction principal component analysis projected orthogonal complement principal subspace min imizing average squared reconstruction error equivalent minimizing variance data projected subspace ignore ie orthogonal complement principal subspace equiva lently maximize variance projection retain principal subspace links projection loss immediately maximumvariance formulation pca discussed section  means obtain solution obtained maximumvariance perspective therefore omit derivation identical presented section  summarize results earlier light projection perspective average squared reconstruction error projecting   dimensional principal subspace jm  λj  cid jm  λj eigenvalues data covariance matrix therefore minimize  need select smallest   eigenvalues implies corresponding eigenvectors basis orthogonal complement principal subspace consequently means basis principal subspace comprises eigenvectors      bm associated largest  eigenvalues data covariance matrix example  mnist digits embedding figure  visualizes training data mmist digits   embedded vector subspace spanned ﬁrst principal com ponents observe relatively clear separation  blue dots  orange dots variation individual draft  mathematics machine learning feedback smmlbookcom  eigenvector computation lowrank approximations cluster embeddings digits   principal subspace highlighted red corresponding original digit ﬁgure reveals variation set  signiﬁcantly greater variation set   eigenvector computation lowrank approximations previous sections obtained basis principal subspace eigenvectors associated largest eigenvalues data covariance matrix   xnxcid   cid        xn  xx cid  rdn  note    matrix ie transpose typical data matrix bishop  murphy  eigenvalues and corresponding eigenvectors  follow approaches perform eigendecomposition see section  compute eigenvalues eigenvectors  directly use singular value decomposition see section   symmetric factorizes xx cid ignoring factor    eigen values  squared singular values  speciﬁcally svd  given cidcidcidcid dn cidcidcidcid dd cidcidcidcid dn  cid cidcidcidcid   rdd  cid right  orthogonal matrices  rdn matrix nonzero entries singular values σii cid  follows σςcidyou cid  σcidyou cid  xx cid      cidv cid cidcid cid   use eigendecomposition svd compute eigenvectors results section  columns eigenvectors xx cid and  furthermore eigenvalues λd  related singular values  columns eigenvectors λd  relationship eigenvalues  singular values  provides connection maximum variance view sec tion  singular value decomposition    deisenroth   faisal   ong published cambridge university press  dimensionality reduction principal component analysis  pca lowrank matrix approximations maximize variance projected data or minimize average squared reconstruction error pca chooses columns  eigenvectors associated  largest eigenvalues data covariance matrix  identi projection ma trix   projects original data lowerdimensional subspace dimension   eckartyoung theorem theorem  section  offers direct way estimate lowdimensional represen tation consider best rankm approximation    argminrkacidm cid cidcid spectral norm deﬁned  eckartyoung theorem states   given truncating svd topm singular value words obtain rdn cid      cidcidcidcid dm σm cidcidcidcid    cid cidcidcidcid   rdn orthogonal matrices        um  right  diagonal matrix σm      vm  nal entries  largest singular values  rdm    rm  diago  practical aspects finding eigenvalues eigenvectors important funda mental machine learning methods require matrix decompositions theory discussed section  solve eigenvalues roots characteristic polynomial however matrices larger  possible need ﬁnd roots poly nomial degree  higher however abelrufﬁni theorem rufﬁni  abel  states exists algebraic solution problem polynomials degree  more therefore practice solve eigenvalues singular values iterative methods implemented modern packages linear algebra applications such pca presented  require eigenvectors wasteful compute de composition discard eigenvectors eigenvalues ﬁrst few turns interested ﬁrst eigenvectors with largest eigenvalues iterative processes directly optimize eigenvectors computationally efﬁ cient eigendecomposition or svd extreme case needing ﬁrst eigenvector simple method called power iteration efﬁcient power iteration chooses random vector  draft  mathematics machine learning feedback smmlbookcom eckartyoung theorem abelrufﬁni theorem nplinalgeigh nplinalgsvd power iteration  pca high dimensions null space  follows iteration xk          sxk sxk cid cid means vector xk multiplied  iteration   sequence vectors con xk normalized ie cid verges eigenvector associated largest eigenvalue  original google rank algorithm  et al  uses al gorithm ranking web  based hyperlinks cid  invertible sufﬁcient ensure  cid   pca high dimensions order pca need compute data covariance matrix  dimensions data covariance matrix   matrix computing eigenvalues eigenvectors matrix computationally expensive scales cubically  therefore pca discussed earlier infeasible high dimensions example xn images  pixels eg   pixel images need compute eigendecomposition   covariance matrix following provide solution problem case substantially fewer data points dimensions ie  assume centered dataset      xn  xn rd cid data covariance matrix given        xn   points   xx cid rdd   matrix columns data cid assume   ie number data points smaller dimensionality data duplicate data points rank covariance matrix        eigen values  intuitively means redundancies following exploit turn   covariance matrix  covariance matrix eigenvalues positive pca ended eigenvector equation sbm  λmbm           bm basis vector principal subspace let rewrite equation bit  deﬁned  obtain sbm  xx cidbm  λmbm  multiply  cid right  lefthand side yields  cidx cid cidcid cid    cidbm cid cidcid cid cm  λmx cidbm  cidxcm  λmcm     deisenroth   faisal   ong published cambridge university press  dimensionality reduction principal component analysis new eigenvectoreigenvalue equation λm remains eigen value conﬁrms results section  nonzero eigenvalues xx cid equal nonzero eigenvalues  cidx obtain   cidx right  associated λm eigenvector matrix  cm   cidbm assuming duplicate data points matrix   cidx rank  invertible implies  nonzero eigenvalues data covariance matrix   matrix compute eigenvalues eigenvectors efﬁciently original   data covariance matrix   cidx going re eigenvectors  cover original eigenvectors need pca currently   cidx leftmultiply eigenvalue know eigenvectors  eigenvector equation  cid xx cid cid cidcid xcm  λmxcm recover data covariance matrix again means recover xcm eigenvector  remark want apply pca algorithm discussed sec tion  need normalize eigenvectors xcm  norm   key steps pca practice following individual steps pca running example summarized figure  given twodimensional dataset figure  want use pca project onedimensional subspace  mean subtraction start centering data computing mean  dataset subtracting single data point ensures dataset mean  figure  mean sub traction strictly necessary reduces risk numerical prob lems  standardization divide data points standard deviation σd dataset dimension         data unit free variance  axis indicated arrows figure  step completes standardization data  eigendecomposition covariance matrix compute data covariance matrix eigenvalues corresponding eigenvectors covariance matrix symmetric spectral theorem the orem  states ﬁnd onb eigenvectors fig ure  eigenvectors scaled magnitude cor draft  mathematics machine learning feedback smmlbookcom standardization  key steps pca practice  original dataset  step  centering sub tracting mean data point  step  dividing standard deviation data unit free data variance  axis figure  steps pca  original dataset  centering  divide standard deviation  eigendecomposi tion  projection  mapping original data space  step  compute eigenval ues eigenvectors arrows data covariance matrix ellipse  step  project data principal subspace  undo standardization projected data original data space responding eigenvalue longer vector spans principal subspace denote  data covariance matrix represented ellipse  projection project data point  rd principal subspace right need standardize  mean µd standard deviation σd training data dth dimension respectively xd   xd µd σd          xd dth component  obtain projection coordinates   bbcidx   bcidx respect basis principal subspace here  ma trix contains eigenvectors associated largest eigenvalues data covariance matrix columns pca returns coordinates  projections     deisenroth   faisal   ong published cambridge university press  xxxxxxxxxxxx  dimensionality reduction principal component analysis having standardized dataset  yields projections context standardized dataset obtain projection original data space ie standardization need undo standardization  multiply standard deviation adding mean obtain xd   xd  σd  µd           figure  illustrates projection original data space example  mnist digits reconstruction following apply pca mnist digits dataset contains  examples handwritten digits   digit  ie contains  pixels interpret image size   examples digits image dataset vector  shown figure  figure  effect increasing number principal components reconstruction illustration purposes apply pca subset mnist digits focus digit   training images digit  determined principal subspace detailed  learned projection matrix reconstruct set test am ages illustrated figure  ﬁrst row figure  shows set original digits test set following rows reconstructions exactly digits principal sub space dimensions     respectively singledimensional principal subspace halfway decent re construction original digits which however blurry generic increasing number principal components pcs reconstruc tions sharper details accounted for  prin draft  mathematics machine learning feedback smmlbookcom originalpcspcspcspcs  latent variable perspective cipal components effectively obtain nearperfect reconstruction choose  pcs recover exact digit compression loss figure  shows average squared reconstruction error cid  cid xn xn   cid cid λi  im  function number  principal components importance principal components drops rapidly marginal gains achieved adding pcs matches exactly observation figure  discovered variance projected data captured principal compo nents  pcs essentially fully reconstruct training data contains digit  some pixels boundaries variation dataset black  latent variable perspective previous sections derived pca notion prob abilistic model maximumvariance projection perspec tives hand approach appealing allows sidestep mathematical difﬁculties come probability the ory hand probabilistic model offer ﬂex ibility useful insights speciﬁcally probabilistic model come likelihood function explicitly deal noisy observations which discuss earlier allow bayesian model comparison marginal likelihood discussed section  view pca generative model allows simulate new data    deisenroth   faisal   ong published cambridge university press  figure  average squared reconstruction error function number principal components average squared reconstruction error sum eigenvalues orthogonal complement principal subspace numberofpcsaveragesquaredreconstructionerror  dimensionality reduction principal component analysis allow straightforward connections related algorithms deal data dimensions missing random applying bayes theorem notion novelty new data point principled way extend model eg mixture pca models pca derived earlier sections special case allow fully bayesian treatment marginalizing model parameters rm possible introducing continuousvalued latent variable  phrase pca probabilistic latentvariable model tipping bishop  proposed latentvariable model probabilistic pca ppca ppca addresses aforementioned issues pca solution obtained maximizing variance projected space minimizing reconstruction error obtained special case maximum likelihood estimation noisefree setting  generative process probabilistic model ppca explicitly write probabilistic model linear di mensionality reduction assume continuous latent variable cid icid linear rela tionship latent variables observed  data rm standardnormal prior pz    bz    cid rd  cid σicid gaussian observation noise  rdm   rd linearafﬁne mapping latent observed cid variables therefore ppca links latent observed variables      px cidx bz   σicid  overall ppca induces following generative process zn zn xn   cidz cidx  icid bzn   σicid   generate data point typical given model parameters follow ancestral sampling scheme ﬁrst sample latent variable zn pz use zn  sample data point conditioned sampled zn ie xn zn    px generative process allows write probabilistic model ie joint distribution random variables section  px     σpz      px immediately gives rise graphical model figure  results section  draft  mathematics machine learning feedback smmlbookcom probabilistic pca ppca ancestral sampling  latent variable perspective zn xn         remark note direction arrow connects latent variables  observed data  arrow points   means ppca model assumes lowerdimensional latent  high dimensional observations  end obviously interested ﬁnding  given observations apply bayesian inference invert arrow implicitly observations latent variables example  generating new data latent variables figure  graphical model probabilistic pca observations xn explicitly depend corresponding latent variables zn   cid icid model parameters   likelihood parameter  shared dataset figure  generating new mnist digits latent variables  generate new data   bz closer stay training data realistic generated data figure  shows latent coordinates mnist digits  pca twodimensional principal subspace blue dots query vector  latent space generate image   bz resembles digit  generated images corresponding latent space representation depending query latent space generated images look different shape rotation size etc query away training data artifacts eg topleft topright digits note intrinsic dimensionality generated images two    deisenroth   faisal   ong published cambridge university press  dimensionality reduction principal component analysis likelihood depend latent variables   likelihood joint distribution results   obtain likelihood proba bilistic model integrating latent variable  see section  px     px    σpzdz bz   σicid cidz  iciddz  cid cid cidx section  know solution integral gaussian distribution mean exx  ezbz    ecidcid   covariance matrix vx  vzbz    vcidcid  vzbz  σi  bvzzbcid  σi  bbcid  σi  likelihood  maximum likelihood map estimation model parameters remark use conditional distribution  maxi mum likelihood estimation depends latent variables likelihood function require maximum likelihood or map estima tion function data  model parameters depend latent variables section  know gaussian random variable  linearafﬁne transformation   bz jointly gaussian dis  icid px  tributed know marginals pz  cidz cidx  bbcid  σicid missing crosscovariance given covx   covzbz     covzz     therefore probabilistic model ppca ie joint distribution latent observed random variables explicitly given cidbbcid  σi  cidcidx bcid     cid cidµ cid cid cid cid cid px  cid cid mean vector length    covariance matrix size          posterior distribution joint gaussian distribution px  determine posterior distribution pz     allows  immediately applying draft  mathematics machine learning feedback smmlbookcom  reading rules gaussian conditioning section  posterior distribu tion latent variable given observation  pz cidz  ccid      bcidbbcid  σix     bcidbbcid  σib  note posterior covariance depend observed data  new observation  data space use  determine posterior distribution corresponding latent variable  co variance matrix  allows assess conﬁdent embedding is covariance matrix  small determinant which measures volumes tells latent embedding  fairly certain obtain pos terior distribution pz  variance faced outlier however explore posterior distribution under stand data points  plausible posterior this exploit generative process underlying ppca allows explore posterior distribution latent variables generating new data plausible posterior  sample latent variable  pz latent variables   sample reconstructed vector   posterior distribution px      repeat process times explore posterior dis tribution  latent variables  implications observed data sampling process effectively hypothesizes data plausible posterior distribution  reading derived pca perspectives  maximizing variance projected space  minimizing average reconstruction error how ever pca interpreted different perspectives let recap rd done took highdimensional data  matrix bcid ﬁnd lowerdimensional representation  rm  columns  eigenvectors data covariance matrix  associated largest eigenvalues lowdimensional representation  highdimensional version in orig rd bbcid   bz  bbcidx inal data space  projection matrix think pca linear autoencoder illustrated fig ure  autoencoder encodes data xn decodes xn similar xn mapping data code called encoder mapping code orig inal data space called decoder consider linear mappings rd code zn    deisenroth   faisal   ong published cambridge university press  autoencoder rm code encoder decoder figure  pca viewed linear autoencoder encodes highdimensional data  lowerdimensional representation code   rm decodes  decoder decoded vector  orthogonal projection original data   dimensional principal subspace recognition network inference network generator code compressed version original data dimensionality reduction principal component analysis original rd code rm bcid rd encoder decoder code given zn  bcidxn rm interested minimiz ing average squared error data xn reconstruction xn  bzn          obtain cid  cid xn xn   cid cid cid cid cidxn bbcidxn cid cid cid means end objective function  discussed section  obtain pca solution minimize squared autoencoding loss replace linear map ping pca nonlinear mapping nonlinear autoencoder prominent example deep autoencoder linear func tions replaced deep neural networks context encoder known recognition network inference network decoder called generator interpretation pca related information theory think code smaller compressed version original data point reconstruct original data code exact data point back slightly distorted noisy version it means compression lossy intuitively want maximize correlation original data lower dimensional code formally related mutual information solution pca discussed section  maximizing mutual information core concept information the ory mackay  discussion ppca assumed parameters model ie   likelihood parameter  known tipping bishop  derive maximum likelihood estimates parameters ppca setting note use different notation  maximum likelihood parameters pro draft  mathematics machine learning feedback smmlbookcom matrix   σi guaranteed positive semideﬁnite smallest eigenvalue data covariance matrix bounded noise variance   reading jecting ddimensional data  dimensional subspace µml  cid xn  bml      σi cid λj  jm  ml  rdm contains  eigenvectors data covariance matrix rm  diagonal matrix eigenvalues   diagλ     λm  rm  associated principal axes diagonal  arbitrary orthogonal matrix maximum likelihood solution bml unique arbitrary orthogonal transformation eg right multiply bml rotation matrix   essentially singular value decomposition see section  outline proof given tipping bishop  maximum likelihood estimate  given  sample mean data maximum likelihood estimator observation noise variance  given  average variance orthog onal complement principal subspace ie average leftover vari ance capture ﬁrst  principal components treated observation noise noisefree limit   ppca pca provide identical solutions data covariance matrix  symmetric di agonalized see section  ie exists matrix  eigenvectors    λt   ppca model data covariance matrix covariance matrix    bbcidσi  gaussian likelihood px  obtain bbcid data covariance equal pca data covariance and factorization given  cov    λt   bbcid       ie obtain maximum likelihood estimate       clear ppca performs de composition data covariance matrix streaming setting data arrives sequentially recom mended use iterative expectation maximization them algorithm maximum likelihood estimation roweis  determine dimensionality latent variables the length code dimensionality lowerdimensional subspace project data gavish donoho  suggest heuristic that estimate noise variance  data    deisenroth   faisal   ong published cambridge university press  dimensionality reduction principal component analysis discard singular values smaller    alternatively use nested crossvalidation section  bayesian model selection cri teria discussed section  determine good estimate intrinsic dimensionality data minka  similar discussion linear regression   place prior distribution parameters model integrate out so  avoid point estimates parameters issues come point estimates see section   al low automatic selection appropriate dimensionality  latent space bayesian pca proposed bishop  prior pµ   placed model parameters generative process allows integrate model parameters instead condi tioning them addresses overﬁtting issues integration analytically intractable bishop  proposes use approximate in ference methods mcmc variational inference refer work gilks et al  blei et al  details approximate inference techniques cidxn zn  ppca considered linear model pxn bzn  cid icid observation dimensions  σicid prior pzn  affected noise allow observation dimension  different variance   obtain factor analysis fa spearman  bartholomew et al  means fa gives likelihood ﬂexibility ppca forces data explained model parameters  µhowever fa longer allows closedform maximum likelihood solution need use iterative scheme expectation maximization algorithm estimate model parameters ppca station ary points global optima longer holds fa compared ppca fa change scale data return different solutions rotate data cidxn algorithm closely related pca independent com ponent analysis ica hyvarinen et al  starting bzn   σicid latentvariable perspective pxn zn  change prior zn nongaussian distributions ica blindsource separation imagine busy train station people talking ears play role microphones linearly mix different speech signals train station goal blind source separation identi constituent parts mixed signals discussed previously context maximum likelihood estimation ppca original pca solution invariant rotation therefore pca identi best lowerdimensional subspace sig nals live signals murphy  ica addresses issue modiing prior distribution pz latent sources draft  mathematics machine learning feedback smmlbookcom bayesian pca factor analysis overly ﬂexible likelihood able explain noise independent component analysis ica blindsource separation  reading require nongaussian priors pz refer books hyvarinen et al  murphy  details ica pca factor analysis ica examples dimensionality re duction linear models cunningham ghahramani  provide broader survey linear dimensionality reduction ppca model discussed allows important ex tensions section  explained pca in dimensionality  signiﬁcantly greater number  data points exploiting insight pca performed computing many inner products idea pushed extreme consid ering inﬁnitedimensional features kernel trick basis kernel pca allows implicitly compute inner products inﬁnite dimensional features scholkopf et al  scholkopf smola  nonlinear dimensionality reduction techniques de rived pca burges  provides good overview auto encoder perspective pca discussed previously section render pca special case deep autoencoder deep autoencoder encoder decoder represented multilayer feedforward neural networks nonlinear mappings set activation functions neural networks identity model equivalent pca different approach nonlinear dimensionality reduction gaussian process latentvariable model gplvm proposed lawrence  gplvm starts latentvariable perspective derive ppca replaces linear relationship latent variables  observations  gaussian process gp instead estimating parameters mapping as ppca gplvm marginalizes model parameters makes point estimates latent variables  similar bayesian pca bayesian gplvm proposed titsias lawrence  maintains distribution latent variables  uses approx imate inference integrate well kernel trick kernel pca deep autoencoder gaussian process latentvariable model gplvm bayesian gplvm    deisenroth   faisal   ong published cambridge university press  density estimation gaussian mixture models earlier  covered fundamental problems machine learning regression   dimensionality reduction    look pillar ma chine learning density estimation journey introduce impor tant concepts expectation maximization them algorithm latent variable perspective density estimation mixture models apply machine learning data aim represent data way straightforward way data points them selves representation data figure  example however approach unhelpful dataset huge interested representing characteristics data density esti mation represent data compactly density paramet ric family eg gaussian beta distribution example looking mean variance dataset order represent data compactly gaussian distribution mean variance tools discussed section  maximum likelihood maximum posteriori estimation use mean variance gaussian represent distribution underlying data ie think dataset typical realization distribution sample it figure  twodimensional dataset meaningfully represented gaussian material published cambridge university press mathematics machine learning marc peter deisenroth  aldo faisal cheng soon ong  version free view download personal use only redistribution resale use derivative works by   deisenroth   faisal   ong  smmlbookcom xx  gaussian mixture model practice gaussian or similarly distributions encoun tered far limited modeling capabilities example gaussian approximation density generated data figure  poor approximation following look ex pressive family distributions use density estimation mixture models mixture models distribution px convex combination  simple base distributions mixture model px  πkpkx cid  cid πk cid   πk    cid components pk members family basic distributions eg gaussians bernoullis gammas πk mixture weights mixture weight mixture models expressive corresponding base distri butions allow multimodal data representations ie datasets multiple clusters example fig ure  focus gaussian mixture models gmms basic distributions gaussians given dataset aim maximize likelihood model parameters train gmm purpose use results     section  however unlike applications discussed earlier linear regression pca ﬁnd closedform maximum likelihood solution instead arrive set dependent simultaneous equations solve iteratively gaussian mixture model density model combine ﬁnite number  gaussian distributions cid cidx µk σk gaussian mixture model  gaussian mixture model px   cid µk σk cid  cid πk cid   πk    cidx πk cid µk σk πk          deﬁned   collection parameters model convex combination gaussian distri bution gives signiﬁcantly ﬂexibility modeling complex densi ties simple gaussian distribution which recover     illustration given figure  displaying weighted    deisenroth   faisal   ong published cambridge university press  density estimation gaussian mixture models components mixture density given  cid   cid      px cidx cidx     cidx  cid   parameter learning maximum likelihood      xn  xn   assume given dataset        drawn iid unknown distribution px ob jective ﬁnd good approximationrepresentation unknown distribution px means gmm  mixture components parameters gmm  means µk covariances σk mixture weights πk summarize free parameters   πk µk σk          example  initial setting figure  gaussian mixture model gaussian mixture distribution black composed convex combination gaussian distributions expressive individual component dashed lines represent weighted gaussian components figure  initial setting gmm black mixture mixture components dashed seven data points discs  simple running example helps illustrate visualize important concepts draft  mathematics machine learning feedback smmlbookcom xpxcomponentcomponentcomponentgmmdensityxpxπnxµσπnxµσπnxµσgmmdensity  parameter learning maximum likelihood consider onedimensional dataset      consisting seven data points wish ﬁnd gmm    components models density data initialize mixture components px  px  px  cidx cidx cidx  cid    cid  cid assign equal weights        model and data points shown figure    corresponding cid cid following obtain maximum likelihood esti mate θml model parameters  start writing like lihood ie predictive distribution training data given pa rameters exploit iid assumption leads factorized likelihood     pxn   pxn   individual likelihood term pxn density obtain loglikelihood cid πk cidxn µk σk cid   gaussian mixture log    log pxn   log πk µk σk     cid cid cid cidxn cidcid cid cid ml maximize loglikelihood aim ﬁnd parameters  deﬁned  normal procedure compute gradient dθ loglikelihood respect model parameters  set  solve  however unlike previous examples max imum likelihood estimation eg discussed linear regression section  obtain closedform solution however exploit iterative scheme ﬁnd good model parameters θml turn algorithm gmms key idea update model parameter time keeping ﬁxed remark consider single gaussian desired density sum   vanishes log applied directly gaussian component log cidx  σcid   logπ  log detσ   µcidσx simple form allows ﬁnd closedform maximum likelihood esti mates   discussed       deisenroth   faisal   ong published cambridge university press  density estimation gaussian mixture models log sum  obtain simple closedform maximum likelihood solution local optimum function exhibits property gradi ent respect parameters vanish necessary condition   case obtain following necessary conditions optimize loglikelihood  respect gmm param eters µk σk πk µk σk πk  cid cid  log pxn µk  cid      cid cid  log pxn σk     log pxn    πk necessary conditions applying chain rule see sec tion  require partial derivatives form  log pxn pxn pxn   µk σk πk         model parameters pxn cidk  πj cid  µj σj following compute partial derivatives   this introduce quantity play central role remainder  responsibilities cidxn  responsibilities deﬁne quantity rnk  πk cidk  πj cidxn µk σk cid cidxn µj σj cid responsibility kth mixture component nth data point responsibility rnk kth mixture component data point xn proportional likelihood pxn πk µk σk  πk cidxn µk σk cid mixture component given data point therefore mixture com ponents high responsibility data point data point plausible sample mixture component note rk normalized probability vector ie right  rn     rnkcid draft  mathematics machine learning feedback smmlbookcom responsibility right follows boltzmanngibbs distribution  parameter learning maximum likelihood cid  rnk   rnk cid  probability vector distributes probabil ity mass  mixture components think right soft assignment xn  mixture components therefore re sponsibility rnk  represents probability xn generated kth mixture component example  responsibilities example figure  compute responsibilities rnk responsibility rnk probability kth mixture component generated nth data point       right   nth row tells responsibilities mixture components xn sum  responsibilities data point sum row  kth column gives overview responsibility kth mixture component mixture component third column responsible ﬁrst data points takes responsibility remaining data points sum entries column gives values nk ie total responsibility kth mixture component example          following determine updates model parameters µk σk πk given responsibilities update equa tions depend responsibilities makes closedform solu tion maximum likelihood estimation problem impossible however given responsibilities updating model parameter time keeping ﬁxed this recompute responsibilities iterating steps eventually converge lo cal optimum speciﬁc instantiation algorithm discuss section   updating means theorem  update gmm means update mean pa rameters µk         gmm given µnew   cidn  rnkxn cidn  rnk responsibilities rnk deﬁned     deisenroth   faisal   ong published cambridge university press  density estimation gaussian mixture models remark update means µk individual mixture compo nents  depends means covariance matrices σk mix ture weights πk rnk given  therefore obtain closedform solution µk once proof  gradient loglikelihood respect mean parameters µk         requires compute partial derivative pxn µk cid πj  πkxn µk µkcidσ cidxn µj σj cid cidxn µk σk cid  πk µk cidxn exploited kth mixture component depends µk use result   µk σk   cid  desired partial derivative respect µk given µk  log pxn µk xn µkcidσ cid pxn πk cidk  πj cid pxn µk cid µk σk cidxn cidxn cidcid rnk µj σj cid cid rnkxn µkcidσ   cid cid cid identity  result partial deriva tive   values rnk responsibilities deﬁned  solve  µnew lµnew   µk  cid obtain cid rnkxn  rnkµnew   µnew   cid cidn  rnkxn cidn  rnk nk cid deﬁned nk  rnk cid total responsibility kth mixture component entire dataset concludes proof theorem  intuitively  interpreted importanceweighted monte carlo estimate mean importance weights data point xn responsibilities rnk kth cluster xn         draft  mathematics machine learning feedback smmlbookcom rnkxn  figure  update mean parameter mixture component gmm mean  pulled individual data points weights given corresponding responsibilities figure  effect updating mean values gmm  gmm updating mean values  gmm updating mean values µk retaining variances mixture weights  parameter learning maximum likelihood therefore mean µk pulled data point xn strength given rnk means pulled stronger data points corresponding mixture component high responsibility ie high likelihood figure  illustrates this interpret mean up date  expected value data points distri bution given rk  rk     right kcidnk  normalized probability vector ie µk  erk   example  mean updates  gmm density individual components prior updating mean values  gmm density individual components updating mean values example figure  mean values updated fol lows             means ﬁrst mixture component regime data mean second component change dramatically figure  illustrates change figure  shows gmm density prior updating means figure  shows gmm density updating mean values µk update mean parameters  look fairly straight forward however note responsibilities rnk function πj µj σj         updates  depend parameters gmm closedform solution ob tained linear regression section  pca   obtained    deisenroth   faisal   ong published cambridge university press  xpxπnxµσπnxµσπnxµσgmmdensityxpxπnxµσπnxµσπnxµσgmmdensity  density estimation gaussian mixture models  updating covariances theorem  updates gmm covariances update co variance parameters σk         gmm given σnew   nk cid rnkxn µkxn µkcid  rnk nk deﬁned   respectively proof prove theorem  approach compute partial respect covariances σk set derivatives loglikelihood  solve σk start general approach σk cid  log pxn σk cid pxn pxn σk know pxn tial derivative pxn sian distribution pxn obtain   obtain remaining par θσk write deﬁnition gaus  see  drop terms kth cid pxn σk σk cid µkcid µkcid πkπ  detσk  exp cid  xn µkcidσ  xn cid  σk σk σk µkcidσ  πkπ detσk  exp cid  xn µkcidσ  xn cid  detσk exp cid  xn µkcidσ  xn µkcid use identities detσk detσk     σk xn  xn µk   xn µkxn µkcidσ obtain after rearranging desired partial derivative required pxn σk µk σk cid  πk cidxn     cid  xn µkxn µkcidσ  cid  putting together partial derivative loglikelihood draft  mathematics machine learning feedback smmlbookcom  parameter learning maximum likelihood respect σk given σk  log pxn σk cid pxn pxn σk cid cid πk cidk  πj cid cidxn cid µk σk µj σj cid cid cidxn cidcid rnk  xn cid   cid   rnkσ   cid cid cidcid cid nk cid  cid µkxn µkcidσ  cid  xn µkxn µkcidσ   rnk rnkxn µkxn µkcid   cid responsibilities rnk appear partial derivative setting partial derivative  obtain necessary optimality condition nkς    rnkxn µkxn µkcid cid  cid cid  cid cid cid nki  rnkxn µkxn µkcid   solving σk obtain σnew   nk cid rnkxn µkxn µkcid  rk probability vector deﬁned  gives sim ple update rule σk         proves theorem  similar update µk  interpret update covariance  importanceweighted expected value square centered data    µk     xn µk example  variance updates example figure  variances updated follows             deisenroth   faisal   ong published cambridge university press  figure  effect updating variances gmm  gmm updating variances  gmm updating variances retaining means mixture weights density estimation gaussian mixture models variances ﬁrst component shrink signiﬁcantly variance second component increases slightly figure  illustrates setting figure  identical but zoomed in figure  shows gmm density indi vidual components prior updating variances figure  shows gmm density updating variances  gmm density individual components prior updating variances  gmm density individual components updating variances similar update mean parameters interpret  monte carlo estimate weighted covariance data points xn associated kth mixture component weights responsibilities rnk updates mean parameters up date depends πj µj σj         responsibilities rnk prohibits closedform solution  updating mixture weights theorem  update gmm mixture weights mixture weights gmm updated πnew   nk           number data points nk deﬁned  proof ﬁnd partial derivative loglikelihood respect weight parameters πk         account con straint cid  πk   lagrange multipliers see section  lagrangian     πk cid  cid cid draft  mathematics machine learning feedback smmlbookcom xpxπnxµσπnxµσπnxµσgmmdensityxpxπnxµσπnxµσπnxµσgmmdensity  parameter learning maximum likelihood cid log cid πk cidxn µk σk cid   cid  cid cid πk loglikelihood  second term encodes equality constraint mixture weights need sum  obtain partial derivative respect πk πk cid πk cid   cid cidxn  πj πk cidk  πj µk σk cidxn cidxn cidxn cidcid nk cidk cid cid µj σj µk σk cid µj σj cid cid      nk πk partial derivative respect lagrange multiplier  setting partial derivatives  necessary condition optimum yields equations cid πk   πk  nk   πk  cid   solving πk obtain cid πk               cid nk allows substitute    obtain πnew   nk gives update weight parameters πk proves theo rem  identi mixture weight  ratio to tal responsibility kth cluster number data points   cid  nk number data points interpreted total responsibility mixture components together πk relative importance kth mixture component dataset remark nk  cidn  rnk update equation  mix ture weights πk depends πj µj σj         re sponsibilities rnk    deisenroth   faisal   ong published cambridge university press  density estimation gaussian mixture models example  weight parameter updates figure  effect updating mixture weights gmm  gmm updating mixture weights  gmm updating mixture weights retaining means variances note different scales vertical axes  gmm density individual components prior updating mixture weights  gmm density individual components updating mixture weights running example figure  mixture weights up dated follows                component gets weightimportance components slightly important figure  illustrates effect updating mixture weights figure  identical figure  shows gmm density individual components prior updating mixture weights figure  shows gmm density updating mixture weights overall having updated means variances weights once obtain gmm shown figure  compared initialization shown figure  parameter updates caused gmm density shift mass data points updating means variances weights once gmm ﬁt figure  remarkably better initialization figure  evidenced loglikelihood values in creased  initialization  complete update cycle  algorithm unfortunately updates    consti tute closedform solution updates parameters µk σk πk mixture model responsibilities rnk depend pa rameters complex way however results suggest simple iterative scheme ﬁnding solution parameters estimation problem maximum likelihood expectation maximization algorithm them algo draft  mathematics machine learning feedback smmlbookcom algorithm xpxπnxµσπnxµσπnxµσgmmdensityxpxπnxµσπnxµσπnxµσgmmdensity  algorithm rithm proposed dempster et al  general iterative scheme learning parameters maximum likelihood map mixture models and generally latentvariable models example gaussian mixture model choose initial values µk σk πk alternate convergence estep evaluate responsibilities rnk posterior probability data point  belonging mixture component  mstep use updated responsibilities reestimate parameters µk σk πk step algorithm increases loglikelihood function neal hinton  convergence check loglikelihood parameters directly concrete instantiation algorithm estimating parameters gmm follows  initialize µk σk πk  estep evaluate responsibilities rnk data point xn cur rent parameters πk µk σk rnk  πk  πj cid cidxn µk σk cid cidxn cid  µj σj  mstep reestimate parameters πk µk σk current responsi bilities rnk from estep µk  rnkxn  cid cid nk nk nk σk  πk  rnkxn µkxn µkcid  example  gmm fit  final gmm ﬁt ﬁve iterations algorithm converges returns gmm  negative loglikelihood function iterations    deisenroth   faisal   ong published cambridge university press  having updated means µk subsequently  update corresponding covariances figure  algorithm applied gmm figure   final gmm ﬁt  negative loglikelihood function iteration xpxπnxµσπnxµσπnxµσgmmdensityiterationnegativeloglikelihood  density estimation gaussian mixture models  dataset  negative loglikelihood  initialization  iteration figure  illustration algorithm ﬁtting gaussian mixture model components twodimensional dataset  dataset  negative loglikelihood lower better function iterations red dots indicate iterations mixture components corresponding gmm ﬁts shown  yellow discs indicate means gaussian mixture components figure  shows ﬁnal gmm ﬁt   iterations   iterations run example figure  obtain ﬁnal result shown figure  ﬁve iterations figure  shows negative loglikelihood evolves function iterations ﬁnal gmm given cidx  cid px   cidx    cid    cid      cidx applied algorithm twodimensional dataset shown figure     mixture components figure  illustrates steps algorithm shows negative loglikelihood function iteration figure  figure  shows draft  mathematics machine learning feedback smmlbookcom xxemiterationnegativeloglikelihoodxxxxxxxx figure  gmm ﬁt responsibilities converges  gmm ﬁt converges  data point colored according responsibilities mixture components  latentvariable perspective  gmm ﬁt  iterations  dataset colored according respon sibilities mixture components corresponding ﬁnal gmm ﬁt figure  visualizes ﬁnal re sponsibilities mixture components data points dataset colored according responsibilities mixture components converges single mixture component clearly responsible data left overlap data clusters right generated mixture components clear data points uniquely assigned single component either blue yellow responsibilities clusters points   latentvariable perspective look gmm perspective discrete latentvariable model ie latent variable  attain ﬁnite set val ues contrast pca latent variables continuous valued numbers rm  advantages probabilistic perspective  jus ti ad hoc decisions previous sections ii allows concrete interpretation responsibilities posterior probabil ities iii iterative algorithm updating model parameters derived principled manner algorithm maximum likelihood parameter estimation latentvariable models  generative process probabilistic model derive probabilistic model gmms useful think generative process ie process allows generate data probabilistic model assume mixture model  components data point  generated exactly mixture component introduce binary indicator variable zk states see section  indicates kth mixture component generated data point        deisenroth   faisal   ong published cambridge university press  xxxx  density estimation gaussian mixture models px zk    cidx µk σk cid  rk probability vector consisting deﬁne        zkcid   exactly  example    valid      zcid    cid select second mixture component    remark kind probability distribution called multi noulli generalization bernoulli distribution values murphy  properties  imply cidk encoding also ofk representation  zk   therefore  onehot far assumed indicator variables zk known how ever practice case place prior distribution pz         πkcid  πk    cid latent variable  kth entry πk  pzk   figure  graphical model gmm single data point probability vector describes probability kth mixture component generated data point  remark sampling gmm construction latentvariable model see corresponding graphical model figure  lends it self simple sampling procedure generative process generate data  sample zi  sample xi pz px zi   ﬁrst step select mixture component via onehot encod ing  random according pz   second step draw sample corresponding mixture component discard samples latent variable left xi valid samples gmm kind sampling samples random variables depend samples variables parents graphical model called ancestral sampling generally probabilistic model deﬁned joint distribution data latent variables see section  prior pz deﬁned   conditional px   obtain  components joint distribution px zk    px zk  pzk    πk cidx µk σk cid draft  mathematics machine learning feedback smmlbookcom onehot encoding ofk representation µk σk         ancestral sampling  latentvariable perspective         px   px    px zk     cidx   µk σk cid   cid cidx fully speciﬁes probabilistic model πk  likelihood obtain likelihood px  latentvariable model need marginalize latent variables see section  case summing latent variables joint px  px   px  zpz     µk σk πk          cid explicitly condition parameters  probabilistic model previously omitted  sum  possible one hot encodings  denoted cid  single nonzero single entry   possible conﬁgurations settings  example     conﬁgurations       summing possible conﬁgurations   equivalent looking nonzero entry zvector writing px   px  zpz cid cid px  zk  pzk   desired marginal distribution given px   cid px  zk  pzk   cid cidx µk σk cid  πk identi gmm model  given dataset immediately obtain likelihood     pxn   cid cid cid πk cidxn µk σk cid     deisenroth   faisal   ong published cambridge university press  density estimation gaussian mixture models figure  graphical model gmm  data points zn xn µk σk                 exactly gmm likelihood  therefore latent variable model latent indicators zk equivalent way thinking gaussian mixture model  posterior distribution let brief look posterior distribution latent variable  according bayes theorem posterior kth component having generated data point  pzk     pzk  px px zk   marginal px given  yields posterior distribution kth indicator variable zk pzk     pzk  px  pzj  px cidk zk   zj   πk cidk  πj cidx cid µk σk cidx µj σj cid  identi responsibility kth mixture component data point  note omitted explicit conditioning gmm parameters πk µk σk          extension dataset far discussed case dataset consists single data point  however concepts prior posterior directly extended case  data points probabilistic interpretation gmm data point xn pos      xn sesses latent variable zn  zn     znkcid rk  previously when considered single data point  omitted index  important draft  mathematics machine learning feedback smmlbookcom cid  latentvariable perspective share prior distribution  latent variables zn corresponding graphical model shown figure  use plate notation conditional distribution px     xn      zn  factorizes data points given px     xn      zn   pxn zn  obtain posterior distribution pznk   reasoning section  apply bayes theorem obtain xn follow pznk   xn  cidk pxn  pxn cidxn πk cidk  πj znk  pznk   znj  pznj   cid µk σk cid  rnk  µj σj cidxn means pzk   xn posterior probability kth mixture component generated data point xn corresponds re sponsibility rnk introduced  responsibilities intuitive mathematically justiﬁed interpreta tion posterior probabilities  algorithm revisited algorithm introduced iterative scheme maximum likelihood estimation derived principled way latent variable perspective given current setting θt model parameters estep calculates expected loglikelihood qθ θt   cid   xθtlog px  log px   θtdz  θpz expectation log px  rior pz model parameters θt maximizing   taken respect poste  θt latent variables mstep selects updated set iteration increase loglikelihood guarantees converges maximum likelihood solution possible algorithm converges local maximum loglikelihood different initializations parameters  multiple runs reduce risk ending bad local optimum details here refer excellent expositions rogers girolami  bishop     deisenroth   faisal   ong published cambridge university press  density estimation gaussian mixture models  reading gmm considered generative model sense straightforward generate new data ancestral sampling bishop  given gmm parameters πk µk σk         sample index  probability vector      πkcid sample cid repeat  times obtain dataset data point  generated gmm figure  generated procedure cidµk σk    assumed number components  known practice case however use nested crossvalidation discussed section  ﬁnd good models gaussian mixture models closely related kmeans clustering algorithm kmeans uses algorithm assign data points clusters treat means gmm cluster centers ignore covariances or set  arrive kmeans nicely described mackay  kmeans makes hard assignment data points cluster centers µk gmm makes soft assignment responsibilities touched latentvariable perspective gmms algorithm note parameter learning general latentvariable models eg nonlinear statespace models ghahramani roweis  roweis ghahramani  reinforcement learning discussed barber  therefore latentvariable per spective gmm useful derive corresponding algorithm principled way bishop  barber  murphy  discussed maximum likelihood estimation via algo rithm ﬁnding gmm parameters standard criticisms maximum likelihood apply here linear regression maximum likelihood suffer severe overﬁtting gmm case happens mean mix ture component identical data point covariance tends  then likelihood approaches inﬁnity bishop  barber  discuss issue detail obtain point estimate parameters πk µk σk         indication uncertainty pa rameter values bayesian approach place prior param eters obtain posterior distribution param eters posterior allows compute model evidence marginal likelihood model comparison gives principled way determine number mixture components un fortunately closedform inference possible setting conjugate prior model however approximations variational inference obtain approximate posterior bishop  draft  mathematics machine learning feedback smmlbookcom figure  histogram orange bars kernel density estimation blue line kernel density estimator produces smooth estimate underlying density histogram unsmoothed count measure data points black fall single bin histogram kernel density estimation  reading  discussed mixture models density estimation plethora density estimation techniques available practice use histograms kernel density estimation histograms provide nonparametric way represent continuous den sities proposed pearson  histogram con structed binning data space count data points fall bin bar drawn center bin height bar proportional number data points bin bin size critical hyperparameter bad choice lead overﬁt ting underﬁtting crossvalidation discussed section  determine good bin size kernel density estimation independently proposed rosenblatt  parzen  nonparametric way density estimation given  iid samples kernel density estimator represents underlying distribution px    cid cid  cid xn  kernel function ie nonnegative function integrates     smoothingbandwidth parameter plays similar role bin size histograms note place kernel single data point xn dataset commonly kernel functions uniform distribution gaussian distribution kernel density esti mates closely related histograms choosing suitable kernel guarantee smoothness density estimate figure  illus trates difference histogram kernel density estimator with gaussianshaped kernel given dataset  data points    deisenroth   faisal   ong published cambridge university press  xpxdatakdehistogram  classiﬁcation support vector machines situations want machine learning algorithm predict number discrete outcomes example email client sorts mail personal mail junk mail outcomes example telescope identiﬁes object night sky galaxy star planet usually small number outcomes importantly usually additional structure outcomes  consider predictors output binary val ues ie possible outcomes machine learning task called binary classiﬁcation contrast   considered prediction problem continuousvalued outputs binary classiﬁcation set possible values labeloutput attain binary  denote words consider predictors form   rd   recall   represent example data point xn feature vector  real numbers labels referred positive negative classes respectively careful infer intuitive attributes positiveness  class example cancer detection task patient cancer labeled    principle distinct values used eg  problem binary classiﬁcation studied defer survey approaches section  true false red blue present approach known support vector machine svm solves binary classiﬁcation task regression su rd pervised learning task set examples xn corresponding binary labels yn  given train         xn  yn  ing data set consisting examplelabel pairs like estimate parameters model smallest classiﬁcation error similar   consider linear model hide away nonlinearity transformation  examples  revisit  section  svm provides stateoftheart results applications sound theoretical guarantees steinwart christmann  main reasons chose illustrate binary classiﬁcation material published cambridge university press mathematics machine learning marc peter deisenroth  aldo faisal cheng soon ong  version free view download personal use only redistribution resale use derivative works by   deisenroth   faisal   ong  smmlbookcom example structure outcomes ordered like case small medium large tshirts binary classiﬁcation input example xn referred inputs data points features instances class probabilistic models mathematically convenient use   binary representation remark example  classiﬁcation support vector machines figure  example  data illustrating intuition data ﬁnd linear classiﬁer separates orange crosses blue discs svms first svm allows geometric way think supervised machine learning   considered machine learning problem terms probabilistic models attacked maximum likelihood estimation bayesian inference consider alternative approach reason geometrically machine learning task relies heavily concepts inner products projections discussed   second reason ﬁnd svms instructive contrast   optimization problem svm admit analytic solution need resort variety optimization tools introduced   svm view machine learning subtly different max imum likelihood view   maximum likelihood view pro poses model based probabilistic view data distribution optimization problem derived contrast svm view starts designing particular function optimized training based geometric intuitions seen similar   derived pca geometric principles svm case start designing loss function minimized training data following principles empirical risk minimization section    let derive optimization problem corresponding training svm examplelabel pairs intuitively imagine binary classiﬁcation data separated hyperplane illustrated figure  here example xn  vector dimension  twodimensional location    corresponding binary label yn different symbols orange cross blue disc hyperplane word commonly machine learning encountered hyper planes section  hyperplane afﬁne subspace di mension   if corresponding vector space dimension  examples consist classes there possible labels features the components vector representing example arranged way allow separateclassi draw ing straight line    deisenroth   faisal   ong published cambridge university press  xx  classiﬁcation support vector machines following formalize idea ﬁnding linear separator classes introduce idea margin extend linear separators allow examples fall wrong side incur ring classiﬁcation error present equivalent ways formalizing svm geometric view section  loss function view section  derive dual version svm lagrange multipliers section  dual svm allows observe way formalizing svm terms convex hulls examples class section  conclude brieﬂy describing kernels numerically solve nonlinear kernelsvm optimization problem  separating hyperplanes given examples represented vectors xi xj way compute similarity inner product  recall section  inner products closely related angle vectors value inner product vectors depends length norm vector furthermore inner products allow rigorously deﬁne geometric concepts orthogonality pro jections xi xj cid cid main idea classiﬁcation algorithms represent data rd partition space ideally way examples label and examples partition case binary classiﬁcation space divided parts corresponding positive negative classes respectively consider particularly convenient partition linearly split rd space halves hyperplane let example  element data space consider function   rd      cid    cid  recall section  hy parametrized  perplanes afﬁne subspaces therefore deﬁne hyperplane separates classes binary classiﬁcation problem cid rd  cidx rd     cid  illustration hyperplane shown figure  vector  vector normal hyperplane  intercept derive  normal vector hyperplane  choosing examples xa xb hyperplane showing vector orthogonal  form equation  xa  xb   xa  xa   xb cid cid cid cid  xb cid cid   draft  mathematics machine learning feedback smmlbookcom  separating hyperplanes figure  equation separating hyperplane   standard way representing equation   ease drawing look hyperplane edge on negative positive  separating hyperplane   projection setting  plane second line obtained linearity inner product section  chosen xa xb hyperplane implies  xa    xb     recall vectors orthogonal inner product zero  orthogonal vector therefore obtain  orthogonal vector hyperplane hyperplane remark recall   think vectors different ways  think parameter vector  arrow indicating direction ie consider  geometric vector contrast think example vector  data point as indicated coordinates ie consider  coordinates vector respect standard basis  xa cid xb cid presented test example classi example pos itive negative depending hyperplane occurs note  deﬁnes hyperplane additionally de ﬁnes direction words deﬁnes positive negative hyperplane therefore classi test example xtest calcu late value function  xtest classi example   xtest cid   otherwise thinking geometrically positive ex amples lie above hyperplane negative examples below hyperplane training classiﬁer want ensure examples positive labels positive hyperplane ie  xn   cid  yn   examples negative labels negative side ie     yn    refer figure  geometric intuition positive negative examples conditions presented single equation cid equation  equivalent   multiply sides   yn   yn   respectively  xn yn cid   cid   cid cid  xn cid cid    deisenroth   faisal   ong published cambridge university press  figure  possible separating hyperplanes linear classiﬁers green lines separate orange crosses blue discs classiﬁer large margin turns generalize steinwart christmann  margin closest examples hyperplane classiﬁcation support vector machines  primal support vector machine based concept distances points hyperplane position discuss support vector machine dataset       xn  yn  linearly separable inﬁnitely candidate hyperplanes refer figure  classiﬁers solve classiﬁcation problem training errors ﬁnd unique solution idea choose separating hyperplane maximizes margin positive negative examples words want positive negative examples separated large margin section  following compute dis tance example hyperplane derive margin recall closest point hyperplane given point example xn obtained orthogonal projection section   concept margin concept margin intuitively simple distance separating hyperplane closest examples dataset assuming dataset linearly separable however trying formalize distance technical wrinkle confusing tech nical wrinkle need deﬁne scale measure distance potential scale consider scale data ie raw values xn problems this change units measurement xn change values xn and hence change distance hyperplane shortly deﬁne scale based equation hyperplane  itself cid   cid consider hyperplane   example xa illustrated figure  loss generality consider example xa     positive hyperplane ie like compute distance   xa hyperplane considering orthogonal projection section  xa hyperplane denote xcid   orthogonal  xa cid cid draft  mathematics machine learning feedback smmlbookcom xx  primal support vector machine xa xcida figure  vector addition express distance hyperplane xa  xcid    cidwcid  hyperplane know distance  scaling vector  length  known use scaling factor  factor work absolute distance xa xcid  convenience choose use vector unit length its norm  obtain dividing  norm  cidwcid  vector addition section  obtain xa  xcid   cid cid way thinking  coordinate xa subspace spanned   expressed distance xa cid hyperplane  choose xa point closest hyperplane distance  margin cid recall like positive examples  hyperplane negative examples dis tance in negative direction hyperplane analogously combination    formulate ob jective yn  xn cid   cid  cid words combine requirements examples  away hyperplane in positive negative direction single inequality interested direction add assumption   model parameter vector  unit length ie use euclidean norm cid assumption allows intuitive interpretation distance   scaling factor vector length   wcidw section  choices inner products section  section  cid cid cid remark reader familiar presentations margin   different standard notice deﬁnition presentation svm provided scholkopf smola  example section  equivalence approaches collecting requirements single constrained optimization cid cid    deisenroth   faisal   ong published cambridge university press  classiﬁcation support vector machines figure  derivation margin   cidwcid  xa xcida cidw cidw cid cid problem obtain objective max wbr subject cidcidcidcid margin yn cid  xn cid cid cidcid data ﬁtting   cid  cid   cid cid cid cidcid cid normalization    says want maximize margin  ensuring data lies correct hyperplane remark concept margin turns highly pervasive ma chine learning vladimir vapnik alexey chervonenkis margin large complexity function class low learning possible vapnik  turns concept useful different approaches theoret ically analyzing generalization error steinwart christmann  shalevshwartz bendavid   traditional derivation margin previous section derived  making observation interested direction  length leading   section derive margin max assumption imization problem making different assumption instead choosing parameter vector normalized choose scale data    choose scale value predictor closest example let denote example dataset closest hyperplane xa   cid cid cid cid figure  identical figure  rescaled orthogonal projection xa hyperplane axes example xa lies exactly margin ie    xcid deﬁnition lie hyperplane ie  xa cid cid  xcid cid acid      draft  mathematics machine learning feedback smmlbookcom recall currently consider linearly separable data  primal support vector machine substituting   obtain cid cid cid exploiting bilinearity inner product see section        xa cid  xa cid cid   cid cid      cid cid observe ﬁrst term  assumption scale ie     section  know cid second term reduces   xa cid  hence   cid cid  simpliﬁcations obtain cid cid cid cid cid cid means derived distance  terms normal vector  hyperplane ﬁrst glance equation counterintuitive think derived distance hyperplane terms length vector  know vector way think consider distance  temporary variable use derivation therefore rest section denote distance hyperplane  cidwcid  section  choice margin equals  equivalent previous assumption distance projection error incurs projecting xa hyperplane   section  cid cid similar argument obtain  want positive negative examples  away hyperplane yields condition cid combining margin maximization fact examples need correct hyperplane based labels gives  xn   cid   yn cid max wb cid cid subject yn  xn cid cid   cid          instead maximizing reciprocal norm  minimize squared norm include constant  affect optimal   yields tidier form compute gradient then objective squared norm results convex quadratic programming problem svm section   cid min cid wb subject yn  xn cid cid   cid           equation  known hard margin svm reason expression hard formulation allow vi olations margin condition section  hard margin svm    deisenroth   faisal   ong published cambridge university press  classiﬁcation support vector machines hard condition relaxed accommodate violations data linearly separable  set margin  section  argued like maximize value  represents distance closest example hyperplane section  scaled data closest example distance  hyperplane section relate derivations equivalent theorem  maximizing margin  consider normalized weights  max wbr cidcidcidcid margin subject yn cid cid  xn cid cidcid data ﬁtting   cid  cid   cid cid cid cidcid cid normalization    equivalent scaling data margin unity min wb  cid cid cid cidcid cid margin subject yn cid cid  xn cid cidcid data ﬁtting   cid  cid proof consider  square strictly monotonic trans formation nonnegative arguments maximum stays consider  objective   reparametrize cid equation new weight vector wcid normalized explicitly wcid cid cidwcidcid  obtain max wcidbr yn subject cidcid wcid wcid cid cid cid cid  xn   cid     equation  explicitly states distance  positive therefore divide ﬁrst constraint  yields max wcidbr subject yn cid wcid wcid cid cid cid cidcid cid wcidcid cid  xn cidcidcidcid bcidcid cid    draft  mathematics machine learning feedback smmlbookcom note   assumed linear separability issue divide   primal support vector machine figure   linearly separable  nonlinearly separable data  linearly separable data large margin  nonlinearly separable data renaming parameters wcidcid bcidcid wcidcid  wcid  gives wcidcid cid cid cid cid cid cid wcid wcid cid cid cid cid cid cid cid cid cid wcid wcid cid cid cid cid cid substituting result  obtain cid cid cidwcidcidare  rearranging max wcidcidbcidcid subject wcidcid cid yn  cid wcidcid xn cid  bcidcid cid   cid cidwcidcidcid yields solution ﬁnal step observe maximizing minimizing  wcidcid  concludes proof theorem  cid  cid  soft margin svm geometric view case data linearly separable wish allow examples fall margin region wrong hyperplane illustrated figure  model allows classiﬁcation errors called soft margin svm section derive resulting optimization problem geometric arguments section  derive equiv alent optimization problem idea loss function la grange multipliers section  derive dual optimization problem svm section  dual optimization problem al lows observe interpretation svm hyperplane bisects line convex hulls corresponding positive negative data examples section  key geometric idea introduce slack variable ξn corresponding examplelabel pair xn yn allows particular example margin wrong hyperplane refer    deisenroth   faisal   ong published cambridge university press  soft margin svm slack variable xxxx  classiﬁcation support vector machines figure  soft margin svm allows examples margin wrong hyperplane slack variable  measures distance positive example  positive margin hyperplane cidw xcid     wrong side cidw cidw cid cid figure  subtract value ξn margin constraining ξn nonnegative encourage correct classiﬁcation samples add ξn objective min wbξ subject cid ξn   cid   cid    cid  xn yn cid ξn cid  cid ξn soft margin svm regularization parameter regularizer alternative parametrizations regularization referred csvm          contrast optimization problem  hard margin svm called soft margin svm parameter    trades size margin total slack have parameter called regularization parameter since following section margin term objective func  called tion  regularization term margin term regularizer books numerical optimization reg ularization parameter multiplied term section  contrast formulation section large value  implies low regularization slack variables larger weight giving priority examples lie correct margin cid cid remark formulation soft margin svm   reg ularized  regularized observing regularization term contain  unregularized term  com plicates theoretical analysis steinwart christmann    decreases computational efﬁciency fan et al   soft margin svm loss function view let consider different approach deriving svm following principle empirical risk minimization section  svm draft  mathematics machine learning feedback smmlbookcom loss function zeroone loss hinge loss  primal support vector machine choose hyperplanes hypothesis class cid        cid section margin corresponds regulariza tion term remaining question is loss function con trast   consider regression problems the output predictor real number  consider binary classiﬁcation problems the output predictor labels  therefore errorloss function single example label pair needs appropriate binary classiﬁcation example squared loss regression  suitable bi nary classiﬁcation remark ideal loss function binary labels count num ber mismatches prediction label means predictor  applied example xn compare output  xn label yn deﬁne loss zero match match denoted  xn  yn called zeroone loss unfortunately zeroone loss results combinatorial optimization problem ﬁnding best parameters   combinatorial optimization problems in contrast continuous optimization problems discussed   general challenging solve loss function corresponding svm consider error output predictor  xn label yn loss de scribes error training data equivalent way derive  use hinge loss   cidt  max     yf    cid cid      correct based corresponding label  hyperplane distance  means  cid  hinge loss returns value zero   correct close hyperplane      example  margin hinge loss returns positive value example wrong hyperplane    hinge loss returns larger value increases linearly words pay penalty closer margin hyperplane prediction correct penalty increases linearly alternative way express hinge loss considering linear pieces illustrated figure  loss corresponding hard margin svm  deﬁned cidt  cid  cid     cidt  cid  cid        deisenroth   faisal   ong published cambridge university press  cid figure  hinge loss convex upper bound zeroone loss regularizer loss term error term regularization classiﬁcation support vector machines loss interpreted allowing examples inside margin given training set  seek minimize total loss regularizing objective cidregularization see section  hinge loss  gives unconstrained optimization problem       xn  yn  min wb  cid cid cid cidcid cid regularizer   cid cid max    xn yn cid cid cidcid error term   cid ﬁrst term  called regularization term regularizer see section  second term called loss term error  arises directly term recall section  term  margin words margin maximization interpreted regularization  cid cid principle unconstrained optimization problem  directly solved subgradient descent methods described section    equivalent observe hinge loss  essentially consists linear parts expressed  consider hinge loss single examplelabel pair  equivalently replace minimization hinge loss  minimization slack variable  constraints equation form equivalent min max   min ξt subject  cid    cid    substituting expression  rearranging constraints obtain exactly soft margin svm  remark let contrast choice loss function section loss function linear regression   recall section  ﬁnding maximum likelihood estimators usually minimize draft  mathematics machine learning feedback smmlbookcom tmaxtzeroonelosshingeloss  dual support vector machine negative loglikelihood furthermore likelihood term linear regression gaussian noise gaussian negative loglikelihood example squared error function squared error function loss function minimized looking maximum likelihood solution  dual support vector machine description svm previous sections terms vari ables   known primal svm recall consider inputs rd  features  dimension  means number parameters the dimension  opti mization problem grows linearly number features following consider equivalent optimization problem the socalled dual view independent number features in stead number parameters increases number examples training set saw similar idea appear   expressed learning problem way scale num ber features useful problems features number examples training dataset dual svm additional advantage easily allows kernels applied shall end  word dual appears mathematical literature particular case refers convex duality following subsections essentially application convex duality discussed section   convex duality lagrange multipliers recall primal soft margin svm  variables    corresponding primal svm primal variables use αn cid    lagrange multiplier corresponding constraint  examples classiﬁed correctly γn cid  lagrange multi plier corresponding nonnegativity constraint slack variable  lagrangian given  lagrange multipliers section follow notation commonly chosen svm literature use   lw       cid    cid cid ξn cid cid αnyn cid  xn     ξn cid cidcid constraint  cid γnξn cid cidcid cid constraint  cid    deisenroth   faisal   ong published cambridge university press  representer theorem representer theorem actually collection theorems saying solution minimizing empirical risk lies subspace section  deﬁned examples support vector classiﬁcation support vector machines differentiating lagrangian  respect primal variables    respectively obtain  wcid αnynxn cid  ξn cid cid αnyn    αn γn  ﬁnd maximum lagrangian setting partial derivatives zero setting  zero ﬁnd   αnynxn  cid particular instance representer theorem kimeldorf wahba  equation  states optimal weight vector primal linear combination examples xn recall sec tion  means solution optimization problem lies span training data additionally constraint obtained setting  zero implies optimal weight vector afﬁne combination examples representer theorem turns hold general settings regularized empirical risk minimization hof mann et al  argyriou dinuzzo  theorem general versions scholkopf et al  necessary sufﬁcient conditions existence yu et al  remark representer theorem  provides explanation support vector machine examples xn corresponding parameters αn   contribute solution  all examples αn   called support vectors support hyperplane substituting expression  lagrangian  obtain dual dξ    yiyjαiαj xi xj yiαi yjαjxj xi cid cid cid cid  cid cid   cid ξi cid yiαi  αi αiξi γiξi  cid cid cid cid  cid note longer terms involving primal variable  setting  zero obtain cidn  ynαn   therefore term involving  vanishes recall inner products symmetric draft  mathematics machine learning feedback smmlbookcom  dual support vector machine bilinear see section  therefore ﬁrst terms  objects terms colored blue simpliﬁed obtain lagrangian dξ    cid cid yiyjαiαj xi xj cid cid cid αi  αi γiξi  cid term equation collection terms contain slack variables ξi setting  zero term  zero furthermore equation recalling lagrange multiplers γi nonnegative conclude αi cid  obtain dual optimization problem svm ex pressed exclusively terms lagrange multipliers αi recall lagrangian duality deﬁnition  maximize dual problem equivalent minimizing negative dual problem end dual svm min cid cid yiyjαiαj xi xj cid cid  cid αi subject yiαi   cid  cid αi cid          dual svm equality constraint  obtained setting  zero inequality constraint αi cid  condition imposed la grange multipliers inequality constraints section  inequality constraint αi cid  discussed previous paragraph set inequality constraints svm called box constraints right lagrange mul limit vector    tipliers inside box deﬁned   axis axisaligned boxes particularly efﬁcient implement numerical solvers dostal     αn cid    obtain dual parameters  recover primal pa rameters  representer theorem  let op timal primal parameter  however remains question obtain parameter  consider example xn lies exactly    yn recall yn  margins boundary ie cid  therefore unknown  computed  xn cid   yn  xn  cid remark principle examples lie exactly margin case compute support vectors median value absolute value difference  xn yn  cid cid  cid    deisenroth   faisal   ong published cambridge university press  turns examples lie exactly margin examples dual parameters lie strictly inside box constraints   αi   derived karush kuhn tucker conditions example scholkopf smola  figure  convex hulls  convex hull points lie boundary  convex hulls positive negative examples convex hull classiﬁcation support vector machines  convex hull  convex hulls positive blue negative orange examples distance be tween convex sets length difference vector    value  derivation fouryears euthesvmbiastermconspiracy  dual svm convex hull view approach obtain dual svm consider alternative geometric argument consider set examples xn label like build convex set contains examples smallest possible set called convex hull illustrated figure  let ﬁrst build intuition convex combination points consider points   corresponding nonnegative weights   cid  αα   equation αxαx describes point line   consider happens add point  weight  cid  cid  αn   convex combination points    spans two dimensional area convex hull area triangle formed edges corresponding pair points add points number points greater number dimen sions points inside convex hull figure  general building convex convex hull introducing nonnegative weights αn cid  corresponding example xn convex hull described set conv   αnxn αn   αn cid   cid cid  cid cid draft  mathematics machine learning feedback smmlbookcom cd  dual support vector machine          clouds points corresponding positive negative classes separated convex hulls overlap given training data       xn  yn  form con vex hulls corresponding positive negative classes respectively pick point  convex hull set positive exam ples closest negative class distribution similarly pick point  convex hull set negative examples closest positive class distribution figure  deﬁne difference vector        picking points   preceding cases requiring closest equivalent minimizing lengthnorm  end corresponding optimization problem arg min  arg min  cid cid  cid   cid  positive convex hull expressed convex combination positive examples ie nonnegative coefﬁcients  use notation   yn   indicate set indices  yn   similarly examples negative labels obtain cid    xn  nyn cid    xn  nyn substituting     obtain objective cid cid cid cid cid min cid  xn nyn cid nyn  xn cid cid cid cid cid let  set coefﬁcients ie concatenation   recall require convex hull coefﬁcients sum one cid nyn    cid     nyn implies constraint cid ynαn       deisenroth   faisal   ong published cambridge university press  classiﬁcation support vector machines result seen multiplying individual classes cid ynαn  cid   nyn cid   nyn nyn cid nyn cid        objective function  constraint  assumption  cid  constrained convex optimization prob lem optimization problem shown dual hard margin svm bennett bredensteiner  remark obtain soft margin dual consider reduced hull reduced hull similar convex hull upper bound size coefﬁcients  maximum possible value elements  restricts size convex hull take words bound  shrinks convex hull smaller volume bennett bredensteiner   kernels consider formulation dual svm  notice in ner product objective occurs examples xi xj inner products examples parameters therefore consider set features φxi represent xi change dual svm replace inner product mod ularity choice classiﬁcation method the svm choice feature representation φx considered separately provides ﬂexibility explore problems independently section discuss representation φx brieﬂy introduce idea kernels technical details φx nonlinear function use svm which assumes linear classiﬁer construct classiﬁers nonlinear examples xn provides second avenue addition soft margin users deal dataset linearly separable turns algorithms statistical methods property observed dual svm inner products occur examples instead explicitly deﬁning nonlinear feature map   computing resulting inner product examples xi xj deﬁne similarity function kxi xj be tween xi xj certain class similarity functions called kernels similarity function implicitly deﬁnes nonlinear feature map   exists kernels deﬁnition functions   hilbert space feature map          kxi xj  φxi φxj cid cidh  draft  mathematics machine learning feedback smmlbookcom reduced hull kernel inputs  kernel function general necessarily restricted rd  kernels figure  svm different kernels note decision boundary nonlinear underlying problem solved linear separating hyperplane albeit nonlinear kernel  svm linear kernel  svm rbf kernel  svm polynomial degree  kernel  svm polynomial degree  kernel unique reproducing kernel hilbert space associated kernel  aronszajn  berlinet thomasagnan    called canonical feature map unique association φx   generalization inner product kernel function  known kernel trick scholkopf smola  shawetaylor cristianini  hides away explicit nonlinear feature map matrix  right   resulting inner products appli cation   dataset called gram matrix referred kernel matrix kernels symmetric positive semideﬁnite functions kernel matrix  symmetric positive semideﬁnite section  right  zcidkz cid   popular examples kernels multivariate realvalued data xi rd polynomial kernel gaussian radial basis function kernel rational quadratic kernel scholkopf smola  rasmussen    deisenroth   faisal   ong published cambridge university press  canonical feature map kernel trick gram matrix kernel matrix firstfeaturesecondfeaturefirstfeaturesecondfeaturefirstfeaturesecondfeaturefirstfeaturesecondfeature  classiﬁcation support vector machines williams  figure  illustrates effect different kernels separating hyperplanes example dataset note solving hyperplanes is hypothesis class functions linear nonlinear surfaces kernel function remark unfortunately ﬂedgling machine learner mul tiple meanings word kernel  word kernel comes idea reproducing kernel hilbert space rkhs aron szajn  saitoh  discussed idea kernel lin ear algebra section  kernel word null space common use word kernel machine learning smoothing kernel kernel density estimation section  explicit representation φx mathematically equivalent kernel representation kxi xj practitioner design kernel function computed efﬁciently inner product explicit feature maps example consider polynomial kernel scholkopf smola  number terms explicit expansion grows quickly even polynomials low degree input dimension large kernel function requires multiplication input dimension provide signiﬁcannot computational savings example gaussian ra dial basis function kernel scholkopf smola  rasmussen williams  corresponding feature space inﬁnite dimen sional case explicitly represent feature space compute similarities pair examples kernel useful aspect kernel trick need original data represented multivariate realvalued data note inner product deﬁned output function  restrict input real numbers hence function  kernel function   deﬁned object eg sets sequences strings graphs distributions benhur et al  gartner  shi et al  sriperumbudur et al  vishwanathan et al   numerical solution conclude discussion svms looking express problems derived  terms concepts presented   consider different approaches ﬁnding optimal solution svm consider loss view svm  ex press unconstrained optimization problem express constrained versions primal dual svms quadratic programs standard form  consider loss function view svm  convex unconstrained optimization problem hinge loss  dif draft  mathematics machine learning feedback smmlbookcom choice kernel parameters kernel chosen nested crossvalidation section   numerical solution ferentiable therefore apply subgradient approach solving it however hinge loss differentiable everywhere single point hinge    point gradient set  therefore subgradient  possible values lie  hinge loss given gt             subgradient apply optimization methods presented section  primal dual svm result convex quadratic pro gramming problem constrained optimization note primal svm  optimization variables size dimen sion  input examples dual svm  optimization variables size number  examples express primal svm standard form  quadratic programming let assume use dot product  inner product rearrange equation primal svm  optimization variables right inequality constraint matches standard form yields optimization min wbξ subject cid ξn ynb  cid    cid ynxcid   ξn cid  ξn cid          concatenating variables   xn single vector carefully collecting terms obtain following matrix form soft margin svm cid  dn       cid min wbξ cid cid subject   nd  cid cid cid cid   cidd cn cidcid preceding optimization problem minimization pa rameters wcid  ξcidcid rdn  use notation  rep  mn represent matrix resent identity matrix size   mn represent matrix ones size zeros size   yn cid   diagy  addition  vector labels        deisenroth   faisal   ong published cambridge university press  recall section  use phrase dot product mean inner product euclidean vector space classiﬁcation support vector machines   matrix elements diagonal  right  matrix obtained concatenating examples similarly perform collection terms dual version svm  express dual svm standard form ﬁrst express kernel matrix  entry kij  kxi xj explicit feature representation xi deﬁne kij  cid convenience notation introduce matrix zeros diagonal store labels is   diagy dual svm written xi xj cid min αcidy ky  cid nα ycid ycid  cid cid cidn  cn subject remark sections   introduced standard forms constraints inequality constraints express dual svms equality constraint inequality constraints ie ax   replaced ax cid  ax cid   particular software implementations convex optimization methods provide ability express equality constraints different possible views svm approaches solving resulting optimization problem ap proach presented here expressing svm problem standard convex optimization form practice main implemen tations svm solvers chang lin  which open source joachims  svms clear welldeﬁned optimiza tion problem approaches based numerical optimization tech niques nocedal wright  applied shawetaylor sun   reading svm approaches studying binary classiﬁcation approaches include perceptron logistic regression fisher dis criminant nearest neighbor naive bayes random forest bishop  murphy  short tutorial svms kernels discrete se quences benhur et al  development svms closely linked empirical risk minimization discussed section  hence svm strong theoretical properties vapnik  stein wart christmann  book kernel methods scholkopf smola  includes details support vector machines draft  mathematics machine learning feedback smmlbookcom  reading optimize them broader book kernel methods shawe taylor cristianini  includes linear algebra approaches different machine learning problems alternative derivation dual svm obtained idea legendrefenchel transform section  derivation considers term unconstrained formulation svm  separately calculates convex conjugates rifkin lippert  readers interested functional analysis view also reg ularization methods view svms referred work wahba  theoretical exposition kernels aronszajn  schwartz  saitoh  manton amblard  requires basic ground ing linear operators akhiezer glazman  idea kernels generalized banach spaces zhang et al  kreın spaces ong et al  loosli et al  observe hinge loss equivalent representations shown   constrained optimization problem  formulation  compar ing svm loss function loss functions steinwart  twopiece formulation  convenient computing subgra dients piece linear formulation  seen section  enables use convex quadratic programming sec tion  tools binary classiﬁcation wellstudied task machine learning words used discrimination separation decision furthermore quantities out binary classiﬁer output linear function often called score real value output ranking examples binary classiﬁcation thought picking threshold ranked examples shawetaylor cris tianini  second quantity considered output binary classiﬁer output determined passed nonlinear function constrain value bounded range ex ample interval   common nonlinear function sigmoid function bishop  nonlinearity results wellcalibrated probabilities gneiting raftery  reid williamson  called class probability estimation output binary classiﬁer ﬁnal binary decision  com monly assumed output classiﬁer svm binary classiﬁer naturally lend probabilistic interpretation approaches converting raw output linear function the score calibrated class probability estimate        involve additional cal ibration step platt  zadrozny elkan  lin et al  training perspective related probabilistic ap proaches mentioned end section  re    deisenroth   faisal   ong published cambridge university press  classiﬁcation support vector machines lationship loss function likelihood also compare sec tions   maximum likelihood approach corresponding wellcalibrated transformation training called logistic regres sion comes class methods called generalized linear mod els details logistic regression point view agresti    mccullagh nelder    naturally bayesian view classiﬁer output estimating posterior distribution bayesian logistic regression bayesian view includes speciﬁcation prior includes design choices conjugacy section  likelihood ad ditionally consider latent functions priors results gaussian process classiﬁcation rasmussen williams   draft  mathematics machine learning feedback smmlbookcom references abel niels   demonstration limpossibilite la resolution algebrique des equations generales qui passent le quatrieme degre grøndahl søn adhikari ani denero john  computational inferential thinking agarwal arvind daume iii hal  geometric view conjugate priors foundations data science gitbooks machine learning   agresti   categorical data analysis wiley akaike hirotugu  new look statistical model identiﬁcation transactions automatic control   akhiezer naum  glazman izrail   theory linear operators hilbert space dover publications alpaydin ethem  introduction machine learning mit press amari shunichi  information geometry applications springer argyriou andreas dinuzzo francesco  uniing view representer theorems in proceedings international conference machine learning aronszajn nachman  theory reproducing kernels transactions amer ican mathematical society   axler sheldon  linear algebra right springer bakir gokhan hofmann thomas scholkopf bernhard smola alexander  taskar ben vishwanathan    eds  predicting structured data mit press barber david  bayesian reasoning machine learning cambridge university barndorffnielsen ole  information exponential families statistical the press ory wiley bartholomew david knott martin moustaki irini  latent variable models factor analysis uniﬁed approach wiley baydin atılım  pearlmutter barak  radul alexey  siskind jeffrey   automatic differentiation machine learning survey journal machine learning research   beck amir teboulle marc  mirror descent nonlinear projected subgra dient methods convex optimization operations research letters   belabbas mohamedali wolfe patrick   spectral methods machine learning new strategies large datasets proceedings national academy sciences  belkin mikhail niyogi partha  laplacian eigenmaps dimensionality reduction data representation neural computation   benhur asa ong cheng soon sonnenburg soren scholkopf bernhard ratsch gunnar  support vector machines kernels computational biology plos computational biology   material published cambridge university press mathematics machine learning marc peter deisenroth  aldo faisal cheng soon ong  version free view download personal use only redistribution resale use derivative works by   deisenroth   faisal   ong  smmlbookcom references bennett kristin  bredensteiner erin   duality geometry svm classiﬁers in proceedings international conference machine learning bennett kristin  bredensteiner erin   geometry learning   of geometry work mathematical association america berlinet alain thomasagnan christine  reproducing kernel hilbert spaces probability statistics springer bertsekas dimitri   nonlinear programming athena scientiﬁc bertsekas dimitri   convex optimization theory athena scientiﬁc bickel peter  doksum kjell  mathematical statistics basic ideas selected topics vol  prentice hall bickson danny dolev danny shental ori siegel paul  wolf jack   linear detection belief propagation in proceedings annual allerton con ference communication control computing billingsley patrick  probability measure wiley bishop christopher   neural networks pattern recognition clarendon press cessing systems bishop christopher   bayesian pca in advances neural information pro bishop christopher   pattern recognition machine learning springer blei david  kucukelbir alp mcauliffe jon   variational inference review statisticians journal american statistical association  blum arvim hardt moritz  ladder reliable leaderboard ma chine learning competitions in international conference machine learning bonnans  frederic gilbert  charles lemarechal claude sagastizabal clau dia   numerical optimization theoretical practical aspects springer borwein jonathan  lewis adrian   convex analysis nonlinear optimization nd edn canadian mathematical society bottou leon  online algorithms stochastic approximations   of online learning neural networks cambridge university press bottou leon curtis frank  nocedal jorge  optimization methods largescale machine learning siam review   boucheron stephane lugosi gabor massart pascal  concentration in equalities nonasymptotic theory independence oxford university press boyd stephen vandenberghe lieven  convex optimization cambridge boyd stephen vandenberghe lieven  introduction applied linear alge university press bra cambridge university press brochu eric cora vlad  freitas nando  tutorial bayesian optimization expensive cost functions application active user modeling hierarchical reinforcement learning tech rept tr department science university british columbia brooks steve gelman andrew jones galin  meng xiaoli eds  hand book markov chain monte carlo chapman hallcrc brown lawrence   fundamentals statistical exponential families ap plications statistical decision theory institute mathematical statistics bryson arthur   gradient method optimizing multistage allocation processes in proceedings harvard university symposium digital computers applications bubeck sebastien  convex optimization algorithms complexity founda tions trends machine learning   buhlmann peter van geer sara  statistics highdimensional data springer draft  mathematics machine learning feedback smmlbookcom references burges christopher  dimension reduction guided tour foundations trends machine learning   carroll  douglas chang jihjie  analysis individual differences multidimensional scaling  way generalization eckartyoung decom position psychometrika   casella george berger roger   statistical inference duxbury cinlar erhan  probability stochastics springer chang chihchung lin chihjen  libsvm library support vector machines acm transactions intelligent systems technology   cheeseman peter  defense probability in proceedings international joint conference artiﬁcial intelligence chollet francois allaire    deep learning  manning publications codd edgar   relational model database management addisonwesley longman publishing cunningham john  ghahramani zoubin  linear dimensionality reduc tion survey insights generalizations journal machine learning research   datta biswa   numerical linear algebra applications siam davidson anthony  hinkley david   bootstrap methods appli cation cambridge university press dean jeffrey corrado greg  monga rajat chen et al  large scale distributed deep networks in advances neural information processing systems deisenroth marc  mohamed shakir  expectation propagation gaus sian process dynamical systems   of advances neural informa tion processing systems deisenroth marc  ohlsson henrik  general perspective gaussian filtering smoothing explaining current deriving new algorithms in proceedings american control conference deisenroth marc  fox dieter rasmussen carl   gaussian processes  transactions pattern dataefﬁcient learning robotics control analysis machine intelligence   dempster arthur  laird nan  rubin donald   maximum likelihood incomplete data algorithm journal royal statistical society   deng li seltzer michael  yu dong acero alex mohamed abdelrahman hinton geoffrey   binary coding speech spectrograms deep autoencoder in proceedings interspeech devroye luc  nonuniform random variate generation springer donoho david  grimes carrie  hessian eigenmaps locally linear embedding techniques highdimensional data proceedings national academy sciences   dostal zdenek  optimal quadratic programming algorithms applications variational inequalities springer douven igor  abduction in stanford encyclopedia philosophy meta physics research lab stanford university downey allen   think stats exploratory data analysis nd edn oreilly media dreyfus stuart  numerical solution variational problems journal mathematical analysis applications   drumm volker weil wolfgang  lineare algebra und analytische geometrie lecture notes universitat karlsruhe th dudley richard   real analysis probability cambridge university press    deisenroth   faisal   ong published cambridge university press  references eaton morris   multivariate statistics vector space approach institute mathematical statistics lecture notes eckart carl young gale  approximation matrix lower rank psychometrika   efron bradley hastie trevor  age statistical inference algorithms evidence data science cambridge university press efron bradley tibshirani robert   introduction bootstrap chap elliott conal  beautiful differentiation in international conference func man hallcrc tional programming evgeniou theodoros pontil massimiliano poggio tomaso  statistical learning theory primer international journal vision   fan rongen chang kaiwei hsieh chojui wang xiangrui lin chihjen  liblinear library large linear classiﬁcation journal machine learning research   gal yarin van der wilk mark rasmussen carl   distributed variational inference sparse gaussian process regression latent variable models in advances neural information processing systems gartner thomas  kernels structured data world scientiﬁc gavish matan donoho david   optimal hard threshold singular values    transactions information theory   gelman andrew carlin john  stern hal  rubin donald   bayesian data analysis chapman hallcrc gentle james   random number generation monte carlo methods springer nature   ghahramani zoubin  probabilistic machine learning artiﬁcial intelligence ghahramani zoubin roweis sam   learning nonlinear dynamical sys tems algorithm in advances neural information processing systems mit press gilks walter  richardson sylvia spiegelhalter david   markov chain monte carlo practice chapman hallcrc gneiting tilmann raftery adrian   strictly proper scoring rules pre diction estimation journal american statistical association  goh gabriel  momentum works distill gohberg israel goldberg seymour krupnik nahum  traces determi nants linear operators birkhauser golan jonathan   linear algebra beginning graduate student ought know springer press golub gene  van loan charles   matrix computations jhu press goodfellow ian bengio yoshua courville aaron  deep learning mit graepel thore candela joaquin quinonerocandela borchert thomas her brich ralf  webscale bayesian clickthrough rate prediction sponsored search advertising microsofts bing search engine in proceedings interna tional conference machine learning griewank andreas walther andrea  introduction automatic differenti ation in proceedings applied mathematics mechanics griewank andreas walther andrea  evaluating derivatives principles techniques algorithmic differentiation siam grimmett geoffrey  welsh dominic  probability introduction oxford university press draft  mathematics machine learning feedback smmlbookcom references grinstead charles  snell  laurie  introduction probability american mathematical society hacking ian  probability inductive logic cambridge university press hall peter  bootstrap edgeworth expansion springer hallin marc paindaveine davy ˇsiman miroslav  multivariate quan tiles multipleoutput regression quantiles cid optimization halfspace depth annals statistics   hasselblatt boris katok anatole  course dynamics panorama recent developments cambridge university press hastie trevor tibshirani robert friedman jerome  elements sta tistical learning  data mining inference prediction springer hausman karol springenberg jost  wang ziyu heess nicolas riedmiller martin  learning embedding space transferable robot skills in proceedings international conference learning representations hazan elad  introduction online convex optimization foundations trends optimization   hensman james fusi nicolo lawrence neil   gaussian processes big data in proceedings conference uncertainty artiﬁcial intelligence herbrich ralf minka tom graepel thore  trueskilltm bayesian skill rating system in advances neural information processing systems hiriarturruty jeanbaptiste lemarechal claude  fundamentals convex analysis springer hoffman matthew  blei david  bach francis  online learning latent dirichlet allocation advances neural information processing systems hoffman matthew  blei david  wang chong paisley john  stochas tic variational inference journal machine learning research   hofmann thomas scholkopf bernhard smola alexander   kernel meth ods machine learning annals statistics   hogben leslie  handbook linear algebra chapman hallcrc horn roger  johnson charles   matrix analysis cambridge university hotelling harold  analysis complex statistical variables principal components journal educational psychology   hyvarinen aapo oja erkki karhunen juha  independent component anal imbens guido  rubin donald   causal inference statistics social biomedical sciences cambridge university press jacod jean protter philip  probability essentials springer jaynes edwin   probability theory logic science cambridge university press ysis wiley press jefferys william  berger james   ockhams razor bayesian anal ysis american scientist   jeffreys harold  theory probability oxford university press jimenez rezende danilo mohamed shakir  variational inference nor malizing flows in proceedings international conference machine learning jimenez rezende danilo mohamed shakir wierstra daan  stochastic backpropagation approximate inference deep generative models in pro ceedings international conference machine learning joachims thorsten  advances kernel methods  support vector learning mit press chap making largescale svm learning practical   jordan michael  ghahramani zoubin jaakkola tommi  saul lawrence   introduction variational methods graphical models machine learn ing      deisenroth   faisal   ong published cambridge university press  references julier simon  uhlmann jeffrey   new extension kalman filter nonlinear systems in proceedings aerosense symposium aerospacedefense sensing simulation controls kaiser marcus hilgetag claus   nonoptimal component placement short processing paths longdistance projections neural systems plos computational biology   kalman dan  singularly valuable decomposition svd matrix col lege mathematics journal   kalman rudolf   new approach linear filtering prediction problems transactions asme  journal basic engineering series   kamthe sanket deisenroth marc   dataefﬁcient reinforcement learning in proceedings international probabilistic model predictive control conference artiﬁcial intelligence statistics katz victor   history mathematics pearsonaddisonwesley kelley henry   gradient theory optimal flight paths ars journal  kimeldorf george  wahba grace  correspondence bayesian estimation stochastic processes smoothing splines annals mathemat ical statistics   kingma diederik  welling max  autoencoding variational bayes in proceedings international conference learning representations kittler josef foglein janos  contextual classiﬁcation multispectral pixel data image vision computing   kolda tamara  bader brett   tensor decompositions applications siam review   koller daphne friedman nir  probabilistic graphical models mit press kong linglong mizera ivan  quantile tomography quantiles multivariate data statistica sinica   lang serge  linear algebra springer lawrence neil   probabilistic nonlinear principal component analysis gaussian process latent variable models journal machine learning research november  leemis lawrence  mcqueston jacquelyn   univariate distribution relationships american statistician   lehmann erich  romano joseph   testing statistical hypotheses springer lehmann erich leo casella george  theory point estimation springer liesen jorg mehrmann volker  linear algebra springer lin hsuantien lin chihjen weng ruby   note platts probabilistic outputs support vector machines machine learning   ljung lennart  identiﬁcation theory user prentice hall loosli gaelle canu stephane ong cheng soon  learning svm kreın spaces  transactions pattern analysis machine intelligence   luenberger david   optimization vector space methods wiley mackay david    bayesian interpolation neural computation   mackay david    introduction gaussian processes   of bishop   ed neural networks machine learning springer mackay david    information theory inference learning algorithms cambridge university press magnus jan  neudecker heinz  matrix differential calculus appli cations statistics econometrics wiley draft  mathematics machine learning feedback smmlbookcom references manton jonathan  amblard pierreolivier  primer reproducing kernel hilbert spaces foundations trends signal processing   markovsky ivan  low rank approximation algorithms implementation appli cations springer maybeck peter   stochastic models estimation control academic press mccullagh peter nelder john   generalized linear models crc press mceliece robert  mackay david   cheng jungfu  turbo decoding instance pearls belief propagation algorithm  journal selected areas communications   mika sebastian ratsch gunnar weston jason scholkopf bernhard muller klausrobert  fisher discriminant analysis kernels   of proceedings workshop neural networks signal processing minka thomas   family algorithms approximate bayesian inference phd thesis massachusetts institute technology minka tom  automatic choice dimensionality pca in advances neural information processing systems mitchell tom  machine learning mcgrawhill mnih volodymyr kavukcuoglu koray silver david et al  humanlevel control deep reinforcement learning nature   moonen marc moor bart  svd signal processing iii algorithms architectures applications elsevier moustaki irini knott martin bartholomew david   latentvariable mod eling american cancer society   muller andreas  guido sarah  introduction machine learning python guide data scientists oreilly publishing murphy kevin   machine learning probabilistic perspective mit press neal radford   bayesian learning neural networks phd thesis depart ment science university toronto neal radford  hinton geoffrey   view algorithm justiﬁes incremental sparse variants   of learning graphical models mit press nelsen roger  introduction copulas springer nesterov yuri  lectures convex optimization springer neumaier arnold  solving illconditioned singular linear systems tu torial regularization siam review   nocedal jorge wright stephen   numerical optimization springer nowozin sebastian gehler peter  jancsary jeremy lampert christoph  eds  advanced structured prediction mit press ohagan anthony  bayeshermite quadrature journal statistical planning inference   ong cheng soon mary xavier canu stephane smola alexander   learn in proceedings international conference ing nonpositive kernels machine learning ormoneit dirk sidenbladh hedvig black michael  hastie trevor  learning tracking cyclic human motion in advances neural information processing systems  lawrence brin sergey motwani rajeev winograd terry  rank citation ranking bringing order web tech rept stanford info lab paquet ulrich  bayesian inference latent variable models phd thesis uni versity cambridge parzen emanuel  estimation probability density function mode annals mathematical statistics      deisenroth   faisal   ong published cambridge university press  references pearl judea  probabilistic reasoning intelligent systems networks plausible pearl judea  causality models reasoning inference nd edn cambridge inference morgan kaufmann university press pearson karl  contributions mathematical theory evolution ii skew variation homogeneous material philosophical transactions royal society  mathematical physical engineering sciences   pearson karl  lines planes closest fit systems points space philosophical magazine   peters jonas janzing dominik scholkopf bernhard  elements causal inference foundations learning algorithms mit press petersen kaare  pedersen michael   matrix cookbook tech rept technical university denmark platt john   probabilistic outputs support vector machines compar isons regularized likelihood methods in advances large margin classiﬁers pollard david  users guide measure theoretic probability cambridge university press polyak roman   legendre transformation modern optimization   of goldengorin  ed optimization applications control data sciences springer press william  teukolsky saul  vetterling william  flannery brian   numerical recipes art scientiﬁc computing cambridge university press proschan michael  presnell brett  expect unexpected condi tional expectation american statistician   raschka sebastian mirjalili vahid  python machine learning machine learning deep learning python scikitlearn tensorflow packt publish ing rasmussen carl  ghahramani zoubin  occams razor in advances neural information processing systems rasmussen carl  ghahramani zoubin  bayesian monte carlo in ad vances neural information processing systems rasmussen carl  williams christopher    gaussian processes ma chine learning mit press reid mark williamson robert   information divergence risk binary experiments journal machine learning research   rifkin ryan  lippert ross   value regularization fenchel duality journal machine learning research   rockafellar ralph   convex analysis princeton university press rogers simon girolami mark  course machine learning chap rosenbaum paul   observation experiment introduction causal man hallcrc inference harvard university press rosenblatt murray  remarks nonparametric estimates density function annals mathematical statistics   roweis sam   algorithms pca spca   of advances neural information processing systems roweis sam  ghahramani zoubin  uniing review linear gaussian models neural computation   roy anindya banerjee sudipto  linear algebra matrix analysis statistics chapman hallcrc rubinstein reuven  kroese dirk   simulation monte carlo method wiley draft  mathematics machine learning feedback smmlbookcom references rufﬁni paolo  teoria generale delle equazioni cui si dimostra impossibile la soluzione algebraica delle equazioni generali di grado superiore al quarto stampe ria di  tommaso daquino rumelhart david  hinton geoffrey  williams ronald   learning representations backpropagating errors nature   sæmundsson steindor hofmann katja deisenroth marc   meta rein forcement learning latent variable gaussian processes in proceedings conference uncertainty artiﬁcial intelligence saitoh saburou  theory reproducing kernels applications longman scientiﬁc technical sarkka simo  bayesian filtering smoothing cambridge university press scholkopf bernhard smola alexander   learning kernels  support vector machines regularization optimization beyond mit press scholkopf bernhard smola alexander  muller klausrobert  kernel in proceedings international conference principal component analysis artiﬁcial neural networks scholkopf bernhard smola alexander  muller klausrobert  nonlinear component analysis kernel eigenvalue problem neural computation  scholkopf bernhard herbrich ralf smola alexander   generalized representer theorem in proceedings international conference computa tional learning theory schwartz laurent  sous espaces hilbertiens despaces vectoriels topologiques et noyaux associes journal danalyse mathematique   schwarz gideon   estimating dimension model annals statistics   shahriari bobak swersky kevin wang ziyu adams ryan  freitas nando  taking human loop review bayesian optimization proceedings    shalevshwartz shai bendavid shai  understanding machine learning theory algorithms cambridge university press shawetaylor john cristianini nello  kernel methods pattern analysis cambridge university press shawetaylor john sun shiliang  review optimization methodologies support vector machines neurocomputing   shental ori siegel paul  wolf jack  bickson danny dolev danny  gaussian belief propagation solver systems linear equations    of proceedings international symposium information theory shewchuk jonathan   introduction conjugate gradient method with agonizing pain shi jianbo malik jitendra  normalized cuts image segmentation  transactions pattern analysis machine intelligence   shi qinfeng petterson james dror gideon langford john smola alexander  vishwanathan     hash kernels structured data journal machine learning research  shiryayev albert   probability springer shor naum   minimization methods nondifferentiable functions springer shotton jamie winn john rother carsten criminisi antonio  texton boost joint appearance shape context modeling multiclass object recog nition segmentation in proceedings european conference vision smith adrian   spiegelhalter david  bayes factors choice criteria linear models journal royal statistical society       deisenroth   faisal   ong published cambridge university press  references snoek jasper larochelle hugo adams ryan   practical bayesian op in advances neural information timization machine learning algorithms processing systems spearman charles  general intelligence objectively determined mea sured american journal psychology   sriperumbudur bharath  gretton arthur fukumizu kenji scholkopf bernhard lanckriet gert    hilbert space embeddings metrics proba bility measures journal machine learning research   steinwart ingo  compare different loss functions risks constructive approximation   steinwart ingo christmann andreas  support vector machines springer stoer josef burlirsch roland  introduction numerical analysis springer strang gilbert  fundamental theorem linear algebra american mathematical monthly   strang gilbert  introduction linear algebra wellesleycambridge press stray jonathan  curious journalists guide data tow center digital journalism columbias graduate school journalism strogatz steven  writing math perplexed traumatized notices american mathematical society   sucar luis  gillies duncan   probabilistic reasoning highlevel vision image vision computing   szeliski richard zabih ramin scharstein daniel et al  compar ative study energy minimization methods markov random fields  transactions pattern analysis machine in smoothnessbased priors telligence   tandra haryono  relationship change variable theorem fundamental theorem calculus lebesgue integral teaching mathematics   tenenbaum joshua  silva vin langford john   global geometric framework nonlinear dimensionality reduction science   tibshirani robert  regression selection shrinkage lasso journal royal statistical society    tipping michael  bishop christopher   probabilistic principal compo nent analysis journal royal statistical society series    titsias michalis  lawrence neil   bayesian gaussian process latent variable model in proceedings international conference artiﬁcial intelli gence statistics toussaint marc  notes gradient descent sipvsinformatikuni stuttgartdemlrmarcnotesgradientdescentpdf trefethen lloyd  bau iii david  numerical linear algebra siam tucker ledyard   mathematical notes threemode factor analysis psychometrika   vapnik vladimir   statistical learning theory wiley vapnik vladimir   overview statistical learning theory  transac tions neural networks   vapnik vladimir   nature statistical learning theory springer vishwanathan    schraudolph nicol  kondor risi borgwardt karsten   graph kernels journal machine learning research   von luxburg ulrike scholkopf bernhard  statistical learning theory models concepts results   of   gabbay  hartmann  woods ed handbook history logic vol  elsevier draft  mathematics machine learning feedback smmlbookcom references wahba grace  spline models observational data society industrial applied mathematics walpole ronald  myers raymond  myers sharon  ye keying  probability statistics engineers scientists prentice hall wasserman larry  statistics springer wasserman larry  nonparametric statistics springer whittle peter  probability expectation springer wickham hadley  tidy data journal statistical software   williams christopher    computing inﬁnite networks in advances neural information processing systems yu yaoliang cheng hao schuurmans dale szepesvari csaba  charac terizing representer theorem in proceedings international conference machine learning zadrozny bianca elkan charles  obtaining calibrated probability esti in proceedings mates decision trees naive bayesian classiﬁers international conference machine learning zhang haizhang xu yuesheng zhang june  reproducing kernel banach spaces machine learning journal machine learning research   zia royce   redish edward  mckay susan   making sense legendre transform american journal physics      deisenroth   faisal   ong published cambridge university press '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":560}]},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","sent = word_tokenize(finalTxt)"],"metadata":{"id":"7tQMTu_zevzK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sent"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZevBtJJIe2e1","executionInfo":{"status":"ok","timestamp":1660001673564,"user_tz":300,"elapsed":4,"user":{"displayName":"Noé Lozano","userId":"09597272047670713906"}},"outputId":"5b9fc28e-dd3c-4ee6-837c-7ade7ac9de7f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['mathematics',\n"," 'machine',\n"," 'learningmarc',\n"," 'peter',\n"," 'deisenrotha',\n"," 'aldo',\n"," 'faisalcheng',\n"," 'soon',\n"," 'ongmathematics',\n"," 'machine',\n"," 'learningdeisenroth',\n"," 'et',\n"," 'althe',\n"," 'fundamental',\n"," 'mathematical',\n"," 'tools',\n"," 'needed',\n"," 'understand',\n"," 'machine',\n"," 'learning',\n"," 'include',\n"," 'linear',\n"," 'algebra',\n"," 'analytic',\n"," 'geometry',\n"," 'matrix',\n"," 'decompositions',\n"," 'vector',\n"," 'calculus',\n"," 'optimization',\n"," 'probability',\n"," 'statistics',\n"," 'topics',\n"," 'traditionally',\n"," 'taught',\n"," 'disparate',\n"," 'courses',\n"," 'making',\n"," 'hard',\n"," 'data',\n"," 'science',\n"," 'science',\n"," 'students',\n"," 'professionals',\n"," 'efﬁ',\n"," 'ciently',\n"," 'learn',\n"," 'mathematics',\n"," 'selfcontained',\n"," 'textbook',\n"," 'bridges',\n"," 'gap',\n"," 'mathematical',\n"," 'machine',\n"," 'learning',\n"," 'texts',\n"," 'introducing',\n"," 'mathematical',\n"," 'concepts',\n"," 'minimum',\n"," 'prerequisites',\n"," 'uses',\n"," 'concepts',\n"," 'derive',\n"," 'central',\n"," 'machine',\n"," 'learning',\n"," 'methods',\n"," 'linear',\n"," 'regression',\n"," 'principal',\n"," 'component',\n"," 'analysis',\n"," 'gaussian',\n"," 'mixture',\n"," 'models',\n"," 'support',\n"," 'vector',\n"," 'machines',\n"," 'students',\n"," 'mathematical',\n"," 'background',\n"," 'derivations',\n"," 'provide',\n"," 'starting',\n"," 'point',\n"," 'machine',\n"," 'learning',\n"," 'texts',\n"," 'learning',\n"," 'mathematics',\n"," 'rst',\n"," 'time',\n"," 'methods',\n"," 'help',\n"," 'build',\n"," 'intuition',\n"," 'practical',\n"," 'experience',\n"," 'applying',\n"," 'mathematical',\n"," 'concepts',\n"," 'includes',\n"," 'worked',\n"," 'examples',\n"," 'exercises',\n"," 'test',\n"," 'understanding',\n"," 'programming',\n"," 'tutorials',\n"," 'offered',\n"," 'books',\n"," 'web',\n"," 'sitemarc',\n"," 'peter',\n"," 'deisenroth',\n"," 'senior',\n"," 'lecturer',\n"," 'statistical',\n"," 'machine',\n"," 'learning',\n"," 'department',\n"," 'computing',\n"," 'împerial',\n"," 'college',\n"," 'londona',\n"," 'aldo',\n"," 'faisal',\n"," 'leads',\n"," 'brain',\n"," 'behaviour',\n"," 'lab',\n"," 'imperial',\n"," 'college',\n"," 'london',\n"," 'reader',\n"," 'neurotechnology',\n"," 'department',\n"," 'bioengineering',\n"," 'department',\n"," 'computingcheng',\n"," 'soon',\n"," 'ong',\n"," 'principal',\n"," 'research',\n"," 'scientist',\n"," 'machine',\n"," 'learning',\n"," 'research',\n"," 'group',\n"," 'data',\n"," 'csiro',\n"," 'adjunct',\n"," 'associate',\n"," 'professor',\n"," 'australian',\n"," 'national',\n"," 'universitycover',\n"," 'image',\n"," 'courtesy',\n"," 'daniel',\n"," 'bosma',\n"," 'moment',\n"," 'getty',\n"," 'imagescover',\n"," 'design',\n"," 'holly',\n"," 'johnsondeisenrith',\n"," 'et',\n"," 'al',\n"," 'cover',\n"," 'contents',\n"," 'foreword',\n"," 'mathematical',\n"," 'foundations',\n"," 'introduction',\n"," 'motivation',\n"," 'finding',\n"," 'words',\n"," 'intuitions',\n"," 'ways',\n"," 'read',\n"," 'book',\n"," 'exercises',\n"," 'feedback',\n"," 'linear',\n"," 'algebra',\n"," 'systems',\n"," 'linear',\n"," 'equations',\n"," 'matrices',\n"," 'solving',\n"," 'systems',\n"," 'linear',\n"," 'equations',\n"," 'vector',\n"," 'spaces',\n"," 'linear',\n"," 'independence',\n"," 'basis',\n"," 'rank',\n"," 'linear',\n"," 'mappings',\n"," 'afﬁne',\n"," 'spaces',\n"," 'reading',\n"," 'exercises',\n"," 'analytic',\n"," 'geometry',\n"," 'inner',\n"," 'products',\n"," 'lengths',\n"," 'distances',\n"," 'angles',\n"," 'orthogonality',\n"," 'norms',\n"," 'orthonormal',\n"," 'basis',\n"," 'orthogonal',\n"," 'complement',\n"," 'orthogonal',\n"," 'projections',\n"," 'rotations',\n"," 'reading',\n"," 'exercises',\n"," 'inner',\n"," 'product',\n"," 'functions',\n"," 'matrix',\n"," 'decompositions',\n"," 'determinant',\n"," 'trace',\n"," 'material',\n"," 'published',\n"," 'cambridge',\n"," 'university',\n"," 'press',\n"," 'mathematics',\n"," 'machine',\n"," 'learning',\n"," 'marc',\n"," 'peter',\n"," 'deisenroth',\n"," 'aldo',\n"," 'faisal',\n"," 'cheng',\n"," 'soon',\n"," 'ong',\n"," 'version',\n"," 'free',\n"," 'view',\n"," 'download',\n"," 'personal',\n"," 'use',\n"," 'only',\n"," 'redistribution',\n"," 'resale',\n"," 'use',\n"," 'derivative',\n"," 'works',\n"," 'by',\n"," 'deisenroth',\n"," 'faisal',\n"," 'ong',\n"," 'smmlbookcom',\n"," 'ii',\n"," 'contents',\n"," 'eigenvalues',\n"," 'eigenvectors',\n"," 'cholesky',\n"," 'decomposition',\n"," 'eigendecomposition',\n"," 'diagonalization',\n"," 'singular',\n"," 'value',\n"," 'decomposition',\n"," 'matrix',\n"," 'approximation',\n"," 'matrix',\n"," 'phylogeny',\n"," 'reading',\n"," 'exercises',\n"," 'vector',\n"," 'calculus',\n"," 'differentiation',\n"," 'univariate',\n"," 'functions',\n"," 'partial',\n"," 'differentiation',\n"," 'gradients',\n"," 'gradients',\n"," 'vectorvalued',\n"," 'functions',\n"," 'gradients',\n"," 'matrices',\n"," 'useful',\n"," 'identities',\n"," 'computing',\n"," 'gradients',\n"," 'backpropagation',\n"," 'automatic',\n"," 'differentiation',\n"," 'higherorder',\n"," 'derivatives',\n"," 'linearization',\n"," 'multivariate',\n"," 'taylor',\n"," 'series',\n"," 'reading',\n"," 'exercises',\n"," 'probability',\n"," 'distributions',\n"," 'construction',\n"," 'probability',\n"," 'space',\n"," 'discrete',\n"," 'continuous',\n"," 'probabilities',\n"," 'sum',\n"," 'rule',\n"," 'product',\n"," 'rule',\n"," 'bayes',\n"," 'theorem',\n"," 'summary',\n"," 'statistics',\n"," 'independence',\n"," 'gaussian',\n"," 'distribution',\n"," 'conjugacy',\n"," 'exponential',\n"," 'family',\n"," 'change',\n"," 'variablesinverse',\n"," 'transform',\n"," 'reading',\n"," 'exercises',\n"," 'continuous',\n"," 'optimization',\n"," 'optimization',\n"," 'gradient',\n"," 'descent',\n"," 'constrained',\n"," 'optimization',\n"," 'lagrange',\n"," 'multipliers',\n"," 'convex',\n"," 'optimization',\n"," 'reading',\n"," 'exercises',\n"," 'ii',\n"," 'central',\n"," 'machine',\n"," 'learning',\n"," 'problems',\n"," 'models',\n"," 'meet',\n"," 'data',\n"," 'data',\n"," 'models',\n"," 'learning',\n"," 'empirical',\n"," 'risk',\n"," 'minimization',\n"," 'parameter',\n"," 'estimation',\n"," 'probabilistic',\n"," 'modeling',\n"," 'inference',\n"," 'directed',\n"," 'graphical',\n"," 'models',\n"," 'draft',\n"," 'mathematics',\n"," 'machine',\n"," 'learning',\n"," 'feedback',\n"," 'smmlbookcom',\n"," 'contents',\n"," 'model',\n"," 'selection',\n"," 'linear',\n"," 'regression',\n"," 'problem',\n"," 'formulation',\n"," 'parameter',\n"," 'estimation',\n"," 'bayesian',\n"," 'linear',\n"," 'regression',\n"," 'maximum',\n"," 'likelihood',\n"," 'orthogonal',\n"," 'projection',\n"," 'reading',\n"," 'problem',\n"," 'setting',\n"," 'maximum',\n"," 'variance',\n"," 'perspective',\n"," 'projection',\n"," 'perspective',\n"," 'eigenvector',\n"," 'computation',\n"," 'lowrank',\n"," 'approximations',\n"," 'pca',\n"," 'high',\n"," 'dimensions',\n"," 'key',\n"," 'steps',\n"," 'pca',\n"," 'practice',\n"," 'latent',\n"," 'variable',\n"," 'perspective',\n"," 'reading',\n"," 'dimensionality',\n"," 'reduction',\n"," 'principal',\n"," 'component',\n"," 'analysis',\n"," 'density',\n"," 'estimation',\n"," 'gaussian',\n"," 'mixture',\n"," 'models',\n"," 'gaussian',\n"," 'mixture',\n"," 'model',\n"," 'parameter',\n"," 'learning',\n"," 'maximum',\n"," 'likelihood',\n"," 'algorithm',\n"," 'latentvariable',\n"," 'perspective',\n"," 'reading',\n"," 'classiﬁcation',\n"," 'support',\n"," 'vector',\n"," 'machines',\n"," 'separating',\n"," 'hyperplanes',\n"," 'primal',\n"," 'support',\n"," 'vector',\n"," 'machine',\n"," 'dual',\n"," 'support',\n"," 'vector',\n"," 'machine',\n"," 'kernels',\n"," 'numerical',\n"," 'solution',\n"," 'reading',\n"," 'references',\n"," 'iii',\n"," 'deisenroth',\n"," 'faisal',\n"," 'ong',\n"," 'published',\n"," 'cambridge',\n"," 'university',\n"," 'press',\n"," 'foreword',\n"," 'machine',\n"," 'learning',\n"," 'latest',\n"," 'long',\n"," 'line',\n"," 'attempts',\n"," 'distill',\n"," 'human',\n"," 'knowledge',\n"," 'reasoning',\n"," 'form',\n"," 'suitable',\n"," 'constructing',\n"," 'ma',\n"," 'chines',\n"," 'engineering',\n"," 'automated',\n"," 'systems',\n"," 'machine',\n"," 'learning',\n"," 'ubiquitous',\n"," 'software',\n"," 'packages',\n"," 'easier',\n"," 'use',\n"," 'nat',\n"," 'ural',\n"," 'desirable',\n"," 'lowlevel',\n"," 'technical',\n"," 'details',\n"," 'abstracted',\n"," 'away',\n"," 'hidden',\n"," 'practitioner',\n"," 'however',\n"," 'brings',\n"," 'danger',\n"," 'practitioner',\n"," 'unaware',\n"," 'design',\n"," 'decisions',\n"," 'and',\n"," 'hence',\n"," 'limits',\n"," 'machine',\n"," 'learning',\n"," 'algorithms',\n"," 'enthusiastic',\n"," 'practitioner',\n"," 'interested',\n"," 'learn',\n"," 'magic',\n"," 'successful',\n"," 'machine',\n"," 'learning',\n"," 'algorithms',\n"," 'currently',\n"," 'faces',\n"," 'daunting',\n"," 'set',\n"," 'prerequisite',\n"," 'knowledge',\n"," 'programming',\n"," 'languages',\n"," 'data',\n"," 'analysis',\n"," 'tools',\n"," 'largescale',\n"," 'computation',\n"," 'associated',\n"," 'frameworks',\n"," 'mathematics',\n"," 'statistics',\n"," 'machine',\n"," 'learning',\n"," 'builds',\n"," 'universities',\n"," 'introductory',\n"," 'courses',\n"," 'machine',\n"," 'learning',\n"," 'tend',\n"," 'spend',\n"," 'early',\n"," 'parts',\n"," 'course',\n"," 'covering',\n"," 'prerequisites',\n"," 'histori',\n"," 'cal',\n"," 'reasons',\n"," 'courses',\n"," 'machine',\n"," 'learning',\n"," 'tend',\n"," 'taught',\n"," 'science',\n"," 'department',\n"," 'students',\n"," 'trained',\n"," 'ﬁrst',\n"," 'areas',\n"," 'knowledge',\n"," 'mathematics',\n"," 'statistics',\n"," 'current',\n"," 'machine',\n"," 'learning',\n"," 'textbooks',\n"," 'primarily',\n"," 'focus',\n"," 'machine',\n"," 'learn',\n"," 'ing',\n"," 'algorithms',\n"," 'methodologies',\n"," 'assume',\n"," 'reader',\n"," 'com',\n"," 'petent',\n"," 'mathematics',\n"," 'statistics',\n"," 'therefore',\n"," 'books',\n"," 'spend',\n"," 'background',\n"," 'mathematics',\n"," 'beginning',\n"," 'book',\n"," 'appendices',\n"," 'people',\n"," 'want',\n"," 'delve',\n"," 'foundations',\n"," 'basic',\n"," 'machine',\n"," 'learning',\n"," 'methods',\n"," 'strug',\n"," 'gle',\n"," 'mathematical',\n"," 'knowledge',\n"," 'required',\n"," 'read',\n"," 'machine',\n"," 'learning',\n"," 'textbook',\n"," 'having',\n"," 'taught',\n"," 'undergraduate',\n"," 'graduate',\n"," 'courses',\n"," 'universi',\n"," 'ties',\n"," 'ﬁnd',\n"," 'gap',\n"," 'high',\n"," 'school',\n"," 'mathematics',\n"," 'math',\n"," 'ematics',\n"," 'level',\n"," 'required',\n"," 'read',\n"," 'standard',\n"," 'machine',\n"," 'learning',\n"," 'textbook',\n"," 'big',\n"," 'people',\n"," 'book',\n"," 'brings',\n"," 'mathematical',\n"," 'foundations',\n"," 'basic',\n"," 'machine',\n"," 'learn',\n"," 'ing',\n"," 'concepts',\n"," 'fore',\n"," 'collects',\n"," 'information',\n"," 'single',\n"," 'place',\n"," 'skills',\n"," 'gap',\n"," 'narrowed',\n"," 'closed',\n"," 'material',\n"," 'published',\n"," 'cambridge',\n"," 'university',\n"," 'press',\n"," 'mathematics',\n"," 'machine',\n"," 'learning',\n"," 'marc',\n"," 'peter',\n"," 'deisenroth',\n"," 'aldo',\n"," 'faisal',\n"," 'cheng',\n"," 'soon',\n"," 'ong',\n"," 'version',\n"," 'free',\n"," 'view',\n"," 'download',\n"," 'personal',\n"," 'use',\n"," 'only',\n"," 'redistribution',\n"," 'resale',\n"," 'use',\n"," 'derivative',\n"," 'works',\n"," 'by',\n"," 'deisenroth',\n"," 'faisal',\n"," 'ong',\n"," 'smmlbookcom',\n"," 'math',\n"," 'linked',\n"," 'popular',\n"," 'mind',\n"," 'phobia',\n"," 'anxiety',\n"," 'think',\n"," 'discussing',\n"," 'spiders',\n"," 'strogatz',\n"," 'foreword',\n"," 'book',\n"," 'machine',\n"," 'learning',\n"," 'machine',\n"," 'learning',\n"," 'builds',\n"," 'language',\n"," 'mathematics',\n"," 'express',\n"," 'concepts',\n"," 'intuitively',\n"," 'obvious',\n"," 'surprisingly',\n"," 'difﬁcult',\n"," 'formalize',\n"," 'formalized',\n"," 'properly',\n"," 'gain',\n"," 'insights',\n"," 'task',\n"," 'want',\n"," 'solve',\n"," 'common',\n"," 'complaint',\n"," 'students',\n"," 'mathematics',\n"," 'globe',\n"," 'topics',\n"," 'covered',\n"," 'little',\n"," 'relevance',\n"," 'practical',\n"," 'problems',\n"," 'believe',\n"," 'machine',\n"," 'learning',\n"," 'obvious',\n"," 'direct',\n"," 'motivation',\n"," 'people',\n"," 'learn',\n"," 'mathematics',\n"," 'book',\n"," 'intended',\n"," 'guidebook',\n"," 'vast',\n"," 'mathematical',\n"," 'lit',\n"," 'erature',\n"," 'forms',\n"," 'foundations',\n"," 'modern',\n"," 'machine',\n"," 'learning',\n"," 'mo',\n"," 'tivate',\n"," 'need',\n"," 'mathematical',\n"," 'concepts',\n"," 'directly',\n"," 'pointing',\n"," 'usefulness',\n"," 'context',\n"," 'fundamental',\n"," 'machine',\n"," 'learning',\n"," 'problems',\n"," 'keeping',\n"," 'book',\n"," 'short',\n"," 'details',\n"," 'advanced',\n"," 'concepts',\n"," 'left',\n"," 'out',\n"," 'equipped',\n"," 'basic',\n"," 'concepts',\n"," 'presented',\n"," 'here',\n"," 'ﬁt',\n"," 'larger',\n"," 'context',\n"," 'machine',\n"," 'learning',\n"," 'reader',\n"," 'ﬁnd',\n"," 'numerous',\n"," 'resources',\n"," 'study',\n"," 'provide',\n"," 'end',\n"," 'respective',\n"," 'readers',\n"," 'mathematical',\n"," 'back',\n"," 'ground',\n"," 'book',\n"," 'provides',\n"," 'brief',\n"," 'precisely',\n"," 'stated',\n"," 'glimpse',\n"," 'machine',\n"," 'learning',\n"," 'contrast',\n"," 'books',\n"," 'focus',\n"," 'methods',\n"," 'models',\n"," 'machine',\n"," 'learning',\n"," 'mackay',\n"," 'bishop',\n"," 'alpaydin',\n"," 'bar',\n"," 'ber',\n"," 'murphy',\n"," 'shalevshwartz',\n"," 'bendavid',\n"," 'rogers',\n"," 'girolami',\n"," 'programmatic',\n"," 'aspects',\n"," 'machine',\n"," 'learning',\n"," 'muller',\n"," 'guido',\n"," 'raschka',\n"," 'mirjalili',\n"," 'chollet',\n"," 'allaire',\n"," 'provide',\n"," 'representative',\n"," 'examples',\n"," 'machine',\n"," 'learning',\n"," 'algo',\n"," 'rithms',\n"," 'instead',\n"," 'focus',\n"," 'mathematical',\n"," 'concepts',\n"," 'models',\n"," 'themselves',\n"," 'hope',\n"," 'readers',\n"," 'able',\n"," 'gain',\n"," 'deeper',\n"," 'understand',\n"," 'ing',\n"," 'basic',\n"," 'questions',\n"," 'machine',\n"," 'learning',\n"," 'connect',\n"," 'practical',\n"," 'ques',\n"," 'tions',\n"," 'arising',\n"," 'use',\n"," 'machine',\n"," 'learning',\n"," 'fundamental',\n"," 'choices',\n"," 'mathematical',\n"," 'model',\n"," 'aim',\n"," 'write',\n"," 'classical',\n"," 'machine',\n"," 'learning',\n"," 'book',\n"," 'instead',\n"," 'intention',\n"," 'provide',\n"," 'mathematical',\n"," 'background',\n"," 'applied',\n"," 'cen',\n"," 'tral',\n"," 'machine',\n"," 'learning',\n"," 'problems',\n"," 'easier',\n"," 'read',\n"," 'machine',\n"," 'learning',\n"," 'textbooks',\n"," 'target',\n"," 'audience',\n"," 'applications',\n"," 'machine',\n"," 'learning',\n"," 'widespread',\n"," 'society',\n"," 'believe',\n"," 'everybody',\n"," 'understanding',\n"," 'underlying',\n"," 'principles',\n"," 'book',\n"," 'written',\n"," 'academic',\n"," 'mathematical',\n"," 'style',\n"," 'enables',\n"," 'precise',\n"," 'concepts',\n"," 'machine',\n"," 'learning',\n"," 'encourage',\n"," 'readers',\n"," 'unfamiliar',\n"," 'seemingly',\n"," 'terse',\n"," 'style',\n"," 'persevere',\n"," 'goals',\n"," 'topic',\n"," 'mind',\n"," 'sprinkle',\n"," 'comments',\n"," 'remarks',\n"," 'text',\n"," 'hope',\n"," 'provides',\n"," 'useful',\n"," 'guidance',\n"," 'respect',\n"," 'big',\n"," 'picture',\n"," 'book',\n"," 'assumes',\n"," 'reader',\n"," 'mathematical',\n"," 'knowledge',\n"," 'commonly',\n"," 'draft',\n"," 'mathematics',\n"," 'machine',\n"," 'learning',\n"," 'feedback',\n"," 'smmlbookcom',\n"," 'foreword',\n"," 'covered',\n"," 'high',\n"," 'school',\n"," 'mathematics',\n"," 'physics',\n"," 'example',\n"," 'reader',\n"," 'seen',\n"," 'derivatives',\n"," 'integrals',\n"," 'before',\n"," 'geometric',\n"," 'vectors',\n"," 'dimensions',\n"," 'starting',\n"," 'there',\n"," 'generalize',\n"," 'con',\n"," 'cepts',\n"," 'therefore',\n"," 'target',\n"," 'audience',\n"," 'book',\n"," 'includes',\n"," 'undergraduate',\n"," 'university',\n"," 'students',\n"," 'evening',\n"," 'learners',\n"," 'learners',\n"," 'participating',\n"," 'online',\n"," 'machine',\n"," 'learning',\n"," 'courses',\n"," 'analogy',\n"," 'music',\n"," 'types',\n"," 'interaction',\n"," 'people',\n"," 'machine',\n"," 'learning',\n"," 'astute',\n"," 'listener',\n"," 'democratization',\n"," 'machine',\n"," 'learning',\n"," 'pro',\n"," 'vision',\n"," 'opensource',\n"," 'software',\n"," 'online',\n"," 'tutorials',\n"," 'cloudbased',\n"," 'tools',\n"," ...]"]},"metadata":{},"execution_count":562}]},{"cell_type":"code","source":["while(\"ee\" in sent) :\n","    sent.remove(\"ee\")"],"metadata":{"id":"8P-_tZlg2LF5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sent[0:100]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7jEFp8Fu2Lqx","executionInfo":{"status":"ok","timestamp":1660001673781,"user_tz":300,"elapsed":220,"user":{"displayName":"Noé Lozano","userId":"09597272047670713906"}},"outputId":"75e37d9e-eb8c-41d3-8cf3-d36e0687efa5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['mathematics',\n"," 'machine',\n"," 'learningmarc',\n"," 'peter',\n"," 'deisenrotha',\n"," 'aldo',\n"," 'faisalcheng',\n"," 'soon',\n"," 'ongmathematics',\n"," 'machine',\n"," 'learningdeisenroth',\n"," 'et',\n"," 'althe',\n"," 'fundamental',\n"," 'mathematical',\n"," 'tools',\n"," 'needed',\n"," 'understand',\n"," 'machine',\n"," 'learning',\n"," 'include',\n"," 'linear',\n"," 'algebra',\n"," 'analytic',\n"," 'geometry',\n"," 'matrix',\n"," 'decompositions',\n"," 'vector',\n"," 'calculus',\n"," 'optimization',\n"," 'probability',\n"," 'statistics',\n"," 'topics',\n"," 'traditionally',\n"," 'taught',\n"," 'disparate',\n"," 'courses',\n"," 'making',\n"," 'hard',\n"," 'data',\n"," 'science',\n"," 'science',\n"," 'students',\n"," 'professionals',\n"," 'efﬁ',\n"," 'ciently',\n"," 'learn',\n"," 'mathematics',\n"," 'selfcontained',\n"," 'textbook',\n"," 'bridges',\n"," 'gap',\n"," 'mathematical',\n"," 'machine',\n"," 'learning',\n"," 'texts',\n"," 'introducing',\n"," 'mathematical',\n"," 'concepts',\n"," 'minimum',\n"," 'prerequisites',\n"," 'uses',\n"," 'concepts',\n"," 'derive',\n"," 'central',\n"," 'machine',\n"," 'learning',\n"," 'methods',\n"," 'linear',\n"," 'regression',\n"," 'principal',\n"," 'component',\n"," 'analysis',\n"," 'gaussian',\n"," 'mixture',\n"," 'models',\n"," 'support',\n"," 'vector',\n"," 'machines',\n"," 'students',\n"," 'mathematical',\n"," 'background',\n"," 'derivations',\n"," 'provide',\n"," 'starting',\n"," 'point',\n"," 'machine',\n"," 'learning',\n"," 'texts',\n"," 'learning',\n"," 'mathematics',\n"," 'rst',\n"," 'time',\n"," 'methods',\n"," 'help',\n"," 'build',\n"," 'intuition',\n"," 'practical',\n"," 'experience',\n"," 'applying']"]},"metadata":{},"execution_count":564}]},{"cell_type":"code","source":["from nltk.stem.wordnet import WordNetLemmatizer\n","lemmatizer = WordNetLemmatizer()\n","\n","lem_sent = [lemmatizer.lemmatize(words_Lemma) for words_Lemma in sent]"],"metadata":{"id":"dMG9zM5OXElV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lem_sent[0:20]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jpfAYKJ2aU-q","executionInfo":{"status":"ok","timestamp":1660001674392,"user_tz":300,"elapsed":4,"user":{"displayName":"Noé Lozano","userId":"09597272047670713906"}},"outputId":"dd3c0948-1285-4931-f7a1-c7b1a2bdc1b1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['mathematics',\n"," 'machine',\n"," 'learningmarc',\n"," 'peter',\n"," 'deisenrotha',\n"," 'aldo',\n"," 'faisalcheng',\n"," 'soon',\n"," 'ongmathematics',\n"," 'machine',\n"," 'learningdeisenroth',\n"," 'et',\n"," 'althe',\n"," 'fundamental',\n"," 'mathematical',\n"," 'tool',\n"," 'needed',\n"," 'understand',\n"," 'machine',\n"," 'learning']"]},"metadata":{},"execution_count":566}]},{"cell_type":"code","source":["no_singl_words = []\n","\n","for line in lem_sent:\n","  res = remove_noisyWords(line)\n","  no_singl_words.append(res)\n","\n","while(\"\" in no_singl_words) :\n","    no_singl_words.remove(\"\")"],"metadata":{"id":"hPZn35VnrCYW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["f = open(\"MathematicsForMachineLearning-lemmatized.txt\", \"x\")\n","f.close()\n","\n","with open(\"MathematicsForMachineLearning-lemmatized.txt\", \"w\") as file1:\n","    for line in no_singl_words:\n","      file1.write(line+\"\\n\")\n","      #file1.writelines(CalcWikibook_wthtStopWords)"],"metadata":{"id":"2z1SXu200IbE"},"execution_count":null,"outputs":[]}]}